<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta name="description" content="Hello,here is kengkeng&#39;s blog." />
      
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title> kengkeng&#39;s life</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dajiangdahe.github.io/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/dajiangdahe.github.io/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      
<section class="cover">
    
      
      <a class="forkMe" href="https://github.com/Shen-Yu/hexo-theme-ayer"
        target="_blank"><img width="149" height="149" src="/dajiangdahe.github.io/images/forkme.png"
          class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/cover1.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/dajiangdahe.github.io/">kengkeng&#39;s life</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['面朝大海，春暖花开', '愿你一生努力，一生被爱', '想要的都拥有，得不到的都释怀'],
        startDelay: 0,
        typeSpeed: 200,
        loop: true,
        backSpeed: 100,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  <ul class="ads">
    
        <li>
            <a target="_blank" rel="noopener" href="https://curl.qcloud.com/kvO7hb43">
                <img src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/ten_1.jpg" width="300" alt="云服务器限时秒杀">
            </a>
        </li>
    
        <li>
            <a target="_blank" rel="noopener" href="https://www.vultr.com/?ref=8630075">
                <img src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/vultr.png" width="300" alt="vultr优惠vps">
            </a>
        </li>
    
</ul>
  
  
  

<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content" id="broad"></div>
</div>
<script type="text/javascript">
    fetch('https://v1.hitokoto.cn')
        .then(response => response.json())
        .then(data => {
            document.getElementById("broad").innerHTML = data.hitokoto;
        })
        .catch(console.error)
</script>

<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-Redis集群模式"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/"
    >Redis集群模式</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/" class="article-date">
  <time datetime="2021-10-22T07:21:45.081Z" itemprop="datePublished">2021-10-22</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/dajiangdahe.github.io/categories/Redis/">Redis</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>从宏观角度回顾一下Redis实现高可用相关的技术。它们包括：持久化、复制、哨兵和集群，其主要作用和解决的问题是：</p>
<ul>
<li>持久化：持久化是最简单的高可用方法(有时甚至不被归为高可用的手段)，主要作用是数据备份，即将数据存储在硬盘，保证数据不会因进程退出而丢失。</li>
<li>复制：复制是高可用Redis的基础，哨兵和集群都是在复制基础上实现高可用的。复制主要实现了数据的多机备份，以及对于读操作的负载均衡和简单的故障恢复。缺陷：故障恢复无法自动化；写操作无法负载均衡；存储能力受到单机的限制。</li>
<li>哨兵：在复制的基础上，哨兵实现了自动化的故障恢复。缺陷：写操作无法负载均衡；存储能力受到单机的限制。</li>
<li>集群：通过集群，Redis解决了写操作无法负载均衡，以及存储能力受到单机限制的问题，实现了较为完善的高可用方案。</li>
</ul>
<h1>1.Redis主从复制</h1>
<h2 id="概念">概念</h2>
<p>主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点(master)，后者称为从节点(slave)；数据的复制是单向的，只能由主节点到从节点。</p>
<p>默认情况下，每台Redis服务器都是主节点；且一个主节点可以有多个从节点(或没有从节点)，但一个从节点只能有一个主节点。</p>
<h2 id="作用">作用</h2>
<ol>
<li>数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式</li>
<li>故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；</li>
<li>负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，从节点提供读服务，分担服务器压力；在多读少写的场景下，通过多个从节点进行分担负载，可以大大的提高Redis服务器的并发量。</li>
<li>高可用基石：除了上述作用以外，主从复制还是哨兵和集群实现的基础。</li>
</ol>
<h2 id="如何使用主从复制">如何使用主从复制</h2>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/a6c2adcacf3c3ebebd2853021a02ca32.png" class="" title="img">
<p><strong>主从复制的开启，完全是在从节点发起的；不需要我们在主节点做任何事情。</strong></p>
<ol>
<li>
<p>开启主从复制</p>
<ul>
<li>在slave 直接执行命令：slaveof <masterip> <masterport></li>
<li>在 slave 配置文件中加入：slaveof <masterip> <masterport></li>
<li>使用启动命令：–slaveof <masterip> <masterport></li>
</ul>
<p>注：在 Redis 5.0 之后，slaveof 相关命令和配置已经被替换成 replicaof，例如 replicaof <masterip> <masterport>。为了兼容旧版本，通过配置的方式仍然支持 slaveof，但是通过命令的方式则不行了。</p>
</li>
<li>
<p>建立socket连接</p>
<p>slave 将根据指定的 IP 地址和端口，向 master 发起套接字（socket）连接，master 在接受accept）slave 的套接字连接之后，为该套接字创建相应的客户端状态，此时连接建立完成。</p>
</li>
<li>
<p>发送Ping命令</p>
<p>slave 向 master 发送一个 PING 命令，以检査套接字的读写状态是否正常、 master 能否正常处理命令请求。</p>
</li>
<li>
<p>身份验证</p>
<p>slave 向 master 发送 AUTH password 命令来进行身份验证。</p>
</li>
<li>
<p>发送端口信息</p>
<p>在身份验证通过后后， slave 将向 master 发送自己的监听端口号， master 收到后记录在 slave 所对应的客户端状态的 slave_listening_port 属性中。</p>
</li>
<li>
<p>发送IP地址</p>
<p>如果配置了 slave_announce_ip，则 slave 向 master 发送 slave_announce_ip 配置的 IP 地址， master 收到后记录在 slave 所对应的客户端状态的 slave_ip 属性。</p>
<p>该配置是用于解决服务器返回内网 IP 时，其他服务器无法访问的情况。可以通过该配置直接指定公网 IP。</p>
</li>
<li>
<p>发送CAPA</p>
<p>CAPA 全称是 capabilities，这边表示的是同步复制的能力。slave 会在这一阶段发送 capa 告诉 master 自己具备的（同步）复制能力， master 收到后记录在 slave 所对应的客户端状态的 slave_capa 属性。</p>
</li>
<li>
<p>数据同步</p>
<p>slave 将向 master 发送 PSYNC 命令， master 收到该命令后判断是进行部分重同步还是完整重同步，然后根据策略进行数据的同步。</p>
<p>需要注意的是，在数据同步阶段之前，从节点是主节点的客户端，主节点不是从节点的客户端；而到了这一阶段及以后，主从节点互为客户端。原因在于：在此之前，主节点只需要响应从节点的请求即可，不需要主动发请求，而在数据同步阶段和后面的命令传播阶段，主节点需要主动向从节点发送请求（如推送缓冲区中的写命令），才能完成复制。</p>
</li>
<li>
<p>命令传播</p>
<p>当完成了同步之后，就会进入命令传播阶段，这时 master 只要一直将自己执行的写命令发送给 slave ，而 slave 只要一直接收并执行 master 发来的写命令，就可以保证 master 和 slave 一直保持一致了。</p>
<p>在命令传播阶段，除了发送写命令，主从节点还维持着心跳机制：PING和REPLCONF ACK。由于心跳机制的原理涉及部分复制，因此将在介绍了部分复制的相关内容后单独介绍该心跳机制。</p>
<p><strong>【延迟与不一致】</strong></p>
<p>需要注意的是，命令传播是异步的过程，即主节点发送写命令后并不会等待从节点的回复；因此实际上主从节点之间很难保持实时的一致性，延迟在所难免。数据不一致的程度，与主从节点之间的网络状况、主节点写命令的执行频率、以及主节点中的repl-disable-tcp-nodelay配置等有关。</p>
<p>repl-disable-tcp-nodelay no：该配置作用于命令传播阶段，控制主节点是否禁止与从节点的TCP_NODELAY；默认no，即不禁止TCP_NODELAY。当设置为yes时，TCP会对包进行合并从而减少带宽，但是发送的频率会降低，从节点数据延迟增加，一致性变差；具体发送频率与Linux内核的配置有关，默认配置为40ms。当设置为no时，TCP会立马将主节点的数据发送给从节点，带宽增加但延迟变小。</p>
<p>一般来说，只有当应用对Redis数据不一致的容忍度较高，且主从节点之间网络状况不好时，才会设置为yes；多数情况使用默认值no。</p>
<p><strong>【优化措施】</strong></p>
<ul>
<li>
<p>优化主从节点之间的网络环境（如在同机房部署）</p>
</li>
<li>
<p>监控主从节点延迟（通过offset）判断，如果从节点延迟过大，通知应用不再通过该从节点读取数据</p>
</li>
<li>
<p>使用集群同时扩展写负载和读负载</p>
</li>
</ul>
</li>
</ol>
<h2 id="示例">示例</h2>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/image-20210924143457687.png" class="" title="image-20210924143457687">
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/image-20210924143508264.png" class="" title="image-20210924143508264">
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/image-20210924143519389.png" class="" title="image-20210924143519389">
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/image-20210924143551169.png" class="" title="image-20210924143551169">
<h2 id="【数据同步阶段】全量复制和部分复制">【数据同步阶段】全量复制和部分复制</h2>
<ol>
<li>全量复制：用于初次复制或其他无法进行部分复制的情况，将主节点中的所有数据都发送给从节点，是一个非常重型的操作。</li>
<li>部分复制：用于网络中断等情况后的复制，只将中断期间主节点执行的写命令发送给从节点，与全量复制相比更加高效。需要注意的是，如果网络中断时间过长，导致主节点没有能够完整地保存中断期间执行的写命令，则无法进行部分复制，仍使用全量复制。</li>
</ol>
<h3 id="全量复制">全量复制</h3>
<p>Redis通过psync命令进行全量复制的过程如下：</p>
<p>（1）从节点判断无法进行部分复制，向主节点发送全量复制的请求；或从节点发送部分复制的请求，但主节点判断无法进行部分复制；具体判断过程需要在讲述了部分复制原理后再介绍。</p>
<p>（2）主节点收到全量复制的命令后，执行bgsave，在后台生成RDB文件，并使用一个缓冲区（称为复制缓冲区）记录从现在开始执行的所有写命令</p>
<p>（3）主节点的bgsave执行完成后，将RDB文件发送给从节点；<strong>从节点首先清除自己的旧数据，然后载入接收的RDB文件</strong>，将数据库状态更新至主节点执行bgsave时的数据库状态</p>
<p>（4）主节点将前述复制缓冲区中的所有写命令发送给从节点，从节点执行这些写命令，将数据库状态更新至主节点的最新状态</p>
<p>（5）如果从节点开启了AOF，则会触发bgrewriteaof的执行，从而保证AOF文件更新至主节点的最新状态</p>
<p>通过全量复制的过程可以看出，全量复制是非常重型的操作：</p>
<p>（1）主节点通过bgsave命令fork子进程进行RDB持久化，该过程是非常消耗CPU、内存(页表复制)、硬盘IO的；关于bgsave的性能问题。</p>
<p>（2）主节点通过网络将RDB文件发送给从节点，对主从节点的带宽都会带来很大的消耗</p>
<p>（3）从节点清空老数据、载入新RDB文件的过程是阻塞的，无法响应客户端的命令；如果从节点执行bgrewriteaof，也会带来额外的消耗</p>
<p><strong>【示例】</strong></p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/image-20210924152335494.png" class="" title="image-20210924152335494">
<h3 id="部分复制">部分复制</h3>
<p>由于全量复制在主节点数据量较大时效率太低，因此Redis2.8开始提供部分复制，用于处理网络中断时的数据同步。</p>
<p>部分复制的实现，依赖于三个重要的概念：</p>
<h4 id="（1）复制偏移量">（1）复制偏移量</h4>
<p>主节点和从节点分别维护一个复制偏移量（offset），代表的是<strong>主节点向从节点传递的字节数</strong>；主节点每次向从节点传播N个字节数据时，主节点的offset增加N；从节点每次收到主节点传来的N个字节数据时，从节点的offset增加N。</p>
<p>offset用于判断主从节点的数据库状态是否一致：如果二者offset相同，则一致；如果offset不同，则不一致，此时可以根据两个offset找出从节点缺少的那部分数据。例如，如果主节点的offset是1000，而从节点的offset是500，那么部分复制就需要将offset为501-1000的数据传递给从节点。而offset为501-1000的数据存储的位置，就是下面要介绍的复制积压缓冲区。</p>
<h4 id="（2）复制积压缓冲区">（2）复制积压缓冲区</h4>
<p>复制积压缓冲区是由主节点维护的、固定长度的、先进先出(FIFO)队列，默认大小1MB；当主节点开始有从节点时创建，其作用是<strong>备份主节点最近发送给从节点的数据</strong>。注意，<strong>无论主节点有一个还是多个从节点，都只需要一个复制积压缓冲区。</strong></p>
<p>在命令传播阶段，主节点除了将写命令发送给从节点，还会发送一份给复制积压缓冲区，作为写命令的备份；除了存储写命令，复制积压缓冲区中还存储了其中的每个字节对应的复制偏移量（offset）。由于复制积压缓冲区定长且是先进先出，所以它保存的是主节点最近执行的写命令；时间较早的写命令会被挤出缓冲区。</p>
<p>由于该缓冲区长度固定且有限，因此可以备份的写命令也有限，当主从节点offset的差距过大超过缓冲区长度时，将无法执行部分复制，只能执行全量复制。反过来说，为了提高网络中断时部分复制执行的概率，可以根据需要增大复制积压缓冲区的大小(通过配置repl-backlog-size)；例如如果网络中断的平均时间是60s，而主节点平均每秒产生的写命令(特定协议格式)所占的字节数为100KB，则复制积压缓冲区的平均需求为6MB，保险起见，可以设置为12MB，来保证绝大多数断线情况都可以使用部分复制。</p>
<p>从节点将offset发送给主节点后，主节点根据offset和缓冲区大小决定能否执行部分复制：</p>
<ul>
<li>如果offset偏移量之后的数据，仍然都在复制积压缓冲区里，则执行部分复制；</li>
<li>如果offset偏移量之后的数据已不在复制积压缓冲区中（数据已被挤出），则执行全量复制。</li>
</ul>
<h4 id="（3）服务器运行ID-runid">（3）服务器运行ID(runid)</h4>
<p>每个Redis节点(无论主从)，在启动时都会自动生成一个随机ID(每次启动都不一样)，由40个随机的十六进制字符组成；runid用来唯一识别一个Redis节点。通过info Server命令，可以查看节点的runid：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/1174710-20180628011537662-712436367.png" class="" title="img">
<p>主从节点初次复制时，主节点将自己的runid发送给从节点，从节点将这个runid保存起来；当断线重连时，从节点会将这个runid发送给主节点；主节点根据runid判断能否进行部分复制：</p>
<ul>
<li>如果从节点保存的runid与主节点现在的runid相同，说明主从节点之前同步过，主节点会继续尝试使用部分复制(到底能不能部分复制还要看offset和复制积压缓冲区的情况)；</li>
<li>如果从节点保存的runid与主节点现在的runid不同，说明从节点在断线前同步的Redis节点并不是当前的主节点，只能进行全量复制。</li>
</ul>
<p><strong>【示例】</strong></p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/image-20210924155035634.png" class="" title="image-20210924155035634">
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/image-20210924155043945.png" class="" title="image-20210924155043945">
<h2 id="【命令传播阶段】心跳机制">【命令传播阶段】心跳机制</h2>
<p>在命令传播阶段，除了发送写命令，主从节点还维持着心跳机制：PING和REPLCONF ACK。心跳机制对于<strong>主从复制的超时判断、数据安全</strong>等有作用。</p>
<h3 id="1-主-从：PING">1.主-&gt;从：PING</h3>
<p>每隔指定的时间，<strong>主节点会向从节点发送PING命令</strong>，这个PING命令的作用，主要是为了让从节点进行超时判断。</p>
<p>PING发送的频率由repl-ping-slave-period参数控制，单位是秒，默认值是10s。</p>
<p>关于该PING命令究竟是由主节点发给从节点，还是相反，有一些争议；因为在Redis的官方文档中，对该参数的注释中说明是从节点向主节点发送PING命令，如下图所示：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/1174710-20180628011653835-25800141.png" class="" title="img">
<p>但是根据该参数的名称(含有ping-slave)，以及代码实现，我认为该PING命令是主节点发给从节点的。相关代码如下：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/1174710-20180628011700171-401488218.png" class="" title="img">
<h3 id="2-从-主：REPLCONF-ACK">2. 从-&gt;主：REPLCONF ACK</h3>
<p>在命令传播阶段，**从节点会向主节点发送REPLCONF ACK命令，**频率是每秒1次；命令格式为：REPLCONF ACK {offset}，其中offset指从节点保存的复制偏移量。REPLCONF ACK命令的作用包括：</p>
<p>（1）实时监测主从节点网络状态：该命令会被主节点用于复制超时的判断。此外，在主节点中使用info Replication，可以看到其从节点的状态中的lag值，代表的是主节点上次收到该REPLCONF ACK命令的时间间隔，在正常情况下，该值应该是0或1，如下图所示：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/1174710-20180628011708219-1385546367.png" class="" title="img">
<p>（2）检测命令丢失：从节点发送了自身的offset，主节点会与自己的offset对比，如果从节点数据缺失（如网络丢包），主节点会推送缺失的数据（这里也会利用复制积压缓冲区）。<strong>注意，offset和复制积压缓冲区，不仅可以用于部分复制，也可以用于处理命令丢失等情形；区别在于前者是在断线重连后进行的，而后者是在主从节点没有断线的情况下进行的。</strong></p>
<p>（3）辅助保证从节点的数量和延迟：Redis主节点中使用min-slaves-to-write和min-slaves-max-lag参数，来保证主节点在不安全的情况下不会执行写命令；所谓不安全，是指从节点数量太少，或延迟过高。例如min-slaves-to-write和min-slaves-max-lag分别是3和10，含义是如果从节点数量小于3个，或所有从节点的延迟值都大于10s，则主节点拒绝执行写命令。而这里从节点延迟值的获取，就是通过主节点接收到REPLCONF ACK命令的时间来判断的，即前面所说的info Replication中的lag值。</p>
<h2 id="【部分事故场景】">【部分事故场景】</h2>
<h3 id="1-复制超时问题">1.复制超时问题</h3>
<p>（1）数据同步阶段：在主从节点进行全量复制bgsave时，主节点需要首先fork子进程将当前数据保存到RDB文件中，然后再将RDB文件通过网络传输到从节点。如果RDB文件过大，主节点在fork子进程+保存RDB文件时耗时过多，可能会导致从节点长时间收不到数据而触发超时；此时从节点会重连主节点，然后再次全量复制，再次超时，再次重连……这是个悲伤的循环。为了避免这种情况的发生，除了注意Redis单机数据量不要过大，另一方面就是适当增大repl-timeout值，具体的大小可以根据bgsave耗时来调整。</p>
<p>（2）命令传播阶段：如前所述，在该阶段主节点会向从节点发送PING命令，频率由repl-ping-slave-period控制；该参数应明显小于repl-timeout值(后者至少是前者的几倍)。否则，如果两个参数相等或接近，网络抖动导致个别PING命令丢失，此时恰巧主节点也没有向从节点发送数据，则从节点很容易判断超时。</p>
<p>（3）慢查询导致的阻塞：如果主节点或从节点执行了一些慢查询（如keys *或者对大数据的hgetall等），导致服务器阻塞；阻塞期间无法响应复制连接中对方节点的请求，可能导致复制超时。</p>
<h3 id="2-复制缓冲区溢出问题">2.复制缓冲区溢出问题</h3>
<p>主从节点超时是复制中断的原因之一，除此之外，还有其他情况可能导致复制中断，其中最主要的是复制缓冲区溢出问题。</p>
<p>前面曾提到过，在全量复制阶段，主节点会将执行的写命令放到复制缓冲区中，该缓冲区存放的数据包括了以下几个时间段内主节点执行的写命令：bgsave生成RDB文件、RDB文件由主节点发往从节点、从节点清空老数据并载入RDB文件中的数据。当主节点数据量较大，或者主从节点之间网络延迟较大时，可能导致该缓冲区的大小超过了限制，此时主节点会断开与从节点之间的连接；这种情况可能引起全量复制-&gt;复制缓冲区溢出导致连接中断-&gt;重连-&gt;全量复制-&gt;复制缓冲区溢出导致连接中断……的循环。</p>
<p>复制缓冲区的大小由client-output-buffer-limit slave {hard limit} {soft limit} {soft seconds}配置，默认值为client-output-buffer-limit slave 256MB 64MB 60，其含义是：如果buffer大于256MB，或者连续60s大于64MB，则主节点会断开与该从节点的连接。该参数是可以通过config set命令动态配置的（即不重启Redis也可以生效）。</p>
<p>当复制缓冲区溢出时，主节点打印日志如下所示：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/1174710-20180628012000540-1190099498.png" class="" title="img">
<p><strong>需要注意的是，复制缓冲区是客户端输出缓冲区的一种，主节点会为每一个从节点分别分配复制缓冲区；而复制积压缓冲区则是一个主节点只有一个，无论它有多少个从节点。</strong></p>
<h3 id="3-第一次建立复制选择">3.第一次建立复制选择</h3>
<p>此时全量复制不可避免，但仍有几点需要注意：如果主节点的数据量较大，应该尽量避开流量的高峰期，避免造成阻塞；如果有多个从节点需要建立对主节点的复制，可以考虑将几个从节点错开，避免主节点带宽占用过大。此外，如果从节点过多，也可以调整主从复制的拓扑结构，由一主多从结构变为树状结构（中间的节点既是其主节点的从节点，也是其从节点的主节点）；但使用树状结构应该谨慎：虽然主节点的直接从节点减少，降低了主节点的负担，但是多层从节点的延迟增大，数据一致性变差；且结构复杂，维护相当困难。</p>
<h3 id="4-主节点重启">4.主节点重启</h3>
<p>主节点重启可以分为两种情况来讨论，一种是故障导致宕机，另一种则是有计划的重启。</p>
<p><strong>主节点宕机</strong></p>
<p>主节点宕机重启后，runid会发生变化，因此不能进行部分复制，只能全量复制。</p>
<p>实际上在主节点宕机的情况下，应进行故障转移处理，将其中的一个从节点升级为主节点，其他从节点从新的主节点进行复制；且故障转移应尽量的自动化，后面文章将要介绍的哨兵便可以进行自动的故障转移。</p>
<p><strong>安全重启：debug reload</strong></p>
<p>在一些场景下，可能希望对主节点进行重启，例如主节点内存碎片率过高，或者希望调整一些只能在启动时调整的参数。如果使用普通的手段重启主节点，会使得runid发生变化，可能导致不必要的全量复制。</p>
<p>为了解决这个问题，Redis提供了debug reload的重启方式：<strong>重启后，主节点的runid和offset都不受影响，避免了全量复制</strong>。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/1174710-20180628012018427-1532559550.png" class="" title="img">
<p>但debug reload是一柄双刃剑：<strong>它会清空当前内存中的数据，重新从RDB文件中加载</strong>，这个过程会导致主节点的阻塞，因此也需要谨慎。</p>
<h3 id="5-网络中断">5.网络中断</h3>
<p>如果主从节点之间出现网络问题，造成短时间内网络中断，可以分为多种情况讨论。</p>
<p>第一种情况：网络问题时间极为短暂，只造成了短暂的丢包，主从节点都没有判定超时（未触发repl-timeout）；此时只需要通过REPLCONF ACK来补充丢失的数据即可。</p>
<p>第二种情况：网络问题时间很长，主从节点判断超时（触发了repl-timeout），且丢失的数据过多，超过了复制积压缓冲区所能存储的范围；此时主从节点无法进行部分复制，只能进行全量复制。为了尽可能避免这种情况的发生，应该根据实际情况适当调整复制积压缓冲区的大小；此外及时发现并修复网络中断，也可以减少全量复制。</p>
<p>第三种情况：介于前述两种情况之间，主从节点判断超时，且丢失的数据仍然都在复制积压缓冲区中；此时主从节点可以进行部分复制。</p>
<h3 id="6-单机内存大小的限制">6.单机内存大小的限制</h3>
<p>fork操作对Redis单机内存大小的限制。实际上在Redis的使用中，限制单机内存大小的因素非常之多，下面总结一下在主从复制中，单机内存过大可能造成的影响：</p>
<p>（1）切主：当主节点宕机时，一种常见的容灾策略是将其中一个从节点提升为主节点，并将其他从节点挂载到新的主节点上，此时这些从节点只能进行全量复制；如果Redis单机内存达到10GB，一个从节点的同步时间在几分钟的级别；如果从节点较多，恢复的速度会更慢。如果系统的读负载很高，而这段时间从节点无法提供服务，会对系统造成很大的压力。</p>
<p>（2）从库扩容：如果访问量突然增大，此时希望增加从节点分担读负载，如果数据量过大，从节点同步太慢，难以及时应对访问量的暴增。</p>
<p>（3）缓冲区溢出：（1）和（2）都是从节点可以正常同步的情形（虽然慢），但是如果数据量过大，导致全量复制阶段主节点的复制缓冲区溢出，从而导致复制中断，则主从节点的数据同步会全量复制-&gt;复制缓冲区溢出导致复制中断-&gt;重连-&gt;全量复制-&gt;复制缓冲区溢出导致复制中断……的循环。</p>
<p>（4）超时：如果数据量过大，全量复制阶段主节点fork+保存RDB文件耗时过大，从节点长时间接收不到数据触发超时，主从节点的数据同步同样可能陷入全量复制-&gt;超时导致复制中断-&gt;重连-&gt;全量复制-&gt;超时导致复制中断……的循环。</p>
<p>此外，主节点单机内存除了绝对量不能太大，其占用主机内存的比例也不应过大：最好只使用50%-65%的内存，留下30%-45%的内存用于执行bgsave命令和创建复制缓冲区等。</p>
<h2 id="【总结】">【总结】</h2>
<p>1、主从复制的作用：宏观的了解主从复制是为了解决什么样的问题，即数据冗余、故障恢复、读负载均衡等。</p>
<p>2、主从复制的操作：即slaveof命令。</p>
<p>3、主从复制的原理：主从复制包括了连接建立阶段、数据同步阶段、命令传播阶段；其中数据同步阶段，有全量复制和部分复制两种数据同步方式；命令传播阶段，主从节点之间有PING和REPLCONF ACK命令互相进行心跳检测。</p>
<p>4、应用中的问题：包括读写分离的问题（数据不一致问题、数据过期问题、故障切换问题等）、复制超时问题、复制中断问题等，然后总结了主从复制相关的配置，其中repl-timeout、client-output-buffer-limit slave等对解决Redis主从复制中出现的问题可能会有帮助。</p>
<p>主从复制虽然解决或缓解了数据冗余、故障恢复、读负载均衡等问题，但其缺陷仍很明显：故障恢复无法自动化；写操作无法负载均衡；存储能力受到单机的限制；这些问题的解决，需要哨兵和集群的帮助</p>
<h1>2.Redis哨兵</h1>
<h2 id="【概念】">【概念】</h2>
<p>Redis Sentinel，即Redis哨兵，在Redis 2.8版本开始引入。**哨兵的核心功能是主节点的自动故障转移。**下面是Redis官方文档对于哨兵功能的描述：</p>
<ul>
<li>监控（Monitoring）：哨兵会不断地检查主节点和从节点是否运作正常。</li>
<li>自动故障转移（Automatic failover）：当主节点不能正常工作时，哨兵会开始自动故障转移操作，它会将失效主节点的其中一个从节点升级为新的主节点，并让其他从节点改为复制新的主节点。</li>
<li>配置提供者（Configuration provider）：客户端在初始化时，通过连接哨兵来获得当前Redis服务的主节点地址。</li>
<li>通知（Notification）：哨兵可以将故障转移的结果发送给客户端。</li>
</ul>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/1174710-20180908182924632-1069251418.png" class="" title="img">
<p>它由两部分组成，哨兵节点和数据节点：</p>
<ul>
<li>哨兵节点：哨兵系统由一个或多个哨兵节点组成，哨兵节点是特殊的redis节点，不存储数据。</li>
<li>数据节点：主节点和从节点都是数据节点。</li>
</ul>
<h2 id="【故障转移】">【故障转移】</h2>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/kismetv/p/9609938.html">https://www.cnblogs.com/kismetv/p/9609938.html</a></p>
<p>哨兵系统的搭建过程，有几点需要注意：</p>
<p>（1）哨兵系统中的主从节点，与普通的主从节点并没有什么区别，故障发现和转移是由哨兵来控制和完成的。</p>
<p>（2）哨兵节点本质上是redis节点。</p>
<p>（3）每个哨兵节点，只需要配置监控主节点，便可以自动发现其他的哨兵节点和从节点。</p>
<p>（4）在哨兵节点启动和故障转移阶段，各个节点的配置文件会被重写(config rewrite)。</p>
<p>（5）本章的例子中，一个哨兵只监控了一个主节点；实际上，一个哨兵可以监控多个主节点，通过配置多条sentinel monitor即可实现。</p>
<h2 id="【基本原理】">【基本原理】</h2>
<p>关于哨兵的原理，关键是了解以下几个概念。</p>
<p>（1）定时任务：每个哨兵节点维护了3个定时任务。定时任务的功能分别如下：</p>
<ul>
<li>通过向主从节点发送info命令获取最新的主从结构；</li>
<li>通过发布订阅功能获取其他哨兵节点的信息；</li>
<li>通过向其他节点发送ping命令进行心跳检测，判断是否下线。</li>
</ul>
<p>（2）主观下线：在心跳检测的定时任务中，如果其他节点超过一定时间没有回复，哨兵节点就会将其进行主观下线。顾名思义，主观下线的意思是一个哨兵节点“主观地”判断下线；与主观下线相对应的是客观下线。</p>
<p>（3）客观下线：哨兵节点在对主节点进行主观下线后，会通过sentinel is-master-down-by-addr命令询问其他哨兵节点该主节点的状态；如果判断主节点下线的哨兵数量达到一定数值，则对该主节点进行客观下线。</p>
<p><strong>需要特别注意的是，客观下线是主节点才有的概念；如果从节点和哨兵节点发生故障，被哨兵主观下线后，不会再有后续的客观下线和故障转移操作。</strong></p>
<p>（4）选举领导者哨兵节点：当主节点被判断客观下线以后，各个哨兵节点会进行协商，选举出一个领导者哨兵节点，并由该领导者节点对其进行故障转移操作。</p>
<p>监视该主节点的所有哨兵都有可能被选为领导者，选举使用的算法是Raft算法；Raft算法的基本思路是先到先得：即在一轮选举中，哨兵A向B发送成为领导者的申请，如果B没有同意过其他哨兵，则会同意A成为领导者。选举的具体过程这里不做详细描述，一般来说，哨兵选择的过程很快，谁先完成客观下线，一般就能成为领导者。</p>
<p>（5）故障转移：选举出的领导者哨兵，开始进行故障转移操作，该操作大体可以分为3个步骤：</p>
<ul>
<li>
<p>在从节点中选择新的主节点：选择的原则是，首先过滤掉不健康的从节点；然后选择优先级最高的从节点(由slave-priority指定)；如果优先级无法区分，则选择复制偏移量最大的从节点；如果仍无法区分，则选择runid最小的从节点。</p>
</li>
<li>
<p>更新主从状态：通过slaveof no one命令，让选出来的从节点成为主节点；并通过slaveof命令让其他节点成为其从节点。</p>
</li>
<li>
<p>将已经下线的主节点(即6379)设置为新的主节点的从节点，当6379重新上线后，它会成为新的主节点的从节点。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/1174710-20180909004056625-1501495024.png" class="" title="img">
</li>
</ul>
<h2 id="【实践建议】">【实践建议】</h2>
<p>1）哨兵节点的数量应不止一个，一方面增加哨兵节点的冗余，避免哨兵本身成为高可用的瓶颈；另一方面减少对下线的误判。此外，这些不同的哨兵节点应部署在不同的物理机上。</p>
<p>（2）哨兵节点的数量应该是奇数，便于哨兵通过投票做出“决策”：领导者选举的决策、客观下线的决策等。</p>
<p>（3）各个哨兵节点的配置应一致，包括硬件、参数等；此外，所有节点都应该使用ntp或类似服务，保证时间准确、一致。</p>
<p>（4）哨兵的配置提供者和通知客户端功能，需要客户端的支持才能实现，如前文所说的Jedis；如果开发者使用的库未提供相应支持，则可能需要开发者自己实现。</p>
<p>（5）当哨兵系统中的节点在docker（或其他可能进行端口映射的软件）中部署时，应特别注意端口映射可能会导致哨兵系统无法正常工作，因为哨兵的工作基于与其他节点的通信，而docker的端口映射可能导致哨兵无法连接到其他节点。例如，哨兵之间互相发现，依赖于它们对外宣称的IP和port，如果某个哨兵A部署在做了端口映射的docker中，那么其他哨兵使用A宣称的port无法连接到A。</p>
<h2 id="【总结】-2">【总结】</h2>
<p>本文首先介绍了哨兵的作用：监控、故障转移、配置提供者和通知；然后讲述了哨兵系统的部署方法，以及通过客户端访问哨兵系统的方法；再然后简要说明了哨兵实现的基本原理；最后给出了关于哨兵实践的一些建议。</p>
<p>在主从复制的基础上，哨兵引入了主节点的自动故障转移，进一步提高了Redis的高可用性；但是哨兵的缺陷同样很明显：哨兵无法对从节点进行自动故障转移，在读写分离场景下，从节点故障会导致读服务不可用，需要我们对从节点做额外的监控、切换操作。</p>
<p>此外，哨兵仍然没有解决写操作无法负载均衡、及存储能力受到单机限制的问题</p>
<h1>3.集群</h1>
<h2 id="【作用】">【作用】</h2>
<p>集群，即Redis Cluster，是Redis 3.0开始引入的分布式存储方案。</p>
<p>集群由多个节点(Node)组成，Redis的数据分布在这些节点中。集群中的节点分为主节点和从节点：只有主节点负责读写请求和集群信息的维护；从节点只进行主节点数据和状态信息的复制。</p>
<p>集群的作用，可以归纳为两点：</p>
<p>1、数据分区：数据分区(或称数据分片)是集群最核心的功能。</p>
<p>集群将数据分散到多个节点，一方面突破了Redis单机内存大小的限制，存储容量大大增加；另一方面每个主节点都可以对外提供读服务和写服务，大大提高了集群的响应能力。</p>
<p>Redis单机内存大小受限问题，在介绍持久化和主从复制时都有提及；例如，如果单机内存太大，bgsave和bgrewriteaof的fork操作可能导致主进程阻塞，主从环境下主机切换时可能导致从节点长时间无法提供服务，全量复制阶段主节点的复制缓冲区可能溢出……。</p>
<p>2、高可用：集群支持主从复制和主节点的自动故障转移（与哨兵类似）；当任一节点发生故障时，集群仍然可以对外提供服务。</p>
<h2 id="【集群方案设计】">【集群方案设计】</h2>
<p>设计集群方案时，至少要考虑以下因素：</p>
<p>（1）高可用要求：根据故障转移的原理，至少需要3个主节点才能完成故障转移，且3个主节点不应在同一台物理机上；每个主节点至少需要1个从节点，且主从节点不应在一台物理机上；因此高可用集群至少包含6个节点。</p>
<p>（2）数据量和访问量：估算应用需要的数据量和总访问量(考虑业务发展，留有冗余)，结合每个主节点的容量和能承受的访问量(可以通过benchmark得到较准确估计)，计算需要的主节点数量。</p>
<p>（3）节点数量限制：Redis官方给出的节点数量限制为1000，主要是考虑节点间通信带来的消耗。在实际应用中应尽量避免大集群；如果节点数量不足以满足应用对Redis数据量和访问量的要求，可以考虑：(1)业务分割，大集群分为多个小集群；(2)减少不必要的数据；(3)调整数据过期策略等。</p>
<p>（4）适度冗余：Redis可以在不影响集群服务的情况下增加节点，因此节点数量适当冗余即可，不用太大。</p>
<h2 id="【基本原理】-2">【基本原理】</h2>
<p><strong>集群最核心的功能是数据分区，因此首先介绍数据的分区规则；然后介绍集群实现的细节：通信机制和数据结构；最后以cluster meet(节点握手)、cluster addslots(槽分配)为例，说明节点是如何利用上述数据结构和通信机制实现集群命令的。</strong></p>
<h3 id="1-数据分区方案">1.数据分区方案</h3>
<p>数据分区有顺序分区、哈希分区等，其中哈希分区由于其天然的随机性，使用广泛；集群的分区方案便是哈希分区的一种。</p>
<p>哈希分区的基本思路是：对数据的特征值（如key）进行哈希，然后根据哈希值决定数据落在哪个节点。常见的哈希分区包括：</p>
<ul>
<li>
<p>哈希取余分区</p>
</li>
<li>
<p>一致性哈希分区</p>
</li>
<li>
<p>带虚拟节点的一致性哈希分区等。</p>
</li>
</ul>
<p>衡量数据分区方法好坏的标准有很多，其中比较重要的两个因素是(1)数据分布是否均匀(2)增加或删减节点对数据分布的影响。由于哈希的随机性，哈希分区基本可以保证数据分布均匀；因此在比较哈希分区方案时，重点要看增减节点对数据分布的影响。</p>
<p>（1）哈希取余分区</p>
<p>哈希取余分区思路非常简单：计算key的hash值，然后对节点数量进行取余，从而决定数据映射到哪个节点上。该方案最大的问题是，当新增或删减节点时，节点数量发生变化，系统中所有的数据都需要重新计算映射关系，引发大规模数据迁移。</p>
<p>（2）一致性哈希分区</p>
<p>一致性哈希算法将整个哈希值空间组织成一个虚拟的圆环，如下图所示，范围为0-2^32-1；对于每个数据，根据key计算hash值，确定数据在环上的位置，然后从此位置沿环顺时针行走，找到的第一台服务器就是其应该映射到的服务器。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/1174710-20181025213424713-1246878063.png" class="" title="img">
<p>与哈希取余分区相比，一致性哈希分区将增减节点的影响限制在相邻节点。以上图为例，如果在node1和node2之间增加node5，则只有node2中的一部分数据会迁移到node5；如果去掉node2，则原node2中的数据只会迁移到node4中，只有node4会受影响。</p>
<p>一致性哈希分区的主要问题在于，当节点数量较少时，增加或删减节点，对单个节点的影响可能很大，造成数据的严重不平衡。还是以上图为例，如果去掉node2，node4中的数据由总数据的1/4左右变为1/2左右，与其他节点相比负载过高。</p>
<p>（3）带虚拟节点的一致性哈希分区</p>
<p>该方案在一致性哈希分区的基础上，引入了虚拟节点的概念。**Redis<strong><strong>集群使用的便是该方案，其中的虚拟节点称为槽（slot</strong></strong>）。**槽是介于数据和实际节点之间的虚拟概念；每个实际节点包含一定数量的槽，每个槽包含哈希值在一定范围内的数据。引入槽以后，数据的映射关系由数据hash-&gt;实际节点，变成了数据hash-&gt;槽-&gt;实际节点。</p>
<p>**在使用了槽的一致性哈希分区中，槽是数据管理和迁移的基本单位。槽解耦了数据和实际节点之间的关系，增加或删除节点对系统的影响很小。**仍以上图为例，系统中有4个实际节点，假设为其分配16个槽(0-15)； 槽0-3位于node1，4-7位于node2，以此类推。如果此时删除node2，只需要将槽4-7重新分配即可，例如槽4-5分配给node1，槽6分配给node3，槽7分配给node4；可以看出删除node2后，数据在其他节点的分布仍然较为均衡。</p>
<p>槽的数量一般远小于2^32，远大于实际节点的数量；在Redis集群中，槽的数量为16384。</p>
<p>下面这张图很好的总结了Redis集群将数据映射到实际节点的过程：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/1174710-20190802191100758-1110624103.png" class="" title="img">
<p>（1）Redis对数据的特征值（一般是key）计算哈希值，使用的算法是CRC16。</p>
<p>（2）根据哈希值，计算数据属于哪个槽。</p>
<p>（3）根据槽与节点的映射关系，计算数据属于哪个节点。</p>
<h3 id="3-节点通信机制">3.节点通信机制</h3>
<h4 id="两个端口">两个端口</h4>
<p>在哨兵系统中，节点分为数据节点和哨兵节点：前者存储数据，后者实现额外的控制功能。在集群中，没有数据节点与非数据节点之分：所有的节点都存储数据，也都参与集群状态的维护。为此，集群中的每个节点，都提供了两个TCP端口：</p>
<ul>
<li>普通端口：即我们在前面指定的端口(7000等)。普通端口主要用于为客户端提供服务（与单机节点类似）；但在节点间数据迁移时也会使用。</li>
<li>集群端口：端口号是普通端口+10000（10000是固定值，无法改变），如7000节点的集群端口为17000。集群端口只用于节点之间的通信，如搭建集群、增减节点、故障转移等操作时节点间的通信；不要使用客户端连接集群接口。为了保证集群可以正常工作，在配置防火墙时，要同时开启普通端口和集群端口。</li>
</ul>
<h4 id="Gossip协议">Gossip协议</h4>
<p>节点间通信，按照通信协议可以分为几种类型：单对单、广播、Gossip协议等。重点是广播和Gossip的对比。</p>
<p>广播是指向集群内所有节点发送消息；优点是集群的收敛速度快(集群收敛是指集群内所有节点获得的集群信息是一致的)，缺点是每条消息都要发送给所有节点，CPU、带宽等消耗较大。</p>
<p>Gossip协议的特点是：在节点数量有限的网络中，每个节点都“随机”的与部分节点通信（并不是真正的随机，而是根据特定的规则选择通信的节点），经过一番杂乱无章的通信，每个节点的状态很快会达到一致。Gossip协议的优点有负载(比广播)低、去中心化、容错性高(因为通信有冗余)等；缺点主要是集群的收敛速度慢。</p>
<h4 id="消息类型">消息类型</h4>
<p>集群中的节点采用固定频率（每秒10次）的定时任务进行通信相关的工作：判断是否需要发送消息及消息类型、确定接收节点、发送消息等。如果集群状态发生了变化，如增减节点、槽状态变更，通过节点间的通信，所有节点会很快得知整个集群的状态，使集群收敛。</p>
<p>节点间发送的消息主要分为5种：meet消息、ping消息、pong消息、fail消息、publish消息。不同的消息类型，通信协议、发送的频率和时机、接收节点的选择等是不同的。</p>
<ul>
<li>MEET消息：在节点握手阶段，当节点收到客户端的CLUSTER MEET命令时，会向新加入的节点发送MEET消息，请求新节点加入到当前集群；新节点收到MEET消息后会回复一个PONG消息。</li>
<li>PING消息：集群里每个节点每秒钟会选择部分节点发送PING消息，接收者收到消息后会回复一个PONG消息。PING消息的内容是自身节点和部分其他节点的状态信息；作用是彼此交换信息，以及检测节点是否在线。PING消息使用Gossip协议发送，接收节点的选择兼顾了收敛速度和带宽成本，具体规则如下：(1)随机找5个节点，在其中选择最久没有通信的1个节点(2)扫描节点列表，选择最近一次收到PONG消息时间大于cluster_node_timeout/2的所有节点，防止这些节点长时间未更新。</li>
<li>PONG消息：PONG消息封装了自身状态数据。可以分为两种：第一种是在接到MEET/PING消息后回复的PONG消息；第二种是指节点向集群广播PONG消息，这样其他节点可以获知该节点的最新信息，例如故障恢复后新的主节点会广播PONG消息。</li>
<li>FAIL消息：当一个主节点判断另一个主节点进入FAIL状态时，会向集群广播这一FAIL消息；接收节点会将这一FAIL消息保存起来，便于后续的判断。</li>
<li>PUBLISH消息：节点收到PUBLISH命令后，会先执行该命令，然后向集群广播这一消息，接收节点也会执行该PUBLISH命令。</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/dajiangdahe.github.io/tags/Redis/" rel="tag">Redis</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Redis持久化"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/dajiangdahe.github.io/2021/10/22/Redis%E6%8C%81%E4%B9%85%E5%8C%96/"
    >Redis持久化</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/dajiangdahe.github.io/2021/10/22/Redis%E6%8C%81%E4%B9%85%E5%8C%96/" class="article-date">
  <time datetime="2021-10-22T07:21:45.058Z" itemprop="datePublished">2021-10-22</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/dajiangdahe.github.io/categories/Redis/">Redis</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>[TOC]</p>
<h1>1.Redis高可用概述</h1>
<p>在介绍Redis高可用之前，先说明一下在Redis的语境中高可用的含义。</p>
<p>我们知道，在web服务器中，高可用是指服务器可以正常访问的时间，衡量的标准是在多长时间内可以提供正常服务（99.9%、99.99%、99.999% 等等）。但是在Redis语境中，高可用的含义似乎要宽泛一些，除了保证提供正常服务(如主从分离、快速容灾技术)，还需要考虑数据容量的扩展、数据安全不会丢失等。</p>
<p>在Redis中，实现高可用的技术主要包括持久化、复制、哨兵和集群，下面分别说明它们的作用，以及解决了什么样的问题。</p>
<ol>
<li><strong>持久化</strong>：持久化是最简单的高可用方法(有时甚至不被归为高可用的手段)，主要作用是数据备份，即将数据存储在硬盘，保证数据不会因进程退出而丢失。</li>
<li><strong>复制</strong>：复制是高可用Redis的基础，哨兵和集群都是在复制基础上实现高可用的。复制主要实现了数据的多机备份，以及对于读操作的负载均衡和简单的故障恢复。缺陷：故障恢复无法自动化；写操作无法负载均衡；存储能力受到单机的限制。</li>
<li><strong>哨兵</strong>：在复制的基础上，哨兵实现了自动化的故障恢复。缺陷：写操作无法负载均衡；存储能力受到单机的限制。</li>
<li><strong>集群</strong>：通过集群，Redis解决了写操作无法负载均衡，以及存储能力受到单机限制的问题，实现了较为完善的高可用方案。</li>
</ol>
<h1>2.Redis持久化概述</h1>
<p>持久化的功能：Redis是内存数据库，数据都是存储在内存中，为了避免进程退出导致数据的永久丢失，需要定期将Redis中的数据以某种形式(数据或命令)从内存保存到硬盘；当下次Redis重启时，利用持久化文件实现数据恢复。除此之外，为了进行灾难备份，可以将持久化文件拷贝到一个远程位置。</p>
<p>Redis持久化分为RDB持久化和AOF持久化**：前者将当前数据保存到硬盘，后者则是将每次执行的==写命令==保存到硬盘（类似于MySQL的binlog）；**由于AOF持久化的实时性更好，即当进程意外退出时丢失的数据更少，因此AOF是目前主流的持久化方式，不过RDB持久化仍然有其用武之地。</p>
<h2 id="RDB持久化">RDB持久化</h2>
<p>RDB持久化是将当前进程中的数据生成快照保存到硬盘(因此也称作快照持久化)，保存的文件后缀是rdb；当Redis重新启动时，可以读取快照文件恢复数据。</p>
<h3 id="1-触发条件">1.触发条件</h3>
<p>RDB持久化的触发分为手动触发和自动触发两种。</p>
<p><strong>save</strong>和<strong>bgsave</strong>的区别</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E6%8C%81%E4%B9%85%E5%8C%96/1c950a7b02087bf43b4490d50ac25f2a11dfcf7e.jpeg" class="" title="img">
<h4 id="1-手动触发">(1)手动触发</h4>
<p><strong>save</strong>命令和<strong>bgsave</strong>命令都可以生成RDB文件。</p>
<p><strong>save</strong>命令会阻塞Redis服务器进程，直到RDB文件创建完毕为止，在Redis服务器阻塞期间，服务器不能处理任何命令请求。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E6%8C%81%E4%B9%85%E5%8C%96/1174710-20180605085242889-137050862.png" class="" title="img">
<p>而<strong>bgsave</strong>命令会创建一个子进程，由子进程来负责创建RDB文件，父进程(即Redis主进程)则继续处理请求。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E6%8C%81%E4%B9%85%E5%8C%96/1174710-20180605085309364-1576452765.png" class="" title="img">
<p>此时服务器执行日志如下：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E6%8C%81%E4%B9%85%E5%8C%96/1174710-20180605085325656-76060516.png" class="" title="img">
<p>bgsave命令执行过程中，只有fork子进程时会阻塞服务器，而对于save命令，整个过程都会阻塞服务器，因此save已基本被废弃，线上环境要杜绝save的使用；后文中也将只介绍bgsave命令。此外，在自动触发RDB持久化时，Redis也会选择bgsave而不是save来进行持久化；下面介绍自动触发RDB持久化的条件。</p>
<h4 id="2-自动触发">(2).自动触发</h4>
<p><strong>save m n</strong>:自动触发最常见的情况是在配置文件中通过save m n，指定当m秒内发生n次变化时，会触发bgsave。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E6%8C%81%E4%B9%85%E5%8C%96/1174710-20180605085420533-1928501600.png" class="" title="img">
<p>除了<strong>save m n</strong> 以外，还有一些其他情况会触发bgsave：</p>
<ul>
<li>在主从复制场景下，如果从节点执行全量复制操作，则主节点会执行bgsave命令，并将rdb文件发送给从节点</li>
<li>执行shutdown命令时，自动执行rdb持久化，如下图所示：</li>
</ul>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E6%8C%81%E4%B9%85%E5%8C%96/1174710-20180605085620830-1223048825.png" class="" title="img">
<h3 id="2-执行流程">2.执行流程</h3>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E6%8C%81%E4%B9%85%E5%8C%96/1174710-20180605085813461-389677620.png" class="" title="img">
<p>图片中的5个步骤所进行的操作如下：</p>
<ol>
<li>
<p>Redis父进程首先判断：当前是否在执行save，或bgsave/bgrewriteaof（后面会详细介绍该命令）的子进程，如果在执行则bgsave命令直接返回。bgsave/bgrewriteaof 的子进程不能同时执行，主要是基于性能方面的考虑：两个并发的子进程同时执行大量的磁盘写操作，可能引起严重的性能问题。</p>
</li>
<li>
<p>父进程执行fork操作创建子进程，这个过程中父进程是阻塞的，Redis不能执行来自客户端的任何命令</p>
</li>
<li>
<p>父进程fork后，bgsave命令返回”Background saving started”信息并不再阻塞父进程，并可以响应其他命令</p>
</li>
<li>
<p>子进程创建RDB文件，根据父进程内存快照生成临时快照文件，完成后对原有文件进行原子替换</p>
</li>
<li>
<p>子进程发送信号给父进程表示完成，父进程更新统计信息</p>
</li>
</ol>
<h3 id="3-RDB优势与劣势">3.RDB优势与劣势</h3>
<p>①、优势</p>
<p>（1）RDB文件紧凑，全量备份，非常适合用于进行备份和灾难恢复。</p>
<p>（2）生成RDB文件的时候，redis主进程会fork()一个子进程来处理所有保存工作，主进程不需要进行任何磁盘IO操作。</p>
<p>（3）RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。</p>
<p>②、劣势</p>
<p>RDB快照是一次全量备份，存储的是内存数据的二进制序列化形式，存储上非常紧凑。当进行快照持久化时，会开启一个子进程专门负责快照持久化，子进程会拥有父进程的内存数据，父进程修改内存子进程不会反应出来，所以在快照持久化期间修改的数据不会被保存，可能丢失数据。</p>
<h2 id="AOF机制">AOF机制</h2>
<p>全量备份总是耗时的，有时候我们提供一种更加高效的方式AOF，工作机制很简单，redis会将每一个收到的写命令都通过write函数追加到文件中。通俗的理解就是日志记录。</p>
<h3 id="1-持久化原理">1.持久化原理</h3>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E6%8C%81%E4%B9%85%E5%8C%96/32fa828ba61ea8d3c2502e396b1b3848251f58b0.jpeg" class="" title="img">
<p>每当有一个<strong>写命令</strong>过来时，就直接保存到我们的AOF中</p>
<h3 id="2-文件重写原理">2.文件重写原理</h3>
<p>AOF的方式也同时带来了另一个问题。持久化文件会变的越来越大。为了压缩aof的持久化文件。redis提供了bgrewriteaof命令。将内存中的数据以命令的方式保存到临时文件中，同时会fork出一条新进程来将文件重写。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E6%8C%81%E4%B9%85%E5%8C%96/09fa513d269759ee28454d2c4cea4b106c22dfd3.jpeg" class="" title="img">
<p>重写AOF文件的操作，并没有读取旧的AOF文件，而是将整个内存中的数据库内容用命令的方式重写了一个新的AOF文件，这点和快照有点类似。</p>
<h3 id="3-触发机制">3.触发机制</h3>
<p>（1）每修改同步always：同步持久化 每次发生数据变更会被立即记录到磁盘 性能较差但数据完整性比较好</p>
<p>（2）每秒同步everysec：异步操作，每秒记录 如果一秒内宕机，有数据丢失</p>
<p>（3）不同no：从不同步</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E6%8C%81%E4%B9%85%E5%8C%96/b17eca8065380cd7df69859ba056a5325982816c.jpeg" class="" title="img">
<h3 id="4-AOF优缺点">4.AOF优缺点</h3>
<p>①、优势</p>
<p>（1）AOF可以更好的保护数据不丢失，一般AOF会每隔1秒，通过一个后台线程执行一次fsync操作，最多丢失1秒钟的数据。</p>
<p>（2）AOF日志文件没有任何磁盘寻址的开销，写入性能非常高，文件不容易破损。</p>
<p>（3）AOF日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。</p>
<p>（4）AOF日志文件的命令通过非常可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据</p>
<p>②、劣势</p>
<p>（1）对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大</p>
<p>（2）AOF开启后，支持的写QPS会比RDB支持的写QPS低，因为AOF一般会配置成每秒fsync一次日志文件，当然，每秒一次fsync，性能也还是很高的</p>
<p>（3）以前AOF发生过bug，就是通过AOF记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。</p>
<h2 id="RDB和AOF到底该如何选择">RDB和AOF到底该如何选择</h2>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E6%8C%81%E4%B9%85%E5%8C%96/8326cffc1e178a82c532308ef2117b8ba977e8ae.jpeg" class="" title="img"> 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/dajiangdahe.github.io/tags/Redis/" rel="tag">Redis</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Redis内存策略"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E7%AD%96%E7%95%A5/"
    >Redis内存策略</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E7%AD%96%E7%95%A5/" class="article-date">
  <time datetime="2021-10-22T07:21:45.013Z" itemprop="datePublished">2021-10-22</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/dajiangdahe.github.io/categories/Redis/">Redis</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1>Redis内存淘汰策略</h1>
<p>当 redis 的内存空间（maxmemory 参数配置）已经用满时，redis 将根据配置的驱逐策略（maxmemory-policy 参数配置），进行相应的动作。在redis4.0之后，增加了两种基于LFU算法实现的内存淘汰策略</p>
<ul>
<li><strong>noeviction</strong>：默认策略，不淘汰任何 key，直接返回错误</li>
<li><strong>allkeys</strong>-<strong>lru</strong>：在所有的 key 中，使用 LRU 算法淘汰部分 key</li>
<li><strong>allkeys</strong>-<strong>lfu</strong>：在所有的 key 中，使用 LFU 算法淘汰部分 key，该算法于 Redis 4.0 新增</li>
<li><strong>allkeys</strong>-<strong>random</strong>：在所有的 key 中，随机淘汰部分 key</li>
<li><strong>volatile</strong>-<strong>lru</strong>：在设置了过期时间的 key 中，使用 LRU 算法淘汰部分 key</li>
<li><strong>volatile</strong>-<strong>lfu</strong>：在设置了过期时间的 key 中，使用 LFU 算法淘汰部分 key，该算法于 Redis 4.0 新增</li>
<li><strong>volatile</strong>-<strong>random</strong>：在设置了过期时间的 key 中，随机淘汰部分 key</li>
<li><strong>volatile</strong>-<strong>ttl</strong>：在设置了过期时间的 key 中，挑选 TTL（time to live，剩余时间）短的 key 淘汰</li>
</ul>
<h3 id="LRU算法">LRU算法</h3>
<p>在操作系统中<strong>LRU</strong>算法淘汰的<strong>不是内存中的对象</strong>，而是页,当内存中数据不足时，通过LRU算法，选择<strong>一页</strong>(一般是4KB)将其交换到虚拟内存区(<strong>Swap</strong>区)。</p>
<p>默认情况下，Redis随机挑选5个键，并且从中选取一个最近最久未使用的key进行淘汰，在配置文件中可以通过maxmemory-samples的值来设置redis需要检查key的个数,但是栓查的越多，耗费的时间也就越久,但是结构越精确</p>
<p>详解：<a target="_blank" rel="noopener" href="https://blog.csdn.net/kuizhu7142/article/details/81115750">https://blog.csdn.net/kuizhu7142/article/details/81115750</a></p>
<p>【<strong>命中率</strong>】</p>
<p>当存在热点数据时，LRU的效率很好，但偶发性的、周期性的批量操作会导致LRU命中率急剧下降，缓存污染情况比较严重。</p>
<p>【<strong>复杂度</strong>】</p>
<p>实现简单。</p>
<p>【<strong>代价</strong>】</p>
<p>命中时需要遍历链表，找到命中的数据块索引，然后需要将数据移到头部。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E7%AD%96%E7%95%A5/aHR0cDovL2ltYWdlcy5jbml0YmxvZy5jb20vYmxvZzIwMTUvNDcxNTMyLzIwMTUwMy8yNTE5NTQzNDk3NDIyMjAucG5n" class="" title="在这里插入图片描述">
<ul>
<li>最开始时，内存空间是空的，因此依次进入A、B、C是没有问题的。</li>
<li>当加入D时，就出现了问题，内存空间不够了，因此根据LRU算法，内存空间中A待的时间最为久远，选择A,将其淘汰。</li>
<li>当再次引用B时，内存空间中的B又处于活跃状态，而C则变成了内存空间中，近段时间最久未使用的。</li>
<li>当再次向内存空间加入E时，这时内存空间又不足了，选择在内存空间中待的最久的C将其淘汰出内存，这时的内存空间存放的对象就是E-&gt;B-&gt;D。</li>
</ul>
<h4 id="实现原理">实现原理</h4>
<p>传统意义的LRU算法是为每一个Cache对象设置一个计数器，每次Cache命中则给计数器+1，而Cache用完，需要淘汰旧内容，放置新内容时，就查看所有的计数器，并将最少使用的内容替换掉。</p>
<p>它的弊端很明显，如果Cache的数量少，问题不会很大， 但是如果Cache的空间过大，达到10W或者100W以上，一旦需要淘汰，则需要遍历所有计算器，其性能与资源消耗是巨大的，效率也就非常的慢了。</p>
<p>它的原理： 将Cache的所有位置都用双连表连接起来，当一个位置被命中之后，就将通过调整链表的指向，将该位置调整到链表头的位置，新加入的Cache直接加到链表头中。</p>
<p>这样，在多次进行Cache操作后，最近被命中的，就会被向链表头方向移动，而没有命中的，而想链表后面移动，链表尾则表示最近最少使用的Cache。</p>
<p>当需要替换内容时候，链表的最后位置就是最少被命中的位置，我们只需要淘汰链表最后的部分即可。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LRUCache</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">LinkedHashMap</span>&lt;<span class="hljs-title">Integer</span>, <span class="hljs-title">Integer</span>&gt;</span>&#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> capacity;<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">LRUCache</span><span class="hljs-params">(<span class="hljs-keyword">int</span> capacity)</span> </span>&#123;<br>        <span class="hljs-keyword">super</span>(capacity, <span class="hljs-number">0.75F</span>, <span class="hljs-keyword">true</span>);<br>        <span class="hljs-keyword">this</span>.capacity = capacity;<br>    &#125;<br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">get</span><span class="hljs-params">(<span class="hljs-keyword">int</span> key)</span> </span>&#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">super</span>.getOrDefault(key, -<span class="hljs-number">1</span>);<br>    &#125;<br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">put</span><span class="hljs-params">(<span class="hljs-keyword">int</span> key, <span class="hljs-keyword">int</span> value)</span> </span>&#123;<br>        <span class="hljs-keyword">super</span>.put(key, value);<br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-function"><span class="hljs-keyword">protected</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">removeEldestEntry</span><span class="hljs-params">(Map.Entry&lt;Integer, Integer&gt; eldest)</span> </span>&#123;<br>        <span class="hljs-keyword">return</span> size() &gt; capacity; <br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LRUCache</span> </span>&#123;<br>    <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DLinkedNode</span> </span>&#123;<br>        <span class="hljs-keyword">int</span> key;<br>        <span class="hljs-keyword">int</span> value;<br>        DLinkedNode prev;<br>        DLinkedNode next;<br>        <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">DLinkedNode</span><span class="hljs-params">()</span> </span>&#123;&#125;<br>        <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">DLinkedNode</span><span class="hljs-params">(<span class="hljs-keyword">int</span> _key, <span class="hljs-keyword">int</span> _value)</span> </span>&#123;key = _key; value = _value;&#125;<br>    &#125;<br><br>    <span class="hljs-keyword">private</span> Map&lt;Integer, DLinkedNode&gt; cache = <span class="hljs-keyword">new</span> HashMap&lt;Integer, DLinkedNode&gt;();<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> size;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> capacity;<br>    <span class="hljs-keyword">private</span> DLinkedNode head, tail;<br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">LRUCache</span><span class="hljs-params">(<span class="hljs-keyword">int</span> capacity)</span> </span>&#123;<br>        <span class="hljs-keyword">this</span>.size = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">this</span>.capacity = capacity;<br>        <span class="hljs-comment">// 使用伪头部和伪尾部节点</span><br>        head = <span class="hljs-keyword">new</span> DLinkedNode();<br>        tail = <span class="hljs-keyword">new</span> DLinkedNode();<br>        head.next = tail;<br>        tail.prev = head;<br>    &#125;<br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">get</span><span class="hljs-params">(<span class="hljs-keyword">int</span> key)</span> </span>&#123;<br>        DLinkedNode node = cache.get(key);<br>        <span class="hljs-keyword">if</span> (node == <span class="hljs-keyword">null</span>) &#123;<br>            <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span>;<br>        &#125;<br>        <span class="hljs-comment">// 如果 key 存在，先通过哈希表定位，再移到头部</span><br>        moveToHead(node);<br>        <span class="hljs-keyword">return</span> node.value;<br>    &#125;<br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">put</span><span class="hljs-params">(<span class="hljs-keyword">int</span> key, <span class="hljs-keyword">int</span> value)</span> </span>&#123;<br>        DLinkedNode node = cache.get(key);<br>        <span class="hljs-keyword">if</span> (node == <span class="hljs-keyword">null</span>) &#123;<br>            <span class="hljs-comment">// 如果 key 不存在，创建一个新的节点</span><br>            DLinkedNode newNode = <span class="hljs-keyword">new</span> DLinkedNode(key, value);<br>            <span class="hljs-comment">// 添加进哈希表</span><br>            cache.put(key, newNode);<br>            <span class="hljs-comment">// 添加至双向链表的头部</span><br>            addToHead(newNode);<br>            ++size;<br>            <span class="hljs-keyword">if</span> (size &gt; capacity) &#123;<br>                <span class="hljs-comment">// 如果超出容量，删除双向链表的尾部节点</span><br>                DLinkedNode tail = removeTail();<br>                <span class="hljs-comment">// 删除哈希表中对应的项</span><br>                cache.remove(tail.key);<br>                --size;<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">else</span> &#123;<br>            <span class="hljs-comment">// 如果 key 存在，先通过哈希表定位，再修改 value，并移到头部</span><br>            node.value = value;<br>            moveToHead(node);<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-keyword">void</span> <span class="hljs-title">addToHead</span><span class="hljs-params">(DLinkedNode node)</span> </span>&#123;<br>        node.prev = head;<br>        node.next = head.next;<br>        head.next.prev = node;<br>        head.next = node;<br>    &#125;<br><br>    <span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-keyword">void</span> <span class="hljs-title">removeNode</span><span class="hljs-params">(DLinkedNode node)</span> </span>&#123;<br>        node.prev.next = node.next;<br>        node.next.prev = node.prev;<br>    &#125;<br><br>    <span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-keyword">void</span> <span class="hljs-title">moveToHead</span><span class="hljs-params">(DLinkedNode node)</span> </span>&#123;<br>        removeNode(node);<br>        addToHead(node);<br>    &#125;<br><br>    <span class="hljs-function"><span class="hljs-keyword">private</span> DLinkedNode <span class="hljs-title">removeTail</span><span class="hljs-params">()</span> </span>&#123;<br>        DLinkedNode res = tail.prev;<br>        removeNode(res);<br>        <span class="hljs-keyword">return</span> res;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h2 id="LFU算法">LFU算法</h2>
<p>LFU（Least Frequently Used）算法，即最少访问算法，根据访问缓存的历史频率来淘汰数据，核心思想是“如果数据在过去一段时间被访问的次数很少，那么将来被访问的概率也会很低”。如果两个元素的访问频率相同，则淘汰最久没被访问的元素。也就是说LFU淘汰的时候会选择两个维度，先比较频率，选择访问频率最小的元素；如果频率相同，则按时间维度淘汰掉最久远的那个元素。</p>
<p>一般会维护两个数据结构：</p>
<ul>
<li>
<p>哈希：用来提供对外部的访问，查询效率更高；</p>
</li>
<li>
<p>双向链表或队列：维护了对元素访问次数的排序</p>
<p>缓存操作导致的链表变化：</p>
</li>
<li>
<p>添加新元素：新元素访问次数为1，放到队尾；</p>
</li>
<li>
<p>缓存淘汰：从队尾开始淘汰，因为队尾元素的访问次数最少；</p>
</li>
<li>
<p>访问缓存：访问缓存会增加元素的访问次数，所以元素在队列或双向链表中的位置会重新排序</p>
</li>
</ul>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E7%AD%96%E7%95%A5/93e8e91e143ea01757667be4b54e4a10.png" class="" title="img">
<p>这里的key就是输入的key，没什么特别的。<strong>关键是value</strong>，它的value不是一个简单的value，而是一个节点对象。节点对象<strong>Node</strong>包含了<strong>key，value，以及频率</strong>，这个Node又会出现在第二个哈希表的value中(等会我们再说)。至于为什么Node中又重复包含了key，因为某些情况下我们不是通过k-v哈希表拿到Node的，而是通过其他方式获得了Node，之后需要用Node中的key反向去k-v哈希表中做一些操作，所以Node中包含了一些冗余信息。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E7%AD%96%E7%95%A5/23d1c0e3501b4b82b5eca0b9a631dc18.png" class="" title="img">
<p>这张<strong>哈希表中的key是频率</strong>，也就是元素被访问的频率(被访问了1次，被访问了两次等等)，<strong>它的value是一个双向链表刚才说的Node对象，现在又出现了，这里的Node其实是双向链表中的一个节点</strong>。第一张图中我们说了Node中包含了一个冗余的key，其实它还包含了一个冗余的频率值，因为某些情况下，我们需要通过Node中的频率值，反向去频率哈希表中做查找，所以也需要一个冗余的频率值。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E7%AD%96%E7%95%A5/c5bf76ee14075961a202f8dbf6dcb6b2.png" class="" title="img">
<ul>
<li>优点 ：一般情况下，LFU效率要优于LRU，能够避免周期性或者偶发性的操作导致缓存命中率下降的问题。</li>
<li>缺点：<strong>复杂度较高</strong>：需要额外维护一个队列或双向链表，复杂度较高。
<ul>
<li><strong>对新缓存不友好</strong>：新加入的缓存容易被清理掉，即使可能会被经常访问</li>
<li><strong>缓存污染</strong>：一旦缓存的访问模式发生变化，访问记录的历史存量，会导致缓存污染；</li>
<li><strong>内存开销</strong>：需要对每一项缓存数据维护一个访问次数，内存成本较大；</li>
<li><strong>处理器开销</strong>：需要对访问次数排序，会增加一定的处理器开销</li>
</ul>
</li>
</ul>
<h3 id="算法实现">算法实现</h3>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LFUCache</span> </span>&#123;<br>    <span class="hljs-keyword">int</span> minfreq, capacity;<br>    Map&lt;Integer, Node&gt; key_table;<br>    Map&lt;Integer, LinkedList&lt;Node&gt;&gt; freq_table;<br><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">LFUCache</span><span class="hljs-params">(<span class="hljs-keyword">int</span> capacity)</span> </span>&#123;<br>        <span class="hljs-keyword">this</span>.minfreq = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">this</span>.capacity = capacity;<br>        key_table = <span class="hljs-keyword">new</span> HashMap&lt;Integer, Node&gt;();;<br>        freq_table = <span class="hljs-keyword">new</span> HashMap&lt;Integer, LinkedList&lt;Node&gt;&gt;();<br>    &#125;<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">get</span><span class="hljs-params">(<span class="hljs-keyword">int</span> key)</span> </span>&#123;<br>        <span class="hljs-keyword">if</span> (capacity == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span>;<br>        &#125;<br>        <span class="hljs-keyword">if</span> (!key_table.containsKey(key)) &#123;<br>            <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span>;<br>        &#125;<br>        Node node = key_table.get(key);<br>        <span class="hljs-keyword">int</span> val = node.val, freq = node.freq;<br>        freq_table.get(freq).remove(node);<br>        <span class="hljs-comment">// 如果当前链表为空，我们需要在哈希表中删除，且更新minFreq</span><br>        <span class="hljs-keyword">if</span> (freq_table.get(freq).size() == <span class="hljs-number">0</span>) &#123;<br>            freq_table.remove(freq);<br>            <span class="hljs-keyword">if</span> (minfreq == freq) &#123;<br>                minfreq += <span class="hljs-number">1</span>;<br>            &#125;<br>        &#125;<br>        <span class="hljs-comment">// 插入到 freq + 1 中</span><br>        LinkedList&lt;Node&gt; list = freq_table.getOrDefault(freq + <span class="hljs-number">1</span>, <span class="hljs-keyword">new</span> LinkedList&lt;Node&gt;());<br>        list.offerFirst(<span class="hljs-keyword">new</span> Node(key, val, freq + <span class="hljs-number">1</span>));<br>        freq_table.put(freq + <span class="hljs-number">1</span>, list);<br>        key_table.put(key, freq_table.get(freq + <span class="hljs-number">1</span>).peekFirst());<br>        <span class="hljs-keyword">return</span> val;<br>    &#125;<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">put</span><span class="hljs-params">(<span class="hljs-keyword">int</span> key, <span class="hljs-keyword">int</span> value)</span> </span>&#123;<br>        <span class="hljs-keyword">if</span> (capacity == <span class="hljs-number">0</span>) &#123;<br>            <span class="hljs-keyword">return</span>;<br>        &#125;<br>        <span class="hljs-keyword">if</span> (!key_table.containsKey(key)) &#123;<br>            <span class="hljs-comment">// 缓存已满，需要进行删除操作</span><br>            <span class="hljs-keyword">if</span> (key_table.size() == capacity) &#123;<br>                <span class="hljs-comment">// 通过 minFreq 拿到 freq_table[minFreq] 链表的末尾节点</span><br>                Node node = freq_table.get(minfreq).peekLast();<br>                key_table.remove(node.key);<br>                freq_table.get(minfreq).pollLast();<br>                <span class="hljs-keyword">if</span> (freq_table.get(minfreq).size() == <span class="hljs-number">0</span>) &#123;<br>                    freq_table.remove(minfreq);<br>                &#125;<br>            &#125;<br>            LinkedList&lt;Node&gt; list = freq_table.getOrDefault(<span class="hljs-number">1</span>, <span class="hljs-keyword">new</span> LinkedList&lt;Node&gt;());<br>            list.offerFirst(<span class="hljs-keyword">new</span> Node(key, value, <span class="hljs-number">1</span>));<br>            freq_table.put(<span class="hljs-number">1</span>, list);<br>            key_table.put(key, freq_table.get(<span class="hljs-number">1</span>).peekFirst());<br>            minfreq = <span class="hljs-number">1</span>;<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            <span class="hljs-comment">// 与 get 操作基本一致，除了需要更新缓存的值</span><br>            Node node = key_table.get(key);<br>            <span class="hljs-keyword">int</span> freq = node.freq;<br>            freq_table.get(freq).remove(node);<br>            <span class="hljs-keyword">if</span> (freq_table.get(freq).size() == <span class="hljs-number">0</span>) &#123;<br>                freq_table.remove(freq);<br>                <span class="hljs-keyword">if</span> (minfreq == freq) &#123;<br>                    minfreq += <span class="hljs-number">1</span>;<br>                &#125;<br>            &#125;<br>            LinkedList&lt;Node&gt; list = freq_table.getOrDefault(freq + <span class="hljs-number">1</span>, <span class="hljs-keyword">new</span> LinkedList&lt;Node&gt;());<br>            list.offerFirst(<span class="hljs-keyword">new</span> Node(key, value, freq + <span class="hljs-number">1</span>));<br>            freq_table.put(freq + <span class="hljs-number">1</span>, list);<br>            key_table.put(key, freq_table.get(freq + <span class="hljs-number">1</span>).peekFirst());<br>        &#125;<br>    &#125;<br>&#125;<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Node</span> </span>&#123;<br>    <span class="hljs-keyword">int</span> key, val, freq;<br><br>    Node(<span class="hljs-keyword">int</span> key, <span class="hljs-keyword">int</span> val, <span class="hljs-keyword">int</span> freq) &#123;<br>        <span class="hljs-keyword">this</span>.key = key;<br>        <span class="hljs-keyword">this</span>.val = val;<br>        <span class="hljs-keyword">this</span>.freq = freq;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h1>Redis过期键删除策略</h1>
<p><strong>Redis中带有过期时间的key是保存在一个字典中（这个字典叫做==过期字典==），以后会定时遍历这个字典，删除过期的key。</strong></p>
<p><strong>检查过期时间：ttl key</strong></p>
<ol>
<li>
<p>查看过期时间 ttl key( 返回值   -1:当前key没有过期时间   &gt;0:当前key的剩余存活时间  -2:当前key不存在)</p>
</li>
<li>
<p>删除过期时间 persist key   把带有过期时间的key变为永久不过期（返回值   1代表删除过期时间成功     0代表当前key没过期时间 or key不存在）</p>

</li>
</ol>
<h2 id="1-定时删除">1.定时删除</h2>
<p>在设置键的过期时间的同时，创建一个定时器（timer），让定时器在键的过期时间来临时，立即执行对键的删除操作。</p>
<ul>
<li><strong>优点</strong>：对内存是最友好的：通过使用定时器，定时删除策略可以保证过期键会尽 可能快地被删除，并释放过期键所占用的内存</li>
<li><strong>缺点</strong>：它对CPU时间是最不友好的：在过期键比较多的情况下，删除过期键这一行为可能会占用相当一部分CPU时间，在内存不紧张但是CPU时间非常紧张的情况下，将CPU时间用在删除和当前任务无关的过期键上，无疑会对服务器的响应时间和吞吐量造成影响创建一个定时器需要用到Redis服务器中的时间事件，而当前时间事件的实现方式——无序链表，查找一个事件的时间复杂度为O(N)——并不能高效地处理大量时间 事件</li>
</ul>
<h2 id="2-惰性删除">2.惰性删除</h2>
<p><strong>每次从键空间获取键时，都检查是否过期，过期则删除，未过期，则返回该键</strong></p>
<p>只有取出键时才对其进行过期检查，不会删除其他键，不会花费CPU太多时间。若一个键早已过期，但一直未被取到，它便会一直占用内存。若有大量的未被访问的键，而服务器又不会主动释放，就会造成大量的内存浪费。</p>

<ul>
<li><strong>优点</strong>：对CPU时间来说是最友好的：程序只会在取出键时才对键进行过期检查，<br>
这可以保证删除过期键的操作只会在非做不可的情况下进行，并且删除的目标仅限于当前处理的键，这个策略不会在删除其他无关的过期键上花费任何CPU时间</li>
<li><strong>缺点</strong>：它对内存是最不友好的：如果一个键已经过期，而这个键又仍 然保留在数据库中，那么只要这个过期键不被删除，它所占用的内存就不会释放。<br>
例如：在使用惰性删除策略时，如果数据库中有非常多的过期键，而这些过期键又恰好没有被访问到的话，那么它们也许永远也不会被删除（除非用户手动执行FLUSHDB），我们甚至可以将这种情况看作是一种内存泄漏——无用的垃圾数据占用了大量的内存，而服务器却不 会自己去释放它们，这对于运行状态非常依赖于内存的Redis服务器来说，肯定不是一个好消息</li>
</ul>
<h3 id="实现原理（expireIfNeeded函数）">实现原理（expireIfNeeded函数）</h3>
<p><strong>expireIfNeeded</strong>函数就像一个<strong>过滤器</strong>，它可以==在命令真正执行之前，过滤掉过期的输入键，从而避免命令接触到过期键== 。惰性删除策略由db.c/expireIfNeeded函数实现，所有读写数据库的Redis命令在执行之前都会调用<strong>expireIfNeeded</strong>函数对输入键进行检查：</p>
<ul>
<li>
<p>如果输入键已经过期，那么expireIfNeeded函数将输入键从数据库中删除</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E7%AD%96%E7%95%A5/20201204131000785.png" class="" title="在这里插入图片描述">
</li>
<li>
<p>如果输入键未过期，那么expireIfNeeded函数不做动作。因为每个被访问的键都可能因为过期而被expireIfNeeded函数删除，所以每个命令的实现函数<strong>都必须能同时处理键存在以及键不存在这两种情况</strong>：</p>
</li>
<li>
<p>当键存在时，命令按照键存在的情况执行</p>
</li>
<li>
<p>当键不存在或者键因为过期而被expireIfNeeded函数删除时，命令按照键不存在的情况执行</p>
</li>
</ul>
<p><strong>举个例子，下图展示了GET命令的执行过程，在这个执行过程中，命令需要判断键是否存在以及键是否过期，然后根据判断来执行合适的动作</strong></p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E7%AD%96%E7%95%A5/20201204131056907.png" class="" title="在这里插入图片描述">
<h2 id="3-定期删除">3.定期删除</h2>
<p><strong>每隔一段时间删除里面的过期键</strong></p>
<p>Redis默认会1秒进行10次过期扫描，过期扫描不会遍历字典中所有的key，而是采用一种贪心的策略。</p>
<ol>
<li>从字典中随机选出20个key。</li>
<li>删除这20个key中过期的key。</li>
<li>如果过期的key比例超过1/4，那就在进行步骤1.</li>
<li>同时，为了保证扫描不会出现循环过度，导致线程卡死的现象，算法会设置扫描时间的上限，默认为25ms。</li>
</ol>
<ul>
<li>
<p><strong>优点</strong>：定期删除策略每隔一段时间执行一次删除过期键操作，并通过限制删除操作执行的时长 和频率来减少删除操作对CPU时间的影响。通过定期删除过期键，定期删除策略有效地减少了因为过期键而带来的内存浪费。</p>
</li>
<li>
<p><strong>缺点</strong>：如果删除操作执行得太频繁，或者执行的时间太长，定期删除策略就会退化成定时删除策略，以至于将CPU时间过多地消耗在删除过期键上面。如果删除操作执行得太少，或者执行的时间太短，定期删除策略又会和惰性删除策略一 样，出现浪费内存的情况。</p>
</li>
</ul>
<h3 id="实现原理（activeExpireCycle函数）">实现原理（activeExpireCycle函数）</h3>
<p>过期键的定期删除策略由redis.c/activeExpireCycle函数实现，每当Redis的服务器周期性操作redis.c/serverCron函数执行时，activeExpireCycle函数就会被调用，它在规定的时间内，分多次遍历服务器中的各个数据库，从数据库的expires字典中随机检查一部分键的过期时间，并删除其中的过期键。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#默认每次检查的数据库数量</span><br>DEFAULT_DB_NUMBERS = <span class="hljs-number">16</span><br> <br><span class="hljs-comment">#默认每个数据库检查的键数量</span><br>DEFAULT_KEY_NUMBERS = <span class="hljs-number">20</span><br> <br><span class="hljs-comment">#全局变量，记录检查进度</span><br>current_db = <span class="hljs-number">0</span><br> <br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">activeExpireCycle</span>():</span><br>    <span class="hljs-comment"># 初始化要检查的数据库数量</span><br>    <span class="hljs-comment"># 如果服务器的数据库数量比 DEFAULT_DB_NUMBERS 要小</span><br>    <span class="hljs-comment"># 那么以服务器的数据库数量为准</span><br>    <span class="hljs-keyword">if</span> server.dbnum &lt; DEFAULT_DB_NUMBERS:<br>        db_numbers = server.dbnum<br>    <span class="hljs-keyword">else</span>:<br>        db_numbers = DEFAULT_DB_NUMBERS<br> <br>    <span class="hljs-comment"># 遍历各个数据库</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(db_numbers):<br>        <span class="hljs-comment"># 如果current_db 的值等于服务器的数据库数量</span><br>        <span class="hljs-comment"># 这表示检查程序已经遍历了服务器的所有数据库一次</span><br>        <span class="hljs-comment"># 将current_db 重置为0 ，开始新的一轮遍历</span><br>        <span class="hljs-keyword">if</span> current_db == server.dbnum:<br>            current_db = <span class="hljs-number">0</span><br>       <br>        <span class="hljs-comment"># 获取当前要处理的数据库</span><br>        redisDb = server.db[current_db]<br> <br>        <span class="hljs-comment"># 将数据库索引增1 ，指向下一个要处理的数据库</span><br>        current_db += <span class="hljs-number">1</span><br> <br>        <span class="hljs-comment"># 检查数据库键</span><br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(DEFAULT_KEY_NUMBERS):<br>            <span class="hljs-comment"># 如果数据库中没有一个键带有过期时间，那么跳过这个数据库</span><br>            <span class="hljs-keyword">if</span> redisDb.expires.size() == <span class="hljs-number">0</span>: <span class="hljs-keyword">break</span><br>                <span class="hljs-comment">#随机获取一个带有过期时间的键</span><br>                key_with_ttl = redisDb.expires.get_random_key()<br>    <br>                <span class="hljs-comment"># 检查键是否过期，如果过期就删除它</span><br>                <span class="hljs-keyword">if</span> is_expired(key_with_ttl):<br>                    delete_key(key_with_ttl)<br>                <span class="hljs-comment"># 已达到时间上限，停止处理</span><br>                <span class="hljs-keyword">if</span> reach_time_limit(): <span class="hljs-keyword">return</span><br><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python">DB_NUMBER = <span class="hljs-number">16</span> <span class="hljs-comment"># 数据库数量，默认16（0-15），可以通过配置无限增加</span><br>KEY_NUMBERS = <span class="hljs-number">20</span> <span class="hljs-comment"># 每次检查key的数量</span><br><br>current_db = <span class="hljs-number">0</span> <span class="hljs-comment"># 记录当前检查到哪个库</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">activeExpireCycle</span>():</span><br>  <br>  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(DB_NUMBERS):<br>  <br>    <span class="hljs-keyword">if</span> current_db == DB_BUMBERS:<br>      current_db = <span class="hljs-number">0</span><br><br>    <span class="hljs-comment"># 获取当前数据库</span><br>    redisDB = server.db[current_db]<br>    first_start = <span class="hljs-literal">True</span><br>    del_key_num = <span class="hljs-number">0</span><br>    current_db += <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">while</span>(first_start <span class="hljs-keyword">or</span> del_key_num &gt; KEY_NUMBERS/<span class="hljs-number">4</span>):<br>      first_start = <span class="hljs-literal">False</span><br>      <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(KEY_NUMBERS):<br>        <br>        _key = redisDB.randomExpireKey()<br>        <span class="hljs-keyword">if</span> is_expire(_key):<br>          <span class="hljs-comment"># 过期 则直接删除</span><br>          delete_key(_key)<br>          del_key_num += <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">if</span> time_is_limit():<br>          <span class="hljs-comment"># 若执行时间太长，则停止，默认25毫秒</span><br>          <span class="hljs-keyword">return</span><br></code></pre></td></tr></table></figure>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/dajiangdahe.github.io/tags/Redis/" rel="tag">Redis</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Redis内存模型"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"
    >Redis内存模型</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time datetime="2021-10-22T07:21:45.000Z" itemprop="datePublished">2021-10-22</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/dajiangdahe.github.io/categories/Redis/">Redis</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1>1.Redis内存划分</h1>
<p>Redis作为内存数据库，在内存中存储的内容主要是数据（键值对key-value）其他部分也会站一定的内存。</p>
<h2 id="数据">数据</h2>
<p>Redis数据类型：</p>
<ul>
<li>string：字符串，可以用作分布式锁</li>
<li>hash：哈希，可以用作早期的购物车</li>
<li>list：列表，可以用作微信文章列表</li>
<li>set：集合，可以用在做交集，差集</li>
<li>zset：有序集合，可以用做排行榜。</li>
<li>HyperLogLog：Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。</li>
<li>Geo：Redis GEO 主要用于存储地理位置信息，并对存储的信息进行操作，该功能在 Redis 3.2 版本新增。</li>
<li>Stream：Redis Stream 主要用于消息队列（MQ，Message Queue），Redis 本身是有一个 Redis 发布订阅 (pub/sub) 来实现消息队列的功能，但它有个缺点就是消息无法持久化，如果出现网络断开、Redis 宕机等，消息就会被丢弃。简单来说发布订阅 (pub/sub) 可以分发消息，但无法记录历史消息。</li>
</ul>
<p>作为数据库，数据是最主要的部分；这部分占用的内存会统计在used_memory中。Redis在存储对象时，并不是直接将数据扔进内存，而是会对对象进行各种包装：如redisObject、SDS等</p>
<h2 id="进程本身运行需要的内存">进程本身运行需要的内存</h2>
<p>Redis主进程本身运行肯定需要占用内存，如代码、常量池等等；这部分内存大约几兆，在大多数生产环境中与Redis数据占用的内存相比可以忽略。这部分内存不是由jemalloc分配，因此不会统计在used_memory中。</p>
<p>补充说明：除了主进程外，Redis创建的子进程运行也会占用内存，如Redis执行AOF、RDB重写时创建的子进程。当然，这部分内存不属于Redis进程，也不会统计在used_memory和used_memory_rss中。</p>
<h2 id="缓冲内存">缓冲内存</h2>
<p>缓冲内存包括客户端缓冲区、复制积压缓冲区、AOF缓冲区等；其中，客户端缓冲存储客户端连接的输入输出缓冲；复制积压缓冲用于部分复制功能；AOF缓冲区用于在进行AOF重写时，保存最近的写入命令。在了解相应功能之前，不需要知道这些缓冲的细节；这部分内存由jemalloc分配，因此会统计在used_memory中。</p>
<h2 id="内存碎片">内存碎片</h2>
<p>内存碎片是Redis在分配、回收物理内存过程中产生的。例如，如果对数据的更改频繁，而且数据之间的大小相差很大，可能导致redis释放的空间在物理内存中并没有释放，但redis又无法有效利用，这就形成了内存碎片。内存碎片不会统计在used_memory中。</p>
<p>如果Redis服务器中的内存碎片已经很大，可以通过安全重启的方式减小内存碎片：因为重启之后，Redis重新从备份文件中读取数据，在内存中进行重排，为每个数据重新选择合适的内存单元，减小内存碎片。</p>
<h1>2.Redis数据存储的细节</h1>
<h2 id="Redis存储细节">Redis存储细节</h2>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/kismetv/p/8654978.html">https://www.cnblogs.com/kismetv/p/8654978.html</a></p>
<p>关于Redis数据存储的细节，涉及到内存分配器（如jemalloc）、简单动态字符串（SDS）、5种对象类型及内部编码、redisObject。在讲述具体内容之前，先说明一下这几个概念之间的关系。</p>
<p>下图是执行set hello world时，所涉及到的数据模型。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/1174710-20180327001055927-1896197804.png" class="" title="img">
<p>（1）dictEntry：Redis是Key-Value数据库，因此对每个键值对都会有一个dictEntry，里面存储了指向Key和Value的指针；next指向下一个dictEntry，与本Key-Value无关。</p>
<p>（2）Key：图中右上角可见，Key（”hello”）并不是直接以字符串存储，而是存储在SDS结构中。</p>
<p>（3）redisObject：Value(“world”)既不是直接以字符串存储，也不是像Key一样直接存储在SDS中，而是存储在redisObject中。实际上，不论Value是5种类型的哪一种，都是通过redisObject来存储的；而redisObject中的type字段指明了Value对象的类型，ptr字段则指向对象所在的地址。不过可以看出，字符串对象虽然经过了redisObject的包装，但仍然需要通过SDS存储。</p>
<p>实际上，redisObject除了type和ptr字段以外，还有其他字段图中没有给出，如用于指定对象内部编码的字段；后面会详细介绍。</p>
<p>（4）jemalloc：无论是DictEntry对象，还是redisObject、SDS对象，都需要内存分配器（如jemalloc）分配内存进行存储。以DictEntry对象为例，有3个指针组成，在64位机器下占24个字节，jemalloc会为它分配32字节大小的内存单元。</p>
<h2 id="SDS与C字符串的比较">SDS与C字符串的比较</h2>
<p>SDS在C字符串的基础上加入了free和len字段，带来了很多好处：</p>
<ul>
<li>获取字符串长度：SDS是O(1)，C字符串是O(n)</li>
<li>缓冲区溢出：使用C字符串的API时，如果字符串长度增加（如strcat操作）而忘记重新分配内存，很容易造成缓冲区的溢出；而SDS由于记录了长度，相应的API在可能造成缓冲区溢出时会自动重新分配内存，杜绝了缓冲区溢出。</li>
<li>修改字符串时内存的重分配：对于C字符串，如果要修改字符串，必须要重新分配内存（先释放再申请），因为如果没有重新分配，字符串长度增大时会造成内存缓冲区溢出，字符串长度减小时会造成内存泄露。而对于SDS，由于可以记录len和free，因此解除了字符串长度和空间数组长度之间的关联，可以在此基础上进行优化：空间预分配策略（即分配内存时比实际需要的多）使得字符串长度增大时重新分配内存的概率大大减小；惰性空间释放策略使得字符串长度减小时重新分配内存的概率大大减小。</li>
<li>存取二进制数据：SDS可以，C字符串不可以。因为C字符串以空字符作为字符串结束的标识，而对于一些二进制文件（如图片等），内容可能包括空字符串，因此C字符串无法正确存取；而SDS以字符串长度len来作为字符串结束标识，因此没有这个问题。</li>
</ul>
<p>此外，由于SDS中的buf仍然使用了C字符串（即以’\0’结尾），因此SDS可以使用C字符串库中的部分函数；但是需要注意的是，只有当SDS用来存储文本数据时才可以这样使用，在存储二进制数据时则不行（’\0’不一定是结尾）。</p>
<p>Redis在存储对象时，一律使用SDS代替C字符串。例如set hello world命令，hello和world都是以SDS的形式存储的。而sadd myset member1 member2 member3命令，不论是键（”myset”），还是集合中的元素（”member1”、 ”member2”和”member3”），都是以SDS的形式存储。除了存储对象，SDS还用于存储各种缓冲区。</p>
<p>只有在字符串不会改变的情况下，如打印日志时，才会使用C字符串。</p>
<h1>3.Redis的对象类型与内部编码</h1>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/1174710-20180327001358239-1304238510.png" class="" title="img">
<h2 id="字符串">字符串</h2>
<h3 id="（1）概况">（1）概况</h3>
<p>字符串是最基础的类型，因为所有的键都是字符串类型，且字符串之外的其他几种复杂类型的元素也是字符串。</p>
<p>字符串长度不能超过512MB。</p>
<h3 id="（2）内部编码">（2）内部编码</h3>
<p>字符串类型的内部编码有3种，它们的应用场景如下：</p>
<ul>
<li>
<p>int：8个字节的长整型。字符串值是整型时，这个值使用long整型表示。</p>
</li>
<li>
<p>embstr：&lt;=39字节的字符串。embstr与raw都使用redisObject和sds保存数据，区别在于，embstr的使用只分配一次内存空间（因此redisObject和sds是连续的），而raw需要分配两次内存空间（分别为redisObject和sds分配空间）。因此与raw相比，embstr的好处在于创建时少分配一次空间，删除时少释放一次空间，以及对象的所有数据连在一起，寻找方便。而embstr的坏处也很明显，如果字符串的长度增加需要重新分配内存时，整个redisObject和sds都需要重新分配空间，因此redis中的embstr实现为只读。</p>
</li>
<li>
<p>raw：大于39个字节的字符串</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/1174710-20180327001417703-15851809.png" class="" title="img">
</li>
</ul>
<p>embstr和raw进行区分的长度，是39；是因为redisObject的长度是16字节，sds的长度是9+字符串长度；因此当字符串长度是39时，embstr的长度正好是16+9+39=64，jemalloc正好可以分配64字节的内存单元。</p>
<h3 id="（3）编码转换">（3）编码转换</h3>
<p>当int数据不再是整数，或大小超过了long的范围时，自动转化为raw。</p>
<p>而对于embstr，由于其实现是只读的，因此在对embstr对象进行修改时，都会先转化为raw再进行修改，因此，只要是修改embstr对象，修改后的对象一定是raw的，无论是否达到了39个字节。示例如下图所示：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/1174710-20180327001426651-1225081171.png" class="" title="img">
<h2 id="列表">列表</h2>
<h3 id="（1）概况-2">（1）概况</h3>
<p>列表（list）用来存储多个有序的字符串，每个字符串称为元素；一个列表可以存储2^32-1个元素。Redis中的列表支持两端插入和弹出，并可以获得指定位置（或范围）的元素，可以充当数组、队列、栈等。</p>
<h3 id="（2）内部编码-2">（2）内部编码</h3>
<p>列表的内部编码可以是压缩列表（ziplist）或双端链表（linkedlist）。</p>
<p>双端链表：由一个list结构和多个listNode结构组成；典型结构如下图所示：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/1174710-20180327001435577-242733744.png" class="" title="img">
<p>通过图中可以看出，双端链表同时保存了表头指针和表尾指针，并且每个节点都有指向前和指向后的指针；链表中保存了列表的长度；dup、free和match为节点值设置类型特定函数，所以链表可以用于保存各种不同类型的值。而链表中每个节点指向的是type为字符串的redisObject。</p>
<p>压缩列表：压缩列表是Redis为了节约内存而开发的，是由一系列特殊编码的<strong>连续内存块</strong>(而不是像双端链表一样每个节点是指针)组成的顺序型数据结构；具体结构相对比较复杂，略。与双端链表相比，压缩列表可以节省内存空间，但是进行修改或增删操作时，复杂度较高；因此当节点数量较少时，可以使用压缩列表；但是节点数量多时，还是使用双端链表划算。</p>
<p>压缩列表不仅用于实现列表，也用于实现哈希、有序列表；使用非常广泛。</p>
<h3 id="（3）编码转换-2">（3）编码转换</h3>
<p>只有同时满足下面两个条件时，才会使用压缩列表：列表中元素数量小于512个；列表中所有字符串对象都不足64字节。如果有一个条件不满足，则使用双端列表；且编码只可能由压缩列表转化为双端链表，反方向则不可能。</p>
<p>下图展示了列表编码转换的特点：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/1174710-20180327001457636-673470263.png" class="" title="img">
<p>其中，单个字符串不能超过64字节，是为了便于统一分配每个节点的长度；这里的64字节是指字符串的长度，不包括SDS结构，因为<strong>压缩列表使用连续、定长内存块存储字符串</strong>，不需要SDS结构指明长度。后面提到压缩列表，也会强调长度不超过64字节，原理与这里类似。</p>
<h2 id="哈希">哈希</h2>
<h3 id="（1）概况-3">（1）概况</h3>
<p>哈希（作为一种数据结构），不仅是redis对外提供的5种对象类型的一种（与字符串、列表、集合、有序结合并列），也是Redis作为Key-Value数据库所使用的数据结构。为了说明的方便，在本文后面当使用“内层的哈希”时，代表的是redis对外提供的5种对象类型的一种；使用“外层的哈希”代指Redis作为Key-Value数据库所使用的数据结构。</p>
<h3 id="（2）内部编码-3">（2）内部编码</h3>
<p>内层的哈希使用的内部编码可以是压缩列表（ziplist）和哈希表（hashtable）两种；Redis的外层的哈希则只使用了hashtable。</p>
<p>压缩列表前面已介绍。与哈希表相比，压缩列表用于元素个数少、元素长度小的场景；其优势在于集中存储，节省空间；同时，虽然对于元素的操作复杂度也由O(1)变为了O(n)，但由于哈希中元素数量较少，因此操作的时间并没有明显劣势。</p>
<p>正常情况下（即hashtable没有进行rehash时）各部分关系如下图所示：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/1174710-20180327001627028-325473621.png" class="" title="img">
<p><strong>dictEntry</strong></p>
<p>dictEntry结构用于保存键值对，结构定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs java">typedef struct dictEntry&#123;<br>    <span class="hljs-keyword">void</span> *key;<br>    union&#123;<br>        <span class="hljs-keyword">void</span> *val;<br>        uint64_tu64;<br>        int64_ts64;<br>    &#125;v;<br>    struct dictEntry *next;<br>&#125;dictEntry;<br></code></pre></td></tr></table></figure>
<p>其中，各个属性的功能如下：</p>
<ul>
<li>key：键值对中的键；</li>
<li>val：键值对中的值，使用union(即共用体)实现，存储的内容既可能是一个指向值的指针，也可能是64位整型，或无符号64位整型；</li>
<li>next：指向下一个dictEntry，用于解决哈希冲突问题</li>
</ul>
<p>在64位系统中，一个dictEntry对象占24字节（key/val/next各占8字节）。</p>
<p><strong>bucket</strong></p>
<p>bucket是一个数组，数组的每个元素都是指向dictEntry结构的指针。redis中bucket数组的大小计算规则如下：大于dictEntry的、最小的2^n；例如，如果有1000个dictEntry，那么bucket大小为1024；如果有1500个dictEntry，则bucket大小为2048。</p>
<p><strong>dictht</strong></p>
<p>dictht结构如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs java">typedef struct dictht&#123;<br>    dictEntry **table;<br>    unsigned <span class="hljs-keyword">long</span> size;<br>    unsigned <span class="hljs-keyword">long</span> sizemask;<br>    unsigned <span class="hljs-keyword">long</span> used;<br>&#125;dictht;<br></code></pre></td></tr></table></figure>
<p>其中，各个属性的功能说明如下：</p>
<ul>
<li>table属性是一个指针，指向bucket；</li>
<li>size属性记录了哈希表的大小，即bucket的大小；</li>
<li>used记录了已使用的dictEntry的数量；</li>
<li>sizemask属性的值总是为size-1，这个属性和哈希值一起决定一个键在table中存储的位置。</li>
</ul>
<p><strong>dict</strong></p>
<p>一般来说，通过使用dictht和dictEntry结构，便可以实现普通哈希表的功能；但是Redis的实现中，在dictht结构的上层，还有一个dict结构。下面说明dict结构的定义及作用。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs java">typedef struct dict&#123;<br>    dictType *type;<br>    <span class="hljs-keyword">void</span> *privdata;<br>    dictht ht[<span class="hljs-number">2</span>];<br>    <span class="hljs-keyword">int</span> trehashidx;<br>&#125; dict;<br></code></pre></td></tr></table></figure>
<p>其中，type属性和privdata属性是为了适应不同类型的键值对，用于创建多态字典。</p>
<p>ht属性和trehashidx属性则用于rehash，即当哈希表需要扩展或收缩时使用。ht是一个包含两个项的数组，每项都指向一个dictht结构，这也是Redis的哈希会有1个dict、2个dictht结构的原因。通常情况下，所有的数据都是存在放dict的ht[0]中，ht[1]只在rehash的时候使用。dict进行rehash操作的时候，将ht[0]中的所有数据rehash到ht[1]中。然后将ht[1]赋值给ht[0]，并清空ht[1]。</p>
<p>因此，Redis中的哈希之所以在dictht和dictEntry结构之外还有一个dict结构，一方面是为了适应不同类型的键值对，另一方面是为了rehash。</p>
<h3 id="（3）编码转换-3">（3）编码转换</h3>
<p>如前所述，Redis中内层的哈希既可能使用哈希表，也可能使用压缩列表。</p>
<p>只有同时满足下面两个条件时，才会使用压缩列表：哈希中元素数量小于512个；哈希中所有键值对的键和值字符串长度都小于64字节。如果有一个条件不满足，则使用哈希表；且编码只可能由压缩列表转化为哈希表，反方向则不可能。</p>
<p>下图展示了Redis内层的哈希编码转换的特点：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/1174710-20180327001855681-1566128865.png" class="" title="img">
<h2 id="集合">集合</h2>
<h3 id="（1）概况-4">（1）概况</h3>
<p>集合（set）与列表类似，都是用来保存多个字符串，但集合与列表有两点不同：集合中的元素是无序的，因此不能通过索引来操作元素；集合中的元素不能有重复。</p>
<p>一个集合中最多可以存储2^32-1个元素；除了支持常规的增删改查，Redis还支持多个集合取交集、并集、差集。</p>
<h3 id="（2）内部编码-4">（2）内部编码</h3>
<p>集合的内部编码可以是整数集合（intset）或哈希表（hashtable）。</p>
<p>哈希表前面已经讲过，这里略过不提；需要注意的是，集合在使用哈希表时，值全部被置为null。</p>
<p>整数集合的结构定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java">typedef struct intset&#123;<br>    uint32_t encoding;<br>    uint32_t length;<br>    int8_t contents[];<br>&#125; intset;<br></code></pre></td></tr></table></figure>
<h3 id="（3）编码转换-4">（3）编码转换</h3>
<p>只有同时满足下面两个条件时，集合才会使用整数集合：集合中元素数量小于512个；集合中所有元素都是整数值。如果有一个条件不满足，则使用哈希表；且编码只可能由整数集合转化为哈希表，反方向则不可能。</p>
<p>下图展示了集合编码转换的特点：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/1174710-20180327001926146-2105183556.png" class="" title="img">
<h2 id="有序集合">有序集合</h2>
<h3 id="（1）概况-5">（1）概况</h3>
<p>有序集合与集合一样，元素都不能重复；但与集合不同的是，有序集合中的元素是有顺序的。与列表使用索引下标作为排序依据不同，有序集合为每个元素设置一个分数（score）作为排序依据。</p>
<h3 id="（2）内部编码-5">（2）内部编码</h3>
<p>有序集合的内部编码可以是压缩列表（ziplist）或跳跃表（skiplist）。ziplist在列表和哈希中都有使用，前面已经讲过，这里略过不提。</p>
<p>跳跃表是一种有序数据结构，通过在每个节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的。除了跳跃表，实现有序数据结构的另一种典型实现是平衡树；大多数情况下，跳跃表的效率可以和平衡树媲美，且跳跃表实现比平衡树简单很多，因此redis中选用跳跃表代替平衡树。跳跃表支持平均O(logN)、最坏O(N)的复杂点进行节点查找，并支持顺序操作。Redis的跳跃表实现由zskiplist和zskiplistNode两个结构组成：前者用于保存跳跃表信息（如头结点、尾节点、长度等），后者用于表示跳跃表节点。具体结构相对比较复杂，略。</p>
<h3 id="（3）编码转换-5">（3）编码转换</h3>
<p>只有同时满足下面两个条件时，才会使用压缩列表：有序集合中元素数量小于128个；有序集合中所有成员长度都不足64字节。如果有一个条件不满足，则使用跳跃表；且编码只可能由压缩列表转化为跳跃表，反方向则不可能。</p>
<p>下图展示了有序集合编码转换的特点：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Redis%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/1174710-20180327001936290-955216194.png" class="" title="img"> 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/dajiangdahe.github.io/tags/Redis/" rel="tag">Redis</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Mysql索引"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/"
    >Mysql索引</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/" class="article-date">
  <time datetime="2021-10-22T07:21:44.982Z" itemprop="datePublished">2021-10-22</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/dajiangdahe.github.io/categories/Mysql/">Mysql</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1>Mysql索引</h1>
<h2 id="【概念】">【概念】</h2>
<p>MySQL 官方对索引的定义为：索引（Index）是帮助 MySQL 高效获取数据的数据结构。简单的理解，索引类似于字典里面的目录。</p>
<h2 id="【索引类型】">【索引类型】</h2>
<p>==从数据结构角度划分：==</p>
<ul>
<li>
<p><strong>hash索引</strong>：底层就是 hash 表。进行查找时，根据 key 调用 hash 函数获得对应的 hashcode，根据 hashcode 找到对应的数<strong>据行地址</strong>，根据地址拿到对应的数据。</p>
</li>
<li>
<p>B 树索引：B树是一种多路搜索树，n 路搜索树代表每个节点最多有 n 个子节点。每个节点<strong>存储 key</strong> + <strong>指向下一层节点的指针</strong>+ <strong>指向 key 数据记录的地址</strong>。查找时，从根结点向下进行查找，直到找到对应的key。</p>
</li>
<li>
<p><strong>B+树索引</strong>：B+树是b树的变种，主要区别在于：B+树的==非叶子节点==只<strong>存储 key + 指向下一层节点的指针</strong>。另外，<strong>B+树的叶子节点之间通过指针来连接</strong>，构成一个有序链表，因此对整棵树的遍历只需要一次线性遍历叶子结点即可（o（n）时间复杂度遍历全部节点）。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/aHR0cHM6Ly9tbWJpei5xcGljLmNuL3N6X21tYml6X3BuZy9LUlJ4dnFHY2ljWkhHY0JXdFdpYmxRb1RFWWdZcUlGQVJ3R0UydDhDdTZKeHZibTFNcDMxMGlhWlU1ekhYazZTNzhpYldUcDlodzQ4RE4wQ3U0ZUlIaWFjRVVnLw" class="" title="img">
</li>
</ul>
<p>==从逻辑结构角度划分：==</p>
<ul>
<li>
<p><strong>聚簇索引</strong>：聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。聚簇索引将索引和数据行放到了一块，找到索引也就找到了数据。因为无需进行回表操作，所以效率很高。<strong>InnoDB 中必然会有，且只会有一个聚簇索引</strong>。通常是主键，如果没有主键，则优先选择非空的唯一索引，如果唯一索引也没有，则会创建一个隐藏的row_id 作为聚簇索引。至于为啥会只有一个聚簇索引，其实很简单，因为我们的数据只会存储一份。</p>
<ul>
<li>优点：
<ul>
<li>数据访问更快，因为聚簇索引将索引和数据保存在同一个B+树中，因此从聚簇索引中获取数据比非聚簇索引更快</li>
<li>聚簇索引对于主键的排序查找和范围查找速度非常快</li>
</ul>
</li>
<li>缺点：
<ul>
<li>插入速度严重依赖于插入顺序，按照主键的<strong>顺序插入</strong>是最快的方式，否则将会出现页分裂，严重影响性能。因此，对于InnoDB表，我们一般都会定义一个<strong>自增的ID列为主键</strong></li>
<li><strong>更新主键的代价很高</strong>，因为将会导致被更新的行移动。因此，对于InnoDB表，我们一般定义主键为不可更新。</li>
<li>二级索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>非聚簇索引</strong>：在<strong>聚簇索引之上创建的索引称之为辅助索引</strong>，辅助索引访问数据总是需要二次查找。辅助索引叶子节点存储的不再是行的物理位置，而是主键值。通过辅助索引首先找到的是主键值，再通过主键值找到数据行的数据页，再通过数据页中的Page Directory找到数据行。</p>
<p>​				1、为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。</p>
<p>2、用非单调的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择。</p>
</li>
<li>
<p><strong>联合索引</strong>：联合索引又叫复合索引。对于复合索引:Mysql从左到右的使用索引中的字段，一个查询可以只使用索引中的一部份，但只能是最左侧部分。例如索引是key index (a,b,c). 可以支持<strong>a</strong> | <strong>a,b</strong>| <strong>a,b,c</strong> 3种组合进行查找，但不支持 b,c进行查找 .当最左侧字段是常量引用时，索引就十分有效。</p>
</li>
<li>
<p><strong>唯一索引</strong></p>
</li>
<li>
<p><strong>普通索引</strong></p>
</li>
<li>
<p><strong>覆盖索引</strong>：覆盖索引（covering index）指一个<strong>查询语句的执行只用从索引中就能够取得</strong>，不必从数据表中读取。也可以称之为实现了索引覆盖。 当一条查询语句符合覆盖索引条件时，MySQL只需要通过索引就可以返回查询所需要的数据，这样避免了查到索引后再返回表操作，减少I/O提高效率。 如，表covering_index_sample中有一个普通索引 idx_key1_key2(key1,key2)。当我们通过SQL语句：select key2 from covering_index_sample where key1 = ‘keytest’;的时候，就可以通过覆盖索引查询，无需回表。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/aHR0cHM6Ly9tbWJpei5xcGljLmNuL3N6X21tYml6X3BuZy9LUlJ4dnFHY2ljWkhHY0JXdFdpYmxRb1RFWWdZcUlGQVJ3eDB5c3FQZ3JlemJteVFCdFRqMmJYWGZpYjR5aWJTZmxBNWhjT0JXZ0NpYXNjVUNvaWJDUDV1WmZuQS8" class="" title="img">
</li>
<li>
<p>**自适应哈希索引(adaptive hash index）：**InnoDB 会监控对表上索引的查找，如果观察到某些索引被频繁访问，索引成为热数据，建立哈希索引可以带来速度的提升，则建立哈希索引，所以称之为自适应（adaptive）的。自适应哈希索引通过缓冲池的 B+ 树构造而来，因此建立的速度很快。而且不需要将整个表都建哈希索引，InnoDB 会自动根据访问的频率和模式来为某些页建立哈希索引。</p>
</li>
</ul>
<h2 id="【InnoDB引擎实现方式】">【InnoDB引擎实现方式】</h2>
<p>InnoDB<strong>也使用B+Tree作为索引结构</strong>，但具体实现方式却与MyISAM截然不同.</p>
<p>1）主键索引：</p>
<p><strong>MyISAM索引文件和数据文件是分离的</strong>，<strong>索引文件仅保存数据记录的地址</strong>。而在<strong>InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构</strong>，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1464190-20191106145200302-932404581.png" class="" title="img">
<p>(图inndb主键索引）是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚簇索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则*<em>MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段（row_id）<em>长度为6个字节，类型为长整形</em></em>。</p>
<p>2）InnoDB的辅助索引</p>
<p>InnoDB的所有辅助索引都引用主键作为data域。例如，下图为定义在Col3上的一个辅助索引：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1464190-20191106145241480-1330791289.png" class="" title="img">
<p>InnoDB 表是基于聚簇索引建立的。因此InnoDB 的索引能提供一种非常快速的主键查找性能。不过，它的辅助索引（Secondary Index， 也就是非主键索引）也会包含主键列，所以，<strong>如果主键定义的比较大，其他索引也将很大</strong>。如果想在表上定义 、很多索引，则争取尽量把主键定义得小一些。InnoDB 不会压缩索引。</p>
<p>文字符的ASCII码作为比较准则。聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。</p>
<p>不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了InnoDB的索引实现后，就很容易明白：</p>
<p>1、为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，<strong>过长的主索引会令辅助索引变得过大。</strong></p>
<p>2、用非单调的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，<strong>非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整</strong>，十分低效，而使用自增字段作为主键则是一个很好的选择。</p>
<p>InnoDB使用的是聚簇索引，将主键组织到一棵B+树中，而行数据就储存在叶子节点上，若使用&quot;where id = 14&quot;这样的条件查找主键，则按照B+树的检索算法即可查找到对应的叶节点，之后获得行数据。若对Name列进行条件搜索，则需要两个步骤：第一步在辅助索引B+树中检索Name，到达其叶子节点获取对应的主键。第二步使用主键在主索引B+树种再执行一次B+树检索操作，最终到达叶子节点即可获取整行数据。</p>
<h2 id="【MyISAM索引实现】">【MyISAM索引实现】</h2>
<p>MyISAM<strong>索引文件和数据文件是分离的</strong>，索引文件仅保存数据记录的地址</p>
<p>1）主键索引：</p>
<p>MyISAM引擎使用B+Tree作为索引结构，叶节点的<strong>data域存放的是数据记录的地址</strong>。下图是MyISAM主键索引的原理图：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1464190-20191106151308457-531875534.png" class="" title="img">
<p>这里设表一共有三列，假设我们以Col1为主键，图myisam1是一个MyISAM表的主索引（Primary key）示意。可以看出MyISAM的索引文件仅仅保存数据记录的地址。</p>
<p>2）辅助索引（Secondary key）</p>
<p>在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，<strong>只是主索引要求key是唯一的，而辅助索引的key可以重复</strong>。如果我们在Col2上建立一个辅助索引，则此索引的结构如下图所示：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1464190-20191106151427711-625351515.png" class="" title="img">
<p>同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。</p>
<p>MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1464190-20191106151527647-152458631.png" class="" title="img">
<h2 id="【最左前缀匹配原则】">【最左前缀匹配原则】</h2>
<p>在MySQL建立联合索引时会遵守最左前缀匹配原则，即最左优先，在检索数据时从联合索引的最左边开始匹配。</p>
<p>举例：创建一个（a,b）的联合索引，那么它的索引树就是下图的样子。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1804577-20200521182659976-48843100.png" class="" title="img">
<p>可以看到a的值是有顺序的，1，1，2，2，3，3，而b的值是没有顺序的1，2，1，4，1，2。但是我们又可发现a在等值的情况下，b值又是按顺序排列的，但是这种顺序是相对的。这是因为<strong>MySQL创建联合索引的规则是首先会对联合索引的最左边第一个字段排序，在第一个字段的排序基础上，然后在对第二个字段进行排序</strong>。所以b=2这种查询条件没有办法利用索引。</p>
<p>由于整个过程是基于explain结果分析的，那接下来在了解下explain中的type字段和key_lef字段。</p>
<p><strong>1.type</strong>：<strong>联接类型。下面给出各种联接类型,按照从最佳类型到最坏类型进行排序:（重点看ref,rang,index）</strong></p>
<p>system：表只有一行记录（等于系统表），这是const类型的特例，平时不会出现，可以忽略不计<br>
　　　　const：表示通过索引一次就找到了，const用于比较primary key 或者 unique索引。因为只需匹配一行数据，所有很快。如果将主键置于where列表中，mysql就能将该查询转换为一个const<br>
　　　　eq_ref：唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配。常见于主键 或 唯一索引扫描。<br>
　　　　注意：ALL全表扫描的表记录最少的表如t1表<br>
　　　　<strong>ref</strong>：非唯一性索引扫描，返回匹配某个单独值的所有行。本质是也是一种索引访问，它返回所有匹配某个单独值的行，然而他可能会找到多个符合条件的行，所以它应该属于查找和扫描的混合体。<br>
　　　　<strong>range</strong>：只检索给定范围的行，使用一个索引来选择行。key列显示使用了那个索引。一般就是在where语句中出现了bettween、&lt;、&gt;、in等的查询。这种索引列上的范围扫描比全索引扫描要好。<strong>只需要开始于某个点，结束于另一个点</strong>，<strong>不用扫描全部索引。</strong><br>
　　　　<strong>index</strong>：Full Index Scan，index与ALL区别为index类型只遍历索引树。这通常为ALL块，应为索引文件通常比数据文件小。（Index与ALL虽然都是读全表，但index是从索引中读取，而ALL是从硬盘读取）<br>
　　　　ALL：Full Table Scan，遍历全表以找到匹配的行</p>
<p><strong>2.key_len</strong>：<strong>显示MySQL实际决定使用的索引的长度。如果索引是NULL，则长度为NULL。如果不是NULL，则为使用的索引的长度。所以通过此字段就可推断出使用了那个索引。</strong></p>
<p>计算规则：</p>
<p>1.定长字段，int占用4个字节，date占用3个字节，char(n)占用n个字符。</p>
<p>2.变长字段varchar(n)，则占用n个字符+两个字节。</p>
<p>3.不同的字符集，一个字符占用的字节数是不同的。Latin1编码的，一个字符占用一个字节，gdk编码的，一个字符占用两个字节，utf-8编码的，一个字符占用三个字节。</p>
<p>（由于我数据库使用的是Latin1编码的格式，所以在后面的计算中，一个字符按一个字节算）</p>
<p>4.对于所有的索引字段，如果设置为NULL，则还需要1个字节。</p>
<h3 id="1-全值匹配查询时"><strong>1.全值匹配查询时</strong></h3>
<p>​	<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1804577-20200521202947593-2126832810.png" class="" title="img"></p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1804577-20200521203003652-366842075.png" class="" title="img">
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1804577-20200521203020127-1655735915.png" class="" title="img">
<p>通过观察上面的结果图可知，where后面的查询条件，不论是使用（id，age，name）（name，id，age）还是（age，name，id）顺序，在查询时都使用到了联合索引，可能有同学会疑惑，为什么底下两个的搜索条件明明没有按照联合索引从左到右进行匹配，却也使用到了联合索引？ 这是因为<strong>MySQL中有查询优化器explain，所以sql语句中字段的顺序不需要和联合索引定义的字段顺序相同，查询优化器会判断纠正这条SQL语句以什么样的顺序执行效率高</strong>，最后才能生成真正的执行计划，所以不论以何种顺序都可使用到联合索引。另外通过观察上<strong>面三个图中的key_len字段，也可说明在搜索时使用的联合索引中的（id_name_age）索引</strong>，因为id为int型，允许null，所以占5个字节，name为char(10)，允许null，又使用的是latin1编码，所以占11个字节，age为int型允许null，所以也占用5个字节，所以该索引长度为21（5+11+5），而上面key_len的值也正好为21，<strong>可证明使用的（id_name_age）索引</strong>。</p>
<h3 id="2-匹配最左边的列时"><strong>2.匹配最左边的列时</strong></h3>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1804577-20200521202447168-1029938685.png" class="" title="img">
<p>该搜索是遵循最左匹配原则的，通过key字段也可知，在搜索过程中使用到了联合索引，且使用的是联合索引中的（id）索引，因为<strong>key_len字段值为5，而id索引的长度正好为5</strong>（因为id为int型，允许null，所以占5个字节）。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1804577-20200521202737461-1486677111.png" class="" title="img">
<p><strong>由于id到name是从左边依次往右边匹配，这两个字段中的值都是有序的，所以也遵循最左匹配原则</strong>，通过key字段可知，在搜索过程中也使用到了联合索引，但使用的是联合索引中的（id_name）索引，因为key_len字段值为16，而(id_name)索引的长度正好为16（因为id为int型，允许null，所以占5个字节，name为char(10)，允许null，又使用的是latin1编码，所以占11个字节）。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1804577-20200521202810363-1061003410.png" class="" title="img">
<p>由于上面三个搜索都是从最左边id依次向右开始匹配的，所以都用到了id_name_age_index联合索引。</p>
<p><strong>==那如果不是依次匹配呢？==</strong></p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1804577-20200521204203700-302471529.png" class="" title="img">
<p>通过key字段可知，在搜索过程中也使用到了联合索引，但使用的是联合索引中的（id）索引，从key_len字段也可知。因为联合索引树是按照id字段创建的，<strong>但age相对于id来说是无序的，只有id只有序的，所以他只能使用联合索引中的id索引。</strong></p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1804577-20200521203757147-65081383.png" class="" title="img">
<p>通过观察发现上面key字段发现在搜索中也使用了id_name_age_index索引，可能许多同学就会疑惑它并没有遵守最左匹配原则，按道理会索引失效，为什么也使用到了联合索引？<strong>因为没有从id开始匹配，且name单独来说是无序的，所以它确实不遵循最左匹配原则，然而从type字段可知，它虽然使用了联合索引，但是它是对整个索引树进行了扫描，正好匹配到该索引，与最左匹配原则无关</strong>，一般只要是某联合索引的一部分，但又不遵循最左匹配原则时，都可能会采用index类型的方式扫描，但它的效率远不如最做匹配原则的查询效率高，index类型类型的扫描方式是从索引第一个字段一个一个的查找，直到找到符合的某个索引，<strong>与all不同的是，index是对所有索引树进行扫描，而all是对整个磁盘的数据进行全表扫描。</strong></p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1804577-20200521203731486-172947522.png" class="" title="img">
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1804577-20200521203818877-809915868.png" class="" title="img">
<p>这两个结果跟上面的是同样的道理，由于它们都没有从最左边开始匹配，所以没有用到联合索引，使用的<strong>都是index全索引扫描。</strong></p>
<h3 id="3-匹配列前缀"><strong>3.匹配列前缀</strong></h3>
<p>如果id是字符型，那么前缀匹配用的是索引，中坠和后缀用的是全表扫描。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> staffs <span class="hljs-keyword">where</span> id <span class="hljs-keyword">like</span> <span class="hljs-string">&#x27;A%&#x27;</span>;<span class="hljs-operator">/</span><span class="hljs-operator">/</span>前缀都是排好序的，使用的都是联合索引<br><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> staffs <span class="hljs-keyword">where</span> id <span class="hljs-keyword">like</span> <span class="hljs-string">&#x27;%A%&#x27;</span>;<span class="hljs-operator">/</span><span class="hljs-operator">/</span>全表查询<br><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> staffs <span class="hljs-keyword">where</span> id <span class="hljs-keyword">like</span> <span class="hljs-string">&#x27;%A&#x27;</span>;<span class="hljs-operator">/</span><span class="hljs-operator">/</span>全表查询<br></code></pre></td></tr></table></figure>
<h3 id="4-匹配范围值"><strong>4.匹配范围值</strong></h3>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1804577-20200521210125009-1177423028.png" class="" title="img">
<p>在匹配的过程中<strong>遇到&lt;&gt;=号，就会停止匹配</strong>，但id本身就是有序的，所以通过possible_keys字段和key_len 字段可知，在该搜索过程中<strong>使用了联合索引的id索引</strong>（因为id为int型，允许null，所以占5个字节），且进行的是rang范围查询。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1804577-20200522115901524-226070462.png" class="" title="img">
<p>由于不遵循最左匹配原则，且在<strong>id&lt;4的范围中，age是无序的，所以使用的是index全索引扫描</strong>。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1804577-20200521210146172-1428008775.png" class="" title="img">
<p>不遵循最左匹配原则，但在数据库中id&lt;2的只有一条（id），所以在<strong>id&lt;2的范围中，age是有序的，所以使用的是rang范围查询。</strong><img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1804577-20200521210203002-1943736888.png" class="" title="img"></p>
<p>不遵循最左匹配原则，而age又是无序的，所以进行的<strong>全索引扫描</strong>。</p>
<h3 id="5-准确匹配第一列并范围匹配其他某一列"><strong>5.准确匹配第一列并范围匹配其他某一列</strong></h3>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E7%B4%A2%E5%BC%95/1804577-20200521210726024-2037980339.png" class="" title="img">
<p>由于搜索中有id=1，所以在id范围内age是无序的，所以只<strong>使用了联合索引中的id索引。</strong></p>
<h2 id="【索引优化问题】">【索引优化问题】</h2>
<p><strong>1.创建索引</strong></p>
<p>对于查询占主要的应用来说，索引显得尤为重要。很多时候性能问题很简单的就是因为我们忘了添加索引而造成的，或者说没有添加更为有效的索引导致。如果不加</p>
<p>索引的话，那么查找任何哪怕只是一条特定的数据都会进行一次全表扫描，如果一张表的数据量很大而符合条件的结果又很少，那么不加索引会引起致命的性能下降。但是也不是什么情况都非得建索引不可，<strong>比如性别可能就只有两个值，建索引不仅没什么优势，还会影响到更新速度，这被称为过度索引。</strong></p>
<p><strong>2.复合索引</strong></p>
<p>比如有一条语句是这样的：select * from users where area=’beijing’ and age=22;</p>
<p>如果我们是在area和age上<strong>分别创建</strong>单个索引的话，由于mysql查询每次只能使用<strong>一个索引</strong>，所以虽然这样已经相对不做索引时全表扫描提高了很多效率，但是如果在area、age两列上创建复合索引的话将带来更高的效率。如果我们创建了(area, age,salary)的复合索引，那么其实相当于创建了(area,age,salary)、(area,age)、(area)三个索引，这被称为最佳左前缀特性。<strong>因此我们在创建复合索引时应该将最常用作限制条件的列放在最左边，依次递减。</strong></p>
<p><strong>3.索引不会包含有NULL值的列</strong></p>
<p>只要列中包含有NULL值都将不会被包含在索引中，<strong>复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的</strong>。所以我们在数据库设计时不要让字段的默认值为NULL。</p>
<p><strong>4.使用短索引</strong></p>
<p><strong>对串列进行索引，如果可能应该指定一个前缀长度</strong>。例如，如果有一个CHAR(255)的 列，如果在前10 个或20 个字符内，多数值是惟一的，那么就不要对整个列进行索引。<strong>短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。</strong></p>
<p><strong>5.排序的索引问题</strong></p>
<p>mysql查询只使用一个索引，<strong>因此如果where子句中已经使用了索引的话，那么order by中的列是不会使用索引的</strong>。因此数据库默认排序可以<strong>符合要求的情况下不要使用排序操作</strong>；尽量不要包含<strong>多个列的排序</strong>，如果需要最好给这些列创建<strong>复合索引</strong>。</p>
<p><strong>6.like语句操作</strong></p>
<p>一般情况下不鼓励使用like操作，如果非使用不可，如何使用也是一个问题。l<strong>ike “%aaa%” 不会使用索引而like “aaa%”可以使用索引。</strong></p>
<p><strong>7.不要在列上进行运算</strong></p>
<p>select * from users where YEAR(adddate)</p>
<p><strong>8.不使用NOT IN</strong></p>
<p>NOT IN都不会使用索引将进行全表扫描。NOT IN可以<strong>NOT EXISTS代替</strong></p>
<h3 id="【慢SQL优化】">【慢SQL优化】</h3>
<h4 id="【如何分析慢SQL语句】">【如何分析慢SQL语句】</h4>
<ul>
<li>通过慢日志检查</li>
<li>通过explain</li>
</ul>
<p>首先要搞明白慢的原因是什么：是查询条件没有命中索引？还是 load 了不需要的数据列？还是数据量太大？所以优化也是针对这三个方向来的。</p>
<h4 id="【SQL语句优化】">【SQL语句优化】</h4>
<ul>
<li>
<p>首先用 explain 分析语句的执行计划，查看使用索引的情况，是不是查询没走索引，如果可以加索引解决，优先采用加索引解决。</p>
</li>
<li>
<p>分析语句，看看是否存在一些导致索引失效的用法，是否 load 了额外的数据，是否加载了许多结果中并不需要的列，对语句进行分析以及重写。慎用部分关键字，如distinct、union关键字等。</p>
</li>
<li>
<p>如果对语句的优化已经无法进行，可以考虑表中的数据量是否太大，如果是的话可以进行垂直拆分或者水平拆分。</p>
</li>
<li>
<p>范围查询，比如在查询某条语句时，加入limit 1,等限制。</p>
</li>
<li>
<p><strong>保证不查询多余的列与行。</strong></p>
</li>
<li>
<p>如果你同时修改或删除过多数据，会造成cpu利用率过高从而影响别人对数据库的访问。如果你删除或修改过多数据，采用单一循环操作，那么会是效率很低，也就是操作时间过程会很漫长。</p>
<p>这样你该怎么做呢？</p>
<p>折中的办法就是，分批操作数据。</p>
</li>
<li>
<p>连接查询的优化</p>
<ul>
<li>
<p>首先你要弄明白你想要的数据是什么样子的，然后再做出决定使用哪一种连接，这很重要。</p>
<p>各种连接的取值大小为：</p>
<ul>
<li>内连接结果集大小取决于左右表满足条件的数量</li>
<li>左连接取决与左表大小，右相反。</li>
<li>完全连接和交叉连接取决与左右两个表的数据总数量</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="【hash】vs【红黑树】vs【B树】vs【B-树】">【hash】vs【红黑树】vs【B树】vs【B+树】</h2>
<p><strong>红黑树</strong>：如果在内存中，红黑树的查找效率比B树更高，但是涉及到磁盘操作，B树就更优了。因为红黑树是二叉树，数据量大时树的层数很高，从树的根结点向下寻找的过程，每读1个节点，都相当于一次IO操作，因此红黑树的I/O操作会比B树多的多。</p>
<p><strong>hash 索引</strong>：如果只查询单个值的话，hash 索引的效率非常高。但是 hash 索引有几个问题：1）不支持范围查询；2）不支持索引值的排序操作；3）不支持联合索引的最左匹配规则。</p>
<p><strong>B树索引</strong>：B树索相比于B+树，在进行范围查询时，需要做局部的中序遍历，可能要跨层访问，跨层访问代表着要进行额外的磁盘I/O操作；另外，B树的非叶子节点存放了数据记录的地址，会导致存放的节点更少，树的层数变高。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/dajiangdahe.github.io/tags/Mysql/" rel="tag">Mysql</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Mysql主从复制"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/"
    >Mysql主从复制</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/" class="article-date">
  <time datetime="2021-10-22T07:21:44.964Z" itemprop="datePublished">2021-10-22</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/dajiangdahe.github.io/categories/Mysql/">Mysql</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1>1.什么是主从复制</h1>
<p>将主数据库中的DDL和DML操作通过二进制日志传输到从数据库上，然后将这些日志重新执行（重做）；从而使得从数据库的数据与主数据库保持一致。</p>
<h1>2.基本原理</h1>
<p>MySQL支持单向、异步复制，复制过程中一个服务器充当主服务器，而一个或多个其它服务器充当从服务器。</p>
<p>MySQL复制是基于主服务器在二进制日志中跟踪所有对数据库的更改。因此，要进行复制，必须在主服务器上启用二进制日志（binlog日志）。每个从服务器从主服务器接收主服务器已经记录到日志的数据。</p>
<p>当一个从服务器连接主服务器时，它通知主服务器从服务器在日志中读取的最后一次成功更新的位置。从服务器接收从那时起发生的任何更新，并在本机上执行相同的更新。然后封锁并等待主服务器通知新的更新。从服务器执行备份不会干扰主服务器，在备份过程中主服务器可以继续处理更新。</p>
<h1>3.主从复制的作用</h1>
<p>1、主数据库出现问题，可以切换到从数据库。</p>
<p>2、可以进行数据库层面的读写分离。</p>
<p>3、可以在从数据库上进行日常备份。</p>
<h1>4.复制过程</h1>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/1606768-20190408141304873-785770801.png" class="" title="img">
<p><em>Binary log：主数据库的二进制日志。</em></p>
<p><em>Relay log：从服务器的中继日志。</em></p>
<p>**第一步：**master在每个事务更新数据完成之前，将该操作记录串行地写入到binlog文件中。</p>
<p>**第二步：**salve开启一个I/O Thread，该线程在master打开一个普通连接，主要工作是binlog dump process。如果读取的进度已经跟上了master，就进入睡眠状态并等待master产生新的事件。I/O线程最终的目的是将这些事件写入到中继日志中。</p>
<p>**第三步：**SQL Thread会读取中继日志，并顺序执行该日志中的SQL事件，从而与主数据库中的数据保持一致。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/dajiangdahe.github.io/tags/Mysql/" rel="tag">Mysql</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Mysql中的锁"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/"
    >Mysql中的锁</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/" class="article-date">
  <time datetime="2021-10-22T07:21:44.937Z" itemprop="datePublished">2021-10-22</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/dajiangdahe.github.io/categories/Mysql/">Mysql</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1>Mysql中的锁</h1>
<p><strong>根据加锁的范围，MySQL里面的锁大致可以分成全局锁、表级锁和行锁三类。</strong></p>
<h2 id="1-全局锁">1.全局锁</h2>
<p>全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql">Flush tables <span class="hljs-keyword">with</span> read lock<br></code></pre></td></tr></table></figure>
<p>当需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。</p>
<p>**全局锁的典型使用场景是，做全库逻辑备份。**也就是把整库每个表都select出来存成文本</p>
<p>但是让整个库都只读，可能出现以下问题：</p>
<ul>
<li>如果在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆</li>
<li>果在从库上备份，那么在备份期间从库不能执行主库同步过来的binlog，会导致主从延迟<br>
在可重复读隔离级别下开启一个事务能够拿到一致性视图</li>
</ul>
<p>官方自带的逻辑备份工具是<strong>mysqldump</strong>。当mysqldump使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。single-transaction只适用于所有的表使用事务引擎的库。</p>
<p><strong>1.既然要全库只读，为什么不使用set global readonly=true的方式？</strong></p>
<ul>
<li>在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此修改global变量的方式影响面更大</li>
<li>在异常处理机制上有差异。如果执行Flush tables with read lock命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高</li>
</ul>
<h2 id="2-表级锁">2.表级锁</h2>
<p><strong>MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL）</strong></p>
<p>表锁的语法是lock tables … read/write。可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象</p>
<p>如果在某个线程A中执行</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql">lock tables t1 read,t2 wirte;<br></code></pre></td></tr></table></figure>
<p>这个语句，则其他线程写t1、读写t2的语句都会被阻塞。同时，线程A在执行unlock tables之前，也只能执行读t1、读写t2的操作。连写t1都不允许</p>
<p>另一类表级的锁是MDL。MDL不需要显式使用，在访问一个表的时候会被自动加上。**MDL的作用是，保证读写的正确性。**如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做了变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定不行</p>
<p>在MySQL5.5版本引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。</p>
<ul>
<li><strong>读锁之间不互斥，因此可以有多个线程同时对一张表增删改查</strong></li>
<li><strong>读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行</strong></li>
</ul>
<p>给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。在对大表操作的时候，需要特别小心，以免对线上服务造成影响。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/201906051620443.png" class="" title="在这里插入图片描述">
<p>session A先启动，这时候会对表t加一个MDL读锁。由于session B需要的也是MDL读锁，因此可以正常执行。之后sesession C会被blocked，是因为session A的MDL读锁还没有释放，而session C需要MDL写锁，因此只能被阻塞。如果只有session C自己被阻塞还没什么关系，但是之后所有要在表t上新申请MDL读锁的请求也会被session C阻塞。所有对表的增删改查操作都需要先申请MDL读锁，就都被锁住，等于这个表现在完全不可读写了。</p>
<p><strong>事务中的MDL锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放</strong></p>
<p><strong>1.如果安全地给小表加字段？</strong></p>
<p>首先要解决长事务，事务不提交，就会一直占着DML锁。在MySQL的information_schema库的innodb_trx表中，可以查到当前执行的事务。如果要做DDL变更的表刚好有长事务在执行，要考虑先暂停DDL，或者kill掉这个长事务</p>
<p><strong>2.如果要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而又不得不加个字段，该怎么做？</strong></p>
<p>在alter table语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后再通过重试命令重复这个过程。</p>
<h2 id="3-行级锁">3.行级锁</h2>
<p>MySQL的行锁是在引擎层由各个引擎自己实现的。但不是所有的引擎都支持行锁，比如MyISAM引擎就不支持行锁</p>
<p>行锁就是针对数据表中行记录的锁。比如事务A更新了一行，而这时候事务B也要更新同一行，则必须等事务A的操作完成后才能进行更新。</p>
<h3 id="1、两阶段锁协议">1、两阶段锁协议</h3>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162055774.png" class="" title="在这里插入图片描述">
<p>事务A持有的两个记录的行锁都是在commit的时候才释放的，事务B的update语句会被阻塞，直到事务A执行commit之后，事务B才能继续执行。</p>
<p><strong>在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议</strong></p>
<p><strong>如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放</strong></p>
<p>【<strong>示例</strong>】</p>
<p>假设要实现一个电影票在线交易业务，顾客A要在影院B购买电影票。业务需要涉及到以下操作：</p>
<p>1.从顾客A账户余额中扣除电影票价</p>
<p>2.给影院B的账户余额增加这张电影票价</p>
<p>3.记录一条交易日志</p>
<p>为了保证交易的原子性，要把这三个操作放在一个事务中。如何安排这三个语句在事务中的顺序呢？</p>
<p>如果同时有另外一个顾客C要在影院B买票，那么这两个事务冲突的部分就是语句2了。因为它们要更新同一个影院账户的余额，需要修改同一行数据。根据两阶段锁协议，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果把语句2安排在最后，比如按照3、1、2这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。</p>
<h3 id="2、死锁和死锁检测">2、死锁和死锁检测</h3>
<p><strong>在并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁</strong>。</p>
<p><strong>【死锁的条件】：</strong></p>
<ul>
<li>
<p>互斥条件：简单的说就是进程抢夺的资源必须是临界资源，一段时间内，该资源只能同时被一个进程所占有。</p>
</li>
<li>
<p>请求和保持条件：当一个进程持有了一个（或者更多）资源，申请另外的资源的时候发现申请的资源被其他进程所持有，当前进程阻塞，但不会是放自己所持有的资源。</p>
</li>
<li>
<p>不可抢占条件：进程已经获得的资源在未使用完毕的情况下不可被其他进程所抢占。</p>
</li>
<li>
<p>循环等待条件：存在一个封闭的进程链，使得每个进程至少占有此链中下一个进程所需要的一个资源。</p>
</li>
</ul>
<p><strong>【预防死锁】：</strong></p>
<ol>
<li>
<p>破坏请求和保持条件：</p>
<p>1.所有进程在开始运行之前，必须一次性获得所有资源，如果无法获得完全，释放已经获得的资源，等待；</p>
<p>2.所有进程在开始运行之前，只获得初始运行所需要的资源，然后在运行过程中不断请求新的资源，同时释放自己已经用完的资源。</p>
</li>
</ol>
<p>2.破坏不可抢占条件：</p>
<p>​		1.只要当一个进程申请一个资源，然而却申请不到的时候，必须释放已经申请到的所有资源。</p>
<p>3.破坏循环等待条件：</p>
<p>​		1.设立一个规则，让进程获取资源的时候按照一定的顺序依次申请，不能违背这个顺序的规则。必须按照顺序申请和释放，想要申请后面的资源必须先把该资源之前的资源全部申请，想要申请前面的资源必须先把该资源之后的资源。</p>
<p>4.破坏互斥条件：</p>
<p>​		没法破坏，是资源本身的性质所引起的。</p>
<p><strong>【死锁避免】：</strong></p>
<p>1.有序资源分配法：</p>
<p>​	这种算法资源按某种规则系统中的所有资源统一编号（例如打印机为1,、磁带机为2、磁盘为3、等等），申请时必须以上升的次序。</p>
<p>​	采用有序资源分配法：R1的编号为1，R2的编号为2,；PA：申请次序应是：R1，R2；PB：申请次序应是：R1，R2；这样就破坏了环路条件，避免了死锁的发生。</p>
<p>2.银行家算法：</p>
<p>​	该算法需要检查申请者对资源的最大需求量，如果系统现存的各类资源可以满足申请者的请求，就满足申请者的请求。这样申请者就可以很快完成其计算，然后释放它占用的资源，从而保证了系统中的所有进程都能完成，所以可避免死锁的发生。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162105380.png" class="" title="在这里插入图片描述">
<p>事务A在等待事务B释放id=2的行锁，而事务B在等待事务A释放id=1的行锁。事务A和事务B在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略：</p>
<p>一种策略是，<strong>直接进入等待，直到超时</strong>。这个超时时间可以通过参数innodb_lock_wait_timeout来设置<br>
另一种策略是，<strong>发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务</strong>，让其他事务得以继续执行。将参数<strong>innodb_deadlock_detect设置为on</strong>，表示开启这个逻辑。</p>
<p>在InnoDB中，<strong>innodb_lock_wait_timeout的默认值是50s</strong>，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过50s才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的</p>
<p>正常情况下还是要采用主动死锁检查策略，而且innodb_deadlock_detect的默认值本身就是on。主动死锁监测在发生死锁的时候，是能够快速发现并进行处理的，但是它有额外负担的。每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁</p>
<p>如果所有事务都要更新同一行的场景，每个新来的被堵住的线程都要判断会不会由于自己的加入导致死锁，这是一个时间复杂度是O(n)的操作。</p>
<p>【<strong>示例</strong>】</p>
<p><strong>怎么解决由这种热点行更新导致的性能问题？</strong></p>
<p>1.如果确保这个业务一定不会出现死锁，可以临时把死锁检测关掉</p>
<p>2.控制并发度</p>
<p>3.将一行改成逻辑上的多行来减少锁冲突。以影院账户为例，可以考虑放在多条记录上，比如10个记录，影院的账户总额等于这10个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成员原来的1/10，可以减少锁等待个数，也就减少了死锁检测的CPU消耗。</p>
<h1>Mysql中的锁</h1>
<p>根据锁的设计思想，MySQL里面的锁大致分为乐观锁和悲观锁。</p>
<h2 id="1、乐观锁和悲观锁的澄清">1、乐观锁和悲观锁的澄清</h2>
<ul>
<li>无论是悲观锁还是乐观锁，他们本质上不是数据库中具体的锁概念，而是我们定义出来，用来描述两种类别的锁的思想。所以有了设计的分类，我们就可以通过这个分类去对数据库中具体的锁进行分门别类；</li>
<li>不过数据库中的乐观锁更倾向叫乐观并发控制（OCC），悲观锁叫悲观并发控制（PCC），还有区别于乐观悲观锁的一种控制叫MVCC，多版本并发控制</li>
<li>也不要把乐观锁和悲观锁与数据库中的行锁，表锁，排他锁，共享锁混为一谈，他们并不是一个维度的东西；前者是一个锁思想，可以将后者根据是否进行趋近于乐观或悲观锁的思想进行分类</li>
<li>乐观锁和悲观锁的概念不仅仅存在于数据库领域，可以说存在线程安全，存在并发的场景几乎都有乐观锁和悲观锁的适用场景，比如Java中也有乐观锁和悲观锁思想的具体实现；但不同领域的乐观和悲观锁的具体实现都不尽相同，要解决的问题也可能有所不一样</li>
</ul>
<h2 id="1、悲观锁">1、悲观锁</h2>
<h3 id="【概念】">【概念】</h3>
<p>在关系数据库管理系统里，悲观并发控制（又名“悲观锁”，Pessimistic Concurrency Control，缩写“PCC”）是一种并发控制的方法; <strong>悲观锁指的是采用一种持悲观消极的态度，默认数据被外界访问时，必然会产生冲突，所以在数据处理的整个过程中都采用加锁的状态，保证同一时间，只有一个线程可以访问到数据，实现数据的排他性</strong>；通常，数据库的悲观锁是利用数据库本身提供的锁机制去实现的.</p>
<p>数据库的悲观并发控制可以解决<strong>读-写冲突和写-写冲突</strong>,指在用<strong>加锁的方式</strong>去解决</p>
<hr>
<h3 id="【实现】">【实现】</h3>
<p><strong>通常情况下，数据库的悲观锁就是利用数据库本身提供的锁去实现的</strong></p>
<ul>
<li>外界要访问某条数据，那它就要首先向数据库申请该数据的锁(某种锁)</li>
<li>如果获得成功，那它就可以操作该数据，在它操作期间，其他客户端就无法再操作该数据了</li>
<li>如果获得失败，则代表同一时间已有其他客户端获得了该锁，那就必须等待其他客户端释放锁</li>
</ul>
<p>当然数据库提供了非常多的锁，每种数据库提供的锁也不尽然相同，所以具体情况就要看是什么锁了,比如行锁，表锁等。</p>
<h3 id="【优缺点】">【优缺点】</h3>
<p>**优点：**适合在写多读少的并发环境中使用，虽然无法维持非常高的性能，但是在乐观锁无法提更好的性能前提下，可以做到数据的安全性<br>
**缺点：**加锁会增加系统开销，虽然能保证数据的安全，但数据处理吞吐量低，不适合在读书写少的场合下使用。</p>
<h2 id="2、乐观锁">2、乐观锁</h2>
<h3 id="【概念】-2">【概念】</h3>
<p>在关系数据库管理系统里，乐观并发控制（又名“乐观锁”，Optimistic Concurrency Control，缩写“OCC”）是一种并发控制的方法；乐观锁（ Optimistic Locking ） 是相对悲观锁而言，<strong>乐观锁是假设认为即使在并发环境中，外界对数据的操作一般是不会造成冲突，所以并不会去加锁(所以乐观锁不是一把锁)，而是在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测</strong>，如果发现冲突了，则让返回冲突信息，让用户决定如何去做下一步，比如说重试，直至成功为止；数据库的乐观锁，并不是利用数据库本身的锁去实现的，可能是利用某种实现逻辑去实现做到乐观锁的思想</p>
<p>数据库的乐观并发控制要解决的是数据库并发场景下的<strong>写-写冲突</strong>，指在用<strong>无锁的方式</strong>去解决</p>
<hr>
<h3 id="【CAS思想】">【CAS思想】</h3>
<p>其实数据库乐观锁的具体实现几乎就跟Java中乐观锁采用的CAS算法思想是一致，所以我们可以从CAS算法中学习到数据库乐观锁的设计：</p>
<p>CAS指令全称为Compare and Swap，它是系统的指令集，<strong>整个CAS操作是一个原子操作，是不可分割的</strong>。从具体的描述上，我们可以这么看CAS操作：</p>
<hr>
<p><strong>CAS指令需要3个操作数，分别是内存位置V，旧的预期值A,和新值B。CAS指令执行时，当我们读取的内置位置V的现值等于旧预期值A时，处理器才会将新值B去更新内置位置V的值。否则它就不执行更新，但无论是否更新V的值，都会返回V的旧值。</strong></p>
<p>我们通俗的放到代码层次上去理解i = 2; i++，就是说：</p>
<ul>
<li>首先线程1从内存位置V中读取到了值，保存并作为旧预期值A. (v = 2 ,a = 2)</li>
<li>然后在因为i要进行++操作，系统会比较内存位置V的现值跟旧预期值A进行比较，既V =? A。</li>
<li>如果相等，B = i++ = 3 ，新值B就会对内存位置V进行更新，所以内存位置V的值就变成了B的值，3</li>
<li>如果不相等，则说明有其他的线程修改过了内存位置V的值，比如线程2在线程1修改i的值前就更新了i的值。，所以线程1会更新变量i失败。但线程不会挂起，而是返回失败状态，等待调用线程决定是否重试或其他操作。(通常会重试直到成功)</li>
</ul>
<h3 id="【实现】-2">【实现】</h3>
<p><strong>通常乐观锁的实现有两种，但它们的内在都是CAS思想的设计：</strong></p>
<ul>
<li><strong>方式一：</strong> 使用数据版本（<code>version</code>）实现</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql">这是乐观锁最常用的一种实现方式。什么是数据版本呢？就是在表中增添一个字段作为该记录的版本标识，比如叫version，每次对该记录的写操作都会让 version<span class="hljs-operator">+</span> <span class="hljs-number">1</span>。<br><br>所以当我们读取了数据(包括version)，做出更新，要提交的时候，就会拿取得的version去跟数据库中的version比较是否一致，如果一致则代表这个时间段，并没有其他的线程的也修改过这个数据，给予更新，同时version <span class="hljs-operator">+</span> <span class="hljs-number">1</span>；如果不一致，则代表在这个时间段，该记录以及被其他线程修改过了， 认为是过期数据，返回冲突信息，让用户决定下一步动作，比如重试（重新读取最新数据，再过更新）<br>update <span class="hljs-keyword">table</span> <span class="hljs-keyword">set</span> num <span class="hljs-operator">=</span> num <span class="hljs-operator">+</span> <span class="hljs-number">1</span> , version <span class="hljs-operator">=</span> version <span class="hljs-operator">+</span> <span class="hljs-number">1</span> <span class="hljs-keyword">where</span> version <span class="hljs-operator">=</span> #&#123;version&#125; <span class="hljs-keyword">and</span> id <span class="hljs-operator">=</span> #&#123;id&#125;<br><br></code></pre></td></tr></table></figure>
<ul>
<li><strong>方式二：</strong> 使用时间戳(<code>timestamp</code>)实现</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql">表中增加一个字段，名称无所谓，比如叫update_time, 字段类型使用时间戳（<span class="hljs-type">timestamp</span>）<br><br>原理和方式一一致，也是在更新提交的时检查当前数据库中数据的时间戳和自己更新前取到的时间戳是否一致，如果一致则代表此刻没有冲突，可以提交更新，同时时间戳更新为当前时间，否则就是该时间段有其他线程也更新提交过，返回冲突信息，等待用户下一步动作。<br>update <span class="hljs-keyword">table</span> <span class="hljs-keyword">set</span> num <span class="hljs-operator">=</span> num <span class="hljs-operator">+</span> <span class="hljs-number">1</span> ,update_time <span class="hljs-operator">=</span> unix_timestamp(now()) <span class="hljs-keyword">where</span> id <span class="hljs-operator">=</span> #&#123;id&#125; <span class="hljs-keyword">and</span> update_time <span class="hljs-operator">=</span> #&#123;updateTime&#125;<br><br></code></pre></td></tr></table></figure>
<p>但是我们要注意的是，要实现乐观锁的思想的同时，我们必须要要保证CAS多个操作的原子性，即<strong>获取数据库数据的版本</strong>，<strong>拿数据库的数据版本与之前拿到的版本的比较</strong>，以及<strong>更新数据</strong>等这几个操作的执行必须是连贯执行，具有复合操作的原子性；所以如果是数据库的SQL,那么我们就要保证<strong>多个SQL操作处于同一个事务中</strong>。</p>
<h3 id="【优缺点】-2">【优缺点】</h3>
<p>**优点：**在读多写少的并发场景下，可以避免数据库加锁的开销，提高Dao层的响应性能。其实很多情况下，我们orm工具都有带有乐观锁的实现，所以这些方法不一定需要我们人为的去实现</p>
<p>**缺点：**在写多读少的并发场景下，即在写操作竞争激烈的情况下，会导致CAS多次重试，冲突频率过高，导致开销比悲观锁更高。</p>
<h1>Mysql锁的底层算法</h1>
<h2 id="1、记录锁">1、记录锁</h2>
<p>​	记录锁是锁住记录，锁住索引记录，而不是真正的数据记录</p>
<ul>
<li>锁是非主键索引，会在索引记录上加锁后，在去主键索引上加锁</li>
<li>表上没有索引，会在隐藏的主键索引上加锁</li>
<li>如果要锁的列没有索引，进行全表记录加锁</li>
</ul>
<h2 id="2、间隙锁">2、间隙锁</h2>
<p>当我们用<strong>范围条件</strong>而不是相等条件检索数据，并请求共享或排他锁时，InnoDB（可重复读、串行化级别下才有效）会给符合条件的已有数据的索引项加锁；对于<strong>键值在条件范围内但并不存在的记录</strong>，叫做“间隙(GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁（Next-Key锁）。</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162243301.png" class="" title="在这里插入图片描述">
<p>当执行<code>select * from t where d=5 for update</code>的时候，就不止是给数据库中已有的6个记录加上了行锁，还同时加了7个间隙锁。这样就确保了无法再插入新的记录。</p>
<p><strong>跟间隙锁存在冲突关系的是往这个间隙中插入一个记录这个操作。间隙锁之间不存在冲突关系</strong></p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162258745.png" class="" title="在这里插入图片描述">
<p>这里sessionB并不会被堵住。因为表t里面并没有c=7会这个记录，因此sessionA加的是间隙锁(5,10)。而sessionB也是在这个间隙加的间隙锁。它们用共同的目标，保护这个间隙，不允许插入值。但它们之间是不冲突的。</p>
<p>间隙锁和行锁合称next-key lock，每个next-key lock是前开后闭区间。表t初始化以后，如果用select * from t for update要把整个表所有记录锁起来，就形成了7个next-key lock，分别是(-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +supremum]。因为+∞是开区间，在实现上，InnoDB给每个索引加了一个不存在的最大值supremum，这样才符合都是前开后闭区间。</p>
<p>间隙锁和next-key lock的引入，解决了幻读的问题，但同时也带来了一些困扰。</p>
<p><strong>间隙锁导致的死锁：</strong></p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162307738.png" class="" title="在这里插入图片描述">
<p>1.sessionA执行select … for update语句，由于id=9这一行并不存在，因此会加上间隙锁(5,10)</p>
<p>2.sessionB执行select … for update语句，同样会加上间隙锁(5,10)，间隙锁之间不会冲突</p>
<p>3.sessionB试图插入一行(9,9,9)，被sessionA的间隙锁挡住了，只好进入等待</p>
<p>4.sessionA试图插入一行(9,9,9)，被sessionB的间隙锁挡住了</p>
<p>两个session进入互相等待状态，形成了死锁，间隙锁的引入可能会导致同样的语句锁住更大的范围，这其实是影响并发度的。在读提交隔离级别下，不存在间隙锁。</p>
<h2 id="3、the-next-key-lock">3、the next-key lock</h2>
<p>该锁是记录锁加gap锁，在RR隔离级别下，对行的扫描、锁定都是使用这种锁。如果查询中包含唯一索引，就会只适用记录锁。因为唯一索引能确定记录行数，其他索引不能确定行数，有可能在其他事务中添加这个索引的数据导致幻读。</p>
<h3 id="1、next-key-lock加锁规则">1、next-key lock加锁规则</h3>
<ul>
<li>原则1：加锁的基本单位是next-key lock，next-key lock是<strong>前开后闭区间</strong></li>
<li>原则2：查找过程中<strong>访问到的对象</strong>才会加锁</li>
<li>优化1：索引上的<strong>等值查询，给唯一索引加锁的时候，next-key lock退化为行锁</strong></li>
<li>优化2：索引上的<strong>等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁</strong></li>
<li>一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止</li>
</ul>
<hr>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">这个规则只限于<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">MySQL5</span>.</span></span>x系列&lt;=<span class="hljs-number">5.7</span>.<span class="hljs-number">24</span>，<span class="hljs-number">8.0</span>系列&lt;=<span class="hljs-number">8.0</span>.<span class="hljs-number">13</span><br></code></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> `t` (<br>  `id` <span class="hljs-type">int</span>(<span class="hljs-number">11</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>,<br>  `c` <span class="hljs-type">int</span>(<span class="hljs-number">11</span>) <span class="hljs-keyword">DEFAULT</span> <span class="hljs-keyword">NULL</span>,<br>  `d` <span class="hljs-type">int</span>(<span class="hljs-number">11</span>) <span class="hljs-keyword">DEFAULT</span> <span class="hljs-keyword">NULL</span>,<br>  <span class="hljs-keyword">PRIMARY</span> KEY (`id`),<br>  KEY `c` (`c`)<br>) ENGINE<span class="hljs-operator">=</span>InnoDB;<br><br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> t <span class="hljs-keyword">values</span>(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>,<span class="hljs-number">5</span>),<br>(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>,<span class="hljs-number">10</span>),(<span class="hljs-number">15</span>,<span class="hljs-number">15</span>,<span class="hljs-number">15</span>),(<span class="hljs-number">20</span>,<span class="hljs-number">20</span>,<span class="hljs-number">20</span>),(<span class="hljs-number">25</span>,<span class="hljs-number">25</span>,<span class="hljs-number">25</span>);<br><br></code></pre></td></tr></table></figure>
<h3 id="2、案例一：等值查询间隙锁">2、案例一：等值查询间隙锁</h3>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162316586.png" class="" title="在这里插入图片描述">
<p>1.由于表t中没有id=7的记录，根据原则1，加锁单位是next-key lock，sessionA加锁范围就是(5,10]</p>
<p>2.根据优化2，这是一个等值查询(id=7)，而id=10不满足查询条件，next-key lock退化成间隙锁，因此最终加锁的范围是(5,10)</p>
<p>所以，sessionB要往这个间隙里面插入id=8的记录会被锁住，但是sessionC修改id=10这行是可以的</p>
<hr>
<h3 id="3、案例二：非唯一索引等值锁">3、案例二：非唯一索引等值锁</h3>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162326725.png" class="" title="在这里插入图片描述">
<p>1.根据原则1，加锁单位是next-key lock，因此会给(0,5]加上next-key lock</p>
<p>2.c是普通索引，因此访问c=5这一条记录是不能马上停下来的，需要向右遍历，查到c=10才放弃。根据原则2，访问到的都要加锁，因此要给(5,10]加next-key lock</p>
<p>3.根据优化2，等值判断，向右遍历，最后一个值不满足c=5这个等值条件，因此退化成间隙锁(5,10)</p>
<p>4.根据原则2，只有访问到的对象才会加锁，这个查询使用覆盖索引，并不需要访问主键索引，所以主键索引上没有任何锁，这就是为什么sessionB的update语句可以执行完成</p>
<p>锁是加在索引上的，在这个例子中，lock in share mode只锁覆盖索引，但是如果是for update，系统会认为你接下来要更新数据，因此会顺便给主键索引上满足条件的行加上行锁，这样的话sessionB的update语句会被阻塞住。如果你要用 lock in share mode 来给行加读锁避免数据被更新的话，就必须得绕过覆盖索引的优化，在查询字段中加入索引中不存在的字段</p>
<h3 id="4、案例三：主键索引范围锁">4、案例三：主键索引范围锁</h3>
<p>1.开始执行的时候，要找到第一个id=10的行，因此本该是next-key lock(5,10]。根据优化1，主键id上的等值条件，退化成行锁，只加了id=10这一行的行锁</p>
<p>2.范围查询就往后继续找，找到id=15这一行停下来，因此需要加next-key lock(10,15]</p>
<p>所以，sessionA这时候锁的范围就是主键索引上，行锁id=10和next-key lock(10,15]</p>
<h3 id="5、案例四：非唯一索引范围锁">5、案例四：非唯一索引范围锁</h3>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162335984.png" class="" title="在这里插入图片描述">
<p>这次sessionA用字段c来判断，加锁规则跟案例三唯一的不同是：在第一次用c=10定位记录的时候，索引c上加上(5,10]这个next-key lock后，由于索引c是非唯一索引，没有优化规则，因此最终sessionA加的锁是索引c上的(5,10]和(10,15]这两个next-key lock</p>
<h3 id="6、案例五：唯一索引范围锁bug">6、案例五：唯一索引范围锁bug</h3>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162344192.png" class="" title="在这里插入图片描述">
<p>sessionA是一个范围查询，按照原则1的话，应该是索引id上只加(10,15]这个next-key lock，并且因为id是唯一键，所以循环判断到id=15这一行就应该停止了</p>
<p>但是实现上，InnoDB会扫描到第一个不满足条件的行为止，也就是id=20。而且由于这是个范围扫描，因此索引id上的(15,20]这个next-key lock也会被锁上</p>
<p>所以，sessionB要更新id=20这一行是会被锁住的。同样地，sessionC要插入id=16的一行，也会被锁住</p>
<h3 id="7、案例六：非唯一索引上存在等值的例子">7、案例六：非唯一索引上存在等值的例子</h3>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> t <span class="hljs-keyword">values</span>(<span class="hljs-number">30</span>,<span class="hljs-number">10</span>,<span class="hljs-number">30</span>);<br></code></pre></td></tr></table></figure>
<p>​		新插入的这一行c=10，现在表里有两个c=10的行。虽然有两个c=10，但是它们的主键值id是不同的，因此这两个c=10的记录之间也是有间隙的.</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162401642.png" class="" title="在这里插入图片描述">
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162410516.png" class="" title="在这里插入图片描述">
<p>sessionA在遍历的时候，先访问第一个c=10的记录。根据原则1，这里加的是(c=5,id=5)到(c=10,id=10)这个next-key lock。然后sessionA向右查找，直到碰到(c=15,id=15)这一行，循环才结束。根据优化2，这是一个等值查询，向右查找到了不满足条件的行，所以会退化成(c=10,id=10)到(c=15,id=15)的间隙锁</p>
<p>也就是说，这个delete语句在索引c上的加锁范围，就是下图中蓝色区域覆盖的部分，这个蓝色区域左右两边都是虚线，表示开区间.</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162418283.png" class="" title="在这里插入图片描述">
<h3 id="8、案例七：limit语句加锁">8、案例七：limit语句加锁</h3>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/2019060516242975.png" class="" title="在这里插入图片描述">
<p>加了limit 2的限制，因此在遍历到(c=10,id=30)这一行之后，满足条件的语句已经有两条，循环就结束了。因此，索引c上的加锁范围就变成了从(c=5,id=5)到(c=10,id=30)这个前开后闭区间，如下图所示：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162438271.png" class="" title="在这里插入图片描述">
<p>再删除数据的时候尽量加limit，这样不仅可以控制删除数据的条数，让操作更安全，还可以减小加锁的范围</p>
<h3 id="9、案例八：一个死锁的例子">9、案例八：一个死锁的例子</h3>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162446585.png" class="" title="在这里插入图片描述">
<p>1.sessionA启动事务后执行查询语句加lock in share mode，在索引c上加了next-key lock(5,10]和间隙锁(10,15)</p>
<p>2.sessionB的update语句也要在索引c上加next-key lock(5,10]，进入锁等待</p>
<p>3.然后sessionA要再插入(8,8,8)这一行，被sessionB的间隙锁锁住。由于出现了死锁，InnoDB让sessionB回滚</p>
<p>sessionB的加next-key lock(5,10]操作，实际上分成了两步，先是加(5,10)间隙锁，加锁成功；然后加c=10的行锁，这时候才被锁住的.</p>
<h3 id="10、不等号条件里的等值查询">10、不等号条件里的等值查询</h3>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">begin</span>;<br><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> t <span class="hljs-keyword">where</span> id<span class="hljs-operator">&gt;</span><span class="hljs-number">9</span> <span class="hljs-keyword">and</span> id<span class="hljs-operator">&lt;</span><span class="hljs-number">12</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> id <span class="hljs-keyword">desc</span> <span class="hljs-keyword">for</span> update;<br><br></code></pre></td></tr></table></figure>
<p>利用上面的加锁规则，这个语句的加锁范围是主键索引上的(0,5]、(5,10]和(10,15)。加锁单位是next-key lock，这里用到了优化2，即索引上的等值查询，向右遍历的时候id=15不满足条件，所以next-key lock退化为了间隙锁(10,15)</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162454802.png" class="" title="在这里插入图片描述">
<p>1.首先这个查询语句的语义是order by id desc，要拿到满足条件的所有行，优化器必须先找到第一个id&lt;12的值</p>
<p>2.这个过程是通过索引树的搜索过程得到的，在引擎内部，其实是要找到id=12的这个值，只是最终没找到，但找到了(10,15)这个间隙</p>
<p>3.然后根据order by id desc，再向左遍历，在遍历过程中，就不是等值查询了，会扫描到id=5这一行，所以会加一个next-key lock (0,5]</p>
<p>在执行过程中，通过树搜索的方式定位记录的时候，用的是等值查询的方法</p>
<h3 id="11、等值查询的过程">11、等值查询的过程</h3>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">begin</span>;<br><span class="hljs-keyword">select</span> id <span class="hljs-keyword">from</span> t <span class="hljs-keyword">where</span> c <span class="hljs-keyword">in</span>(<span class="hljs-number">5</span>,<span class="hljs-number">20</span>,<span class="hljs-number">10</span>) lock <span class="hljs-keyword">in</span> share mode;<br><br></code></pre></td></tr></table></figure>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/2019060516250347.png" class="" title="在这里插入图片描述">
<p>这条in语句使用了索引c并且rows=3，说明这三个值都是通过B+树搜索定位的</p>
<p><strong>在查找c=5的时候，先锁住了(0,5]。但是因为c不是唯一索引，为了确认还有没有别的记录c=5，就要向右遍历，找到c=10确认没有了，这个过程满足优化2，所以加了间隙锁(5,10)。执行c=10会这个逻辑的时候，加锁的范围是(5,10]和(10,15)，执行c=20这个逻辑的时候，加锁的范围是(15,20]和(20,25)</strong></p>
<p>这条语句在索引c上加的三个记录锁的顺序是：先加c=5的记录锁，再加c=10的记录锁，最后加c=20的记录锁</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> id <span class="hljs-keyword">from</span> t <span class="hljs-keyword">where</span> c <span class="hljs-keyword">in</span>(<span class="hljs-number">5</span>,<span class="hljs-number">20</span>,<span class="hljs-number">10</span>) <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> c <span class="hljs-keyword">desc</span> <span class="hljs-keyword">for</span> update;<br><br></code></pre></td></tr></table></figure>
<p>由于语句里面是order by c desc，这三个记录锁的加锁顺序是先锁c=20，然后c=10，最后是c=5。这两条语句要加锁相同的资源，但是加锁顺序相反。当这两条语句并发执行的时候，就可能出现死锁。</p>
<h3 id="12、insert语句的锁为什么这么多？">12、insert语句的锁为什么这么多？</h3>
<h4 id="1、insert-…-select语句">1、insert … select语句</h4>
<p>表t和t2的表结构、初始化数据语句如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> `t` (<br>  `id` <span class="hljs-type">int</span>(<span class="hljs-number">11</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span> AUTO_INCREMENT,<br>  `c` <span class="hljs-type">int</span>(<span class="hljs-number">11</span>) <span class="hljs-keyword">DEFAULT</span> <span class="hljs-keyword">NULL</span>,<br>  `d` <span class="hljs-type">int</span>(<span class="hljs-number">11</span>) <span class="hljs-keyword">DEFAULT</span> <span class="hljs-keyword">NULL</span>,<br>  <span class="hljs-keyword">PRIMARY</span> KEY (`id`),<br>  <span class="hljs-keyword">UNIQUE</span> KEY `c` (`c`)<br>) ENGINE<span class="hljs-operator">=</span>InnoDB;<br><br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> t <span class="hljs-keyword">values</span>(<span class="hljs-keyword">null</span>, <span class="hljs-number">1</span>,<span class="hljs-number">1</span>);<br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> t <span class="hljs-keyword">values</span>(<span class="hljs-keyword">null</span>, <span class="hljs-number">2</span>,<span class="hljs-number">2</span>);<br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> t <span class="hljs-keyword">values</span>(<span class="hljs-keyword">null</span>, <span class="hljs-number">3</span>,<span class="hljs-number">3</span>);<br><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> t <span class="hljs-keyword">values</span>(<span class="hljs-keyword">null</span>, <span class="hljs-number">4</span>,<span class="hljs-number">4</span>);<br><br><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> t2 <span class="hljs-keyword">like</span> t;<br><br></code></pre></td></tr></table></figure>
<p><strong>在可重复读隔离级别下，binlog_format=statement时执行下面这个语句时，需要对表t的所有行和间隙加锁</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> t2(c,d) <span class="hljs-keyword">select</span> c,d <span class="hljs-keyword">from</span> t;<br><br></code></pre></td></tr></table></figure>
<h4 id="2、insert循环写入">2、insert循环写入</h4>
<p>要往表t2中插入一行数据，这一行的c值是表t中c值的最大值加1，SQL语句如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> t2(c,d)  (<span class="hljs-keyword">select</span> c<span class="hljs-operator">+</span><span class="hljs-number">1</span>, d <span class="hljs-keyword">from</span> t force index(c) <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> c <span class="hljs-keyword">desc</span> limit <span class="hljs-number">1</span>);<br><br></code></pre></td></tr></table></figure>
<p>这个语句的加锁范围，就是表t索引c上的(3,4]和(4,supermum]这两个next-key lock，以及主键索引上id=4这一行</p>
<p>执行流程是从表t中按照索引c倒序吗，扫描第一行，拿到结果写入到表t2中，因此整条语句的扫描行数是1</p>
<p>但如果要把这一行的数据插入到表t中的话:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> t(c,d)  (<span class="hljs-keyword">select</span> c<span class="hljs-operator">+</span><span class="hljs-number">1</span>, d <span class="hljs-keyword">from</span> t force index(c) <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> c <span class="hljs-keyword">desc</span> limit <span class="hljs-number">1</span>);<br><br></code></pre></td></tr></table></figure>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162511513.png" class="" title="在这里插入图片描述">
<p>explain结果中的Extra字段中Using temporary字段，表示这个语句用到了临时表</p>
<p>执行流程如下：</p>
<p>1.创建临时表，表里有两个字段c和d</p>
<p>2.按照索引c扫描表t，依次取c=4、3、2、1，然后回表，读到c和d的值写入临时表</p>
<p>3.由于语义里面有limit 1，所以只取了临时表的第一行，再插入到表t中</p>
<p>这个语句会导致在表t上做全表扫描，并且会给索引c上的所有间隙都加上共享的next-key lock。所以，这个语句执行期间，其他事务不能在这个表上插入数据</p>
<p>需要临时表是因为这类一边遍历数据，一边更新数据的情况，如果读出来的数据直接写回原表，就可能在遍历过程中，读到刚刚插入的记录，新插入的记录如果参与计算逻辑，就跟语义不符.</p>
<h4 id="3、insert唯一键冲突">3、insert唯一键冲突</h4>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162520967.png" class="" title="在这里插入图片描述">
<p>sessionA执行的insert语句，发生唯一键冲突的时候，并不只是简单地报错返回，还在冲突的索引上加了锁，sessionA持有索引c上的(5,10]共享next-key lock（读锁）</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162531598.png" class="" title="在这里插入图片描述">
<p>在sessionA执行rollback语句回滚的时候，sessionC几乎同时发现死锁并返回</p>
<p>1.在T1时刻，启动sessionA，并执行insert语句，此时在索引c的c=5上加了记录锁。这个索引是唯一索引，因此退化为记录锁</p>
<p>2.在T2时刻，sessionA回滚。这时候，sessionB和sessionC都试图继续执行插入操作，都要加上写锁。两个session都要等待对方的行锁，所以就出现了死锁</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/20190605162540324.png" class="" title="在这里插入图片描述">
<h4 id="4、insert-into-…-on-duplicate-key-update">4、insert into … on duplicate key update</h4>
<p>上面这个例子是主键冲突后直接报错，如果改写成</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> t <span class="hljs-keyword">values</span>(<span class="hljs-number">11</span>,<span class="hljs-number">10</span>,<span class="hljs-number">10</span>) <span class="hljs-keyword">on</span> duplicate key update d<span class="hljs-operator">=</span><span class="hljs-number">100</span>; <br><br></code></pre></td></tr></table></figure>
<p>就会给索引c上(5,10]加一个排他的next-key lock（写锁）</p>
<p>insert into … on duplicate key update的语义逻辑是，插入一行数据，如果碰到唯一键约束，就继续执行后面的更新语句。如果有多个列违反了唯一性索引，就会按照索引的顺序，修改跟第一个索引冲突的行</p>
<p>表t里面已经有了(1,1,1)和(2,2,2)这两行，执行这个语句效果如下：</p>
<img src="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E9%94%81/2019060516255097.png" class="" title="在这里插入图片描述">
<p>主键id是先判断的，MySQL认为这个语句跟id=2这一行冲突，所以修改的是id=2的行</p>
<p><strong>思考题：</strong><br>
1、如果要删除一个表里面的前10000行数据，有以下三种方法可以做到：</p>
<ul>
<li>第一种，直接执行delete from T limit 10000;</li>
<li>第二种，在一个连接中循环执行20次delete from T limit 500；</li>
<li>第三种，在20个连接中同时执行delete from T limit 500；<br>
选择哪一种方式比较好？</li>
</ul>
<p><strong>参考答案：</strong></p>
<ul>
<li>
<p>第一种方式，单个语句占用时间长，锁的时间也比较长，而且大事务还会导致主从延迟</p>
</li>
<li>
<p>第三种方式，会人为造成锁冲突</p>
</li>
<li>
<p>第二种方式相对较好</p>
</li>
</ul>
<hr>
<h1>MyISAM和MySQL的锁的对比总结</h1>
<p><strong>对于ＭyISAM的表锁，主要有以下几点</strong></p>
<p>（１）共享读锁（S）之间是兼容的，但共享读锁（S）和排他写锁（X）之间，以及排他写锁之间（X）是互斥的，也就是说读和写是串行的。</p>
<p>（２）在一定条件下，ＭyISAM允许查询和插入并发执行，我们可以利用这一点来解决应用中对同一表和插入的锁争用问题。</p>
<p>（３）ＭyISAM默认的锁调度机制是写优先，这并不一定适合所有应用，用户可以通过设置LOW_PRIPORITY_UPDATES参数，或在INSERT、UPDATE、DELETE语句中指定LOW_PRIORITY选项来调节读写锁的争用。</p>
<p>（４）由于表锁的锁定粒度大，读写之间又是串行的，因此，如果更新操作较多，ＭyISAM表可能会出现严重的锁等待，可以考虑采用InnoDB表来减少锁冲突。</p>
<p><strong>对于InnoDB表，主要有以下几点</strong></p>
<p>（１）InnoDB的行销是基于索引实现的，如果不通过索引访问数据，InnoDB会使用表锁。</p>
<p>（２）InnoDB间隙锁机制，以及InnoDB使用间隙锁的原因。</p>
<p>（３）在不同的隔离级别下，InnoDB的锁机制和一致性读策略不同。</p>
<p>（４）ＭySQL的恢复和复制对InnoDB锁机制和一致性读策略也有较大影响。</p>
<p>（５）锁冲突甚至死锁很难完全避免。</p>
<p><strong>在了解InnoDB的锁特性后，用户可以通过设计和SQL调整等措施减少锁冲突和死锁，包括：</strong></p>
<ul>
<li>尽量使用较低的隔离级别</li>
<li>精心设计索引，并尽量使用索引访问数据，使加锁更精确，从而减少锁冲突的机会。</li>
<li>选择合理的事务大小，小事务发生锁冲突的几率也更小。</li>
<li>给记录集显示加锁时，最好一次性请求足够级别的锁。比如要修改数据的话，最好直接申请排他锁，而不是先申请共享锁，修改时再请求排他锁，这样容易产生死锁。</li>
<li>不同的程序访问一组表时，应尽量约定以相同的顺序访问各表，对一个表而言，尽可能以固定的顺序存取表中的行。这样可以大减少死锁的机会。</li>
<li>尽量用相等条件访问数据，这样可以避免间隙锁对并发插入的影响。</li>
<li>不要申请超过实际需要的锁级别；除非必须，查询时不要显示加锁。</li>
<li>对于一些特定的事务，可以使用表锁来提高处理速度或减少死锁的可能。</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/dajiangdahe.github.io/tags/Mysql/" rel="tag">Mysql</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Mysql中的日志"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E6%97%A5%E5%BF%97/"
    >Mysql中的日志</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/dajiangdahe.github.io/2021/10/22/Mysql%E4%B8%AD%E7%9A%84%E6%97%A5%E5%BF%97/" class="article-date">
  <time datetime="2021-10-22T07:21:44.913Z" itemprop="datePublished">2021-10-22</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/dajiangdahe.github.io/categories/Mysql/">Mysql</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1>Mysql中的日志</h1>
<ul>
<li>错误日志</li>
<li>查询日志</li>
<li>redo log（重做日志）</li>
<li>undo log（回滚日志）</li>
<li>bin log（二进制日志）</li>
</ul>
<h2 id="1-错误日志">1.错误日志</h2>
<p>用来记录 MySQL 服务器运行过程中的错误信息,默认开启无法关闭.<br>
复制环境下，从服务器进程的信息也会被记录进错误日志</p>
<p>默认情况下，错误日志是存储在数据库的数据文件目录中，名称为 hostname.err，其中 hostname 为服务器主机名。在 MySQL 5.5.7 之前，数据库管理员可以删除很长时间之前的错误日志，以节省服务器上的硬盘空间， MySQL 5.5.7 之后，服务器将关闭此项功能，只能使用重命名原来的错误日志文件，手动冲洗日志创建一个新的，命令为：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql">mv hostname.err hostname.err.old mysqladmin flush<span class="hljs-operator">-</span>logs<br></code></pre></td></tr></table></figure>
<h2 id="2-查询日志">2.查询日志</h2>
<p><strong>查询日志里面记录了数据库执行的所有命令，不管语句是否正确，都会被记录</strong>，具体原因如下:</p>
<ul>
<li>insert 查询为了避免数据冲突，如果此前插入过数据，当前插入的数据如果跟主键或唯一键的数据重复那肯定会报错；</li>
<li>update 时也会查询因为更新的时候很可能会更新某一块数据；</li>
<li>delete 查询，只删除符合条件的数据；</li>
</ul>
<p>因此都会产生日志，在并发操作非常多的场景下，查询信息会非常多，那么如果都记录下来会导致 IO 非常大，影响 MySQL 性能，因此如果不是在调试环境下，是不建议开启查询日志功能的。</p>
<p>查询日志模式是关闭的，可以通过以下命令开启查询日志：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">set</span> <span class="hljs-keyword">global</span> generallog<span class="hljs-operator">=</span><span class="hljs-number">1</span> <span class="hljs-keyword">set</span> <span class="hljs-keyword">global</span> logoutput<span class="hljs-operator">=</span>‘<span class="hljs-keyword">table</span>’;<br></code></pre></td></tr></table></figure>
<h2 id="3-redo-log-重做日志-一种缓存机制">3.redo log(重做日志) 一种缓存机制</h2>
<p><strong>是什么</strong>:为了最大程度的避免数据写入时，因为 IO 瓶颈造成的性能问题</p>
<p>怎么办:<strong>先将数据</strong>写入内存中，再<strong>批量把内存中的数据</strong>统一刷回磁盘。为了避免将数据刷回磁盘过程中，因为掉电或系统故障带来的数据丢失问题，InnoDB 采用 redo log 来解决此问题。</p>
<h2 id="4-undo-log（回滚日志）">4.undo log（回滚日志）</h2>
<p><strong>是什么</strong>：用于存储日志被修改前的值，从而保证如果修改出现异常，可以使用 undo log 日志来实现回滚操作。</p>
<p><strong>怎么做</strong>:<strong>undo log</strong> 和 redo log 记录物理日志不一样，它是逻辑日志，可以认为当 delete 一条记录时，undo log 中会记录一条对应的 insert 记录，反之亦然，当 update 一条记录时，它记录一条对应相反的 update 记录，当执行 rollback 时，就可以从 undo log 中的逻辑记录读取到相应的内容并进行回滚。<strong>undo log 默认存放在共享表空间中</strong>，在 ySQL 5.6 中，undo log 的存放位置还可以通过变量 innodbundodirectory 来自定义存放目录，默认值为“.”表示 datadir 目录。</p>
<h2 id="5-bin-log-二进制日志">5.bin log(二进制日志)</h2>
<p>是一个二进制文件，主要记录所有数据库表结构变更bin log 中记录了对 MySQL 数据库<strong>执行更改的所有操作</strong>，并且记录了语句发生时间、执行时长、操作数据等其它额外信息，但是<strong>它不记录 SELECT、SHOW 等那些不修改数据的 SQL 语句</strong>。</p>
<p>【<strong>binlog 的作用如下</strong>】：</p>
<ul>
<li><strong>恢复（recovery）</strong>：某些数据的恢复需要二进制日志。比如，在一个数据库全备文件恢复后，用户可以通过二进制日志进行 point-in-time 的恢复；</li>
<li><strong>复制（replication）</strong>：其原理与恢复类似，通过复制和执行二进制日志使一台远程的MySQL数据库（一般称为 slave 或者 standby）与一台 MySQL 数据库（一般称为 master 或者 primary）进行实时同步；</li>
<li><strong>审计（audit）</strong>：用户可以通过二进制日志中的信息来进行审计，判断是否有对数据库进行注入攻击。</li>
<li>binlog 对于事务存储引擎的崩溃恢复也有非常重要的作用</li>
</ul>
<p>开启方法:</p>
<p>binlog 默认是关闭状态，可以在 MySQL 配置文件（my.cnf）中通过配置参数 log-bin = [base-name] 开启记录 binlog 日志，如果不指定 base-name，则默认二进制日志文件名为主机名，并以自增的数字作为后缀，比如：mysql-bin.000001，所在目录为数据库所在目录（datadir）。</p>
<p>通过以下命令来查询 binlog 是否开启：</p>
<blockquote>
<p>show variables like ‘log_%’;</p>
</blockquote>
<p><strong>binlog 格式分为: STATEMENT、ROW 和 MIXED 三种：</strong></p>
<p><strong>STATEMENT格式</strong>：的 binlog 记录的是数据库上执行的原生 SQL 语句。这种格式的优点是简单，简单地记录和执行这些语句，能够让主备保持同步，在主服务器上执行的 SQL 语句，在从服务器上执行同样的语句。另一个好处是二进制日志里的时间更加紧凑，所以相对而言，基于语句的复制模式不会使用太多带宽，同时也节约磁盘空间。</p>
<p><strong>ROW 格式</strong>：是从 MySQL 5.1 开始支持基于行的复制，也就是基于数据的复制，基于行的更改。这种方式会将实际数据记录在二进制日志中，它有其自身的一些优点和缺点，最大的好处是可以正确地复制每一行数据，一些语句可以被更加有效地复制，另外就是几乎没有基于行的复制模式无法处理的场景，对于所有的 SQL 构造、触发器、存储过程等都能正确执行；它的缺点就是二进制日志可能会很大，而且不直</p>
<p><strong>MIXED 格式</strong>：也是 MySQL 默认使用的二进制日志记录方式，但 MIXED 格式默认采用基于语句的复制，一旦发现基于语句的无法精确的复制时，就会采用基于行的复制。比如用到 UUID()、USER()、CURRENTUSER()、ROWCOUNT() 等无法确定的函数。</p>
<h1>MySQL日志问题扩充</h1>
<h2 id="1-redo-log-和-binlog-有什么区别？">1.redo log 和 binlog 有什么区别？</h2>
<p>redo log（重做日志）和 binlog（归档日志）都是 MySQL 的重要的日志，它们的区别如下：</p>
<ul>
<li>redo log 是物理日志，记录的是“在某个数据页上做了什么修改”。</li>
<li>binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。</li>
<li>redo log 是 InnoDB 引擎特有的；</li>
<li>binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。</li>
<li>redo log 是循环写的，空间固定会用完；</li>
<li>binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。最开始 MySQL 里并没有 InnoDB 引擎，MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统，也就是 redo log 来实现 crash-safe 能力。（两段式提交，分成<strong>prepare + commit</strong> 来保证crash-safe）</li>
</ul>
<h2 id="2-什么是-crash-safe？">2.什么是 crash-safe？</h2>
<p>crash-safe 是指发生宕机等意外情况下，服务器重启后数据依然不会丢失的情况。</p>
<h2 id="3-什么是脏页和干净页？">3.什么是脏页和干净页？</h2>
<p>MySQL 为了操作的性能优化，会把数据更新先放入内存中，之后再统一更新到磁盘。当内存数据和磁盘数据内容不一致的时候，我们称这个内存页为脏页；内存数据写到磁盘后，内存的数据和磁盘上的内容就一致了，我们称为“干净页”。</p>
<h2 id="4-MySQL一页的大小是多少？">4.MySQL一页的大小是多少？</h2>
<p>16KB</p>
<h2 id="5-什么情况下会引发-MySQL-刷脏页（flush）的操作？">5.什么情况下会引发 MySQL 刷脏页（flush）的操作？</h2>
<ul>
<li>内存写满了，这个时候就会引发 flush 操作，对应到 InnoDB 就是 redo log 写满了；</li>
<li>系统的内存不足了，当需要新的内存页的时候，就会淘汰一些内存页，如果淘汰的是脏页这个时候就会触发 flush 操作；</li>
<li>系统空闲的时候，MySQL 会同步内存中的数据到磁盘也会触发 flush 操作；</li>
<li>MySQL 服务关闭的时候也会刷脏页，触发 flush 操作。</li>
</ul>
<h2 id="6-MySQL-刷脏页的速度很慢可能是什么原因？">6.MySQL 刷脏页的速度很慢可能是什么原因？</h2>
<p>在 MySQL 中单独刷一个脏页的速度是很快的，如果发现刷脏页的速度很慢，说明触发了 MySQL 刷脏页的“连坐”机制，MySQL 的“连坐”机制是指当 MySQL 刷脏页的时候如果发现相邻的数据页也是脏页也会一起刷掉，而这个动作可以一直蔓延下去，这就是导致 MySQL 刷脏页慢的原因了。</p>
<h2 id="7-如何控制-MySQL-只刷新当前脏页？">7.如何控制 MySQL 只刷新当前脏页？</h2>
<p>在 InnoDB 中设置 innodbflushneighbors 这个参数的值为 0，来规定 MySQL 只刷当前脏页，MySQL 8 这个值默认是 0。</p>
<h2 id="8-MySQL-的-WAL-技术是解决什么问题的？">8.MySQL 的 WAL 技术是解决什么问题的？</h2>
<p>WAL 技术的全称是 Write Ahead Logging（中文：预写式日志），是先写日志，再写磁盘的方式，因为每次更新都写磁盘的话 IO 成本很高，所以才有了 WAL 技术。</p>
<h2 id="9-为什么有时候会感觉-MySQL-偶尔卡一下？">9.为什么有时候会感觉 MySQL 偶尔卡一下？</h2>
<p>如果偶尔感觉 MySQL 卡一下，可能是 MySQL 正在刷脏页，正在把内存中的更新操作刷到磁盘中。</p>
<h2 id="10-redo-log-和-binlog-是怎么关联的">10.redo log 和 binlog 是怎么关联的?</h2>
<p>它们有一个共同的数据字段，叫 XID。崩溃恢复的时候，会按顺序扫描 redo log：</p>
<ul>
<li>如果碰到既有 prepare、又有 commit 的 redo log，就直接提交；</li>
<li>如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。</li>
</ul>
<h2 id="11-MySQL-怎么知道-binlog-是完整的">11.MySQL 怎么知道 binlog 是完整的?</h2>
<ul>
<li>statement 格式的 binlog，完整的标识是最后有 COMMIT 关键字。</li>
<li>row 格式的 binlog，完整的标识是最后会有一个 XID event 关键字。</li>
</ul>
<h2 id="12-MySQL-中可不可以只要-binlog，不要-redo-log？">12.MySQL 中可不可以只要 binlog，不要 redo log？</h2>
<p>不可以，binlog 没有崩溃恢复的能力。</p>
<h2 id="13-MySQL-中可不可以只要-redo-log，不要-binlog？">13.MySQL 中可不可以只要 redo log，不要 binlog？</h2>
<p>不可以，原因有以下两个：</p>
<ul>
<li>redo log 是循环写不能保证所有的历史数据，这些历史数据只能在 binlog 中找到；</li>
<li>binlog 是高可用的基础，高可用的实现原理就是 binlog 复制。</li>
</ul>
<h2 id="14-为什么-binlog-cache-是每个线程自己维护的，而-redo-log-buffer-是全局共用的？">14.为什么 binlog cache 是每个线程自己维护的，而 redo log buffer 是全局共用的？</h2>
<p>因为 binlog 是不能“被打断的”，一个事务的 binlog 必须连续写，因此要整个事务完成后，再一起写到文件里。而 redo log 并没有这个要求，中间有生成的日志可以写到 redo log buffer 中，redo log buffer 中的内容还能“搭便车”，其他事务提交的时候可以被一起写到磁盘中。</p>
<h2 id="15-事务执行期间，还未提交，如果发生-crash，redo-log-丢失，会导致主备不一致呢？">15.事务执行期间，还未提交，如果发生 crash，redo log 丢失，会导致主备不一致呢？</h2>
<p>不会，因为这时候 binlog 也还在 binlog cache 里，没发给备库，crash 以后 redo log 和 binlog 都没有了，从业务角度看这个事务也没有提交，所以数据是一致的。</p>
<h2 id="16-在-MySQL-中用什么机制来优化随机读-写磁盘对-IO-的消耗？">16.在 MySQL 中用什么机制来优化随机读/写磁盘对 IO 的消耗？</h2>
<p>redo log 是<strong>用来节省随机写磁盘的 IO 消耗</strong>，而 <strong>change buffer 主要是节省随机读磁盘的 IO</strong> 消耗。redo log 会把 MySQL 的更新操作先记录到内存中，之后再统一更新到磁盘，而 change buffer 也是把关键查询数据先加载到内存中，以便优化 MySQL 的查询。</p>
<p>A.redo log 是 InnoDB 引擎特有的，它的固定大小的</p>
<p>B.redo log 日志是不全的，只有最新的一些日志，这和它的内存大小有关</p>
<p>C.redo log 可以保证数据库异常重启之后，数据不丢失</p>
<p>D binlog 是 MySQL 自带的日志，但它并不能保证数据库异常重启之后数据不丢失。</p>
<p>E.binlog 日志是追加写的，后面的日志并不会覆盖前面的日志，redo log 日志是固定大小的，后面的日志会覆盖前面的日志。</p>
<h2 id="17-有没有办法把-MySQL-的数据恢复到过去某个指定的时间节点？怎么恢复？">17.有没有办法把 MySQL 的数据恢复到过去某个指定的时间节点？怎么恢复？</h2>
<p>可以恢复，只要你<strong>备份了这段时间的所有 binlog</strong>，同时做了<strong>全量数据库的定期备份</strong>，比如，一天一备，或者三天一备，这取决于你们的备份策略，这个时候你就可以把之前备份的数据库先还原到测试库，从备份的时间点开始，将备份的 binlog 依次取出来，重放到你要恢复数据的那个时刻，这个时候就完成了数据到指定节点的恢复。比如，今天早上 9 点的时候，你想把数据恢复成今天早上 6:00:00 的状态，这个时候你可以先取出今天凌晨（00:01:59）备份的数据库文件，还原到测试库，再从 binlog 文件中依次取出 00:01:59 之后的操作信息，重放到 6:00:00 这个时刻，这就完成了数据库的还原。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/dajiangdahe.github.io/tags/Mysql/" rel="tag">Mysql</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MysqlMVCC"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/dajiangdahe.github.io/2021/10/22/MysqlMVCC/"
    >Mysql MVCC</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/dajiangdahe.github.io/2021/10/22/MysqlMVCC/" class="article-date">
  <time datetime="2021-10-22T07:21:44.891Z" itemprop="datePublished">2021-10-22</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/dajiangdahe.github.io/categories/Mysql/">Mysql</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1>Mysql MVCC</h1>
<h2 id="【数据库事务】">【数据库事务】</h2>
<h3 id="【概念】">【概念】</h3>
<p>事务是应用程序中一系列严密的操作，所有操作必须成功完成，否则在每个操作中所作的所有更改都会被撤消。也就是事务具有原子性，一个事务中的一系列的操作要么全部成功，要么一个都不做。</p>
<h3 id="【特性】">【特性】</h3>
<ul>
<li><strong>原子性（Atomicity）</strong></li>
</ul>
<p>原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，这和前面两篇博客介绍事务的功能是一样的概念，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。</p>
<ul>
<li><strong>一致性（Consistency）</strong></li>
</ul>
<p>一致性是指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。</p>
<p>拿转账来说，假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。</p>
<ul>
<li><strong>隔离性（Isolation）</strong></li>
</ul>
<p>隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。</p>
<p>即要达到这么一种效果：对于任意两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始之前就已经结束，要么在T1结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。</p>
<p>关于事务的隔离性数据库提供了多种隔离级别，稍后会介绍到。</p>
<ul>
<li><strong>持久性（Durability）</strong></li>
</ul>
<p>持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。</p>
<p>​		例如我们在使用JDBC操作数据库时，在提交事务方法后，提示用户事务操作完成，当我们程序执行完成直到看到提示后，就可以认定事务以及正确提交，即使这时候数据库出现了问题，也必须要将我们的事务完全执行完成，否则就会造成我们看到提示事务处理完毕，但是数据库因为故障而没有执行事务的重大错误。</p>
<h3 id="【并发产生的问题】">【并发产生的问题】</h3>
<ul>
<li><strong>丢失修改</strong></li>
</ul>
<p>当两个或多个事务选择同行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题一一最后的更新覆盖了由其他事务所做的更新。  例如，两个程序员修改同一java文件。每程序员独立地更改其副本，然后保存更改后的副本，这样就覆盖了原始文档。最后保存其更改副本的编辑人员覆盖前一个程序员所做的更改。</p>
<ul>
<li><strong>脏读</strong></li>
</ul>
<p>脏读是指在一个事务处理过程里读取了另一个未提交的事务中的数据。当一个事务正在多次修改某个数据，而在这个事务中这多次的修改都还未提交，这时一个并发的事务来访问该数据，就会造成两个事务得到的数据不一致。</p>
<img src="/dajiangdahe.github.io/2021/10/22/MysqlMVCC/aHR0cHM6Ly9tbWJpei5xcGljLmNuL3N6X21tYml6X3BuZy9LUlJ4dnFHY2ljWkZxZzc4MjI5eWppYlBuOE9iczN2U040Vlo3TEtYanpsM3E1WE9wSWtxUUFLcGFwUDhoY0pLVlhHM21LbWtsMEk0WG9pYVM3eVRrdndWdy82NDA" class="" title="img">
<ul>
<li><strong>不可重复读</strong></li>
</ul>
<p>不可重复读是指在对于数据库中的某个数据，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，被另一个事务修改并提交了。</p>
<img src="/dajiangdahe.github.io/2021/10/22/MysqlMVCC/aHR0cHM6Ly9tbWJpei5xcGljLmNuL3N6X21tYml6X3BuZy9LUlJ4dnFHY2ljWkZxZzc4MjI5eWppYlBuOE9iczN2U040cWdRWWFrM043c29CRFZ3eVdKRHJpYU1ycVB2bE9zSXdURlpnVldxM1lNcUlTbUYxTkREWDVqZy82NDA" class="" title="img">
<ul>
<li><strong>虚读(幻读)</strong></li>
</ul>
<p>一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”。</p>
<img src="/dajiangdahe.github.io/2021/10/22/MysqlMVCC/aHR0cHM6Ly9tbWJpei5xcGljLmNuL3N6X21tYml6X3BuZy9LUlJ4dnFHY2ljWkZxZzc4MjI5eWppYlBuOE9iczN2U040WG1BSWpwRTRIVlpZaE5hTHZLNUNIeGNLWW4yaWJwZW9oVUxFbnE5ZThFSkNpYlZ0aWJZaGFzcW9nLzY0MA" class="" title="img">
<h3 id="【隔离级别】">【隔离级别】</h3>
<p>① Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。</p>
<p>② Repeatable read (可重复读)：可避免脏读、不可重复读的发生。</p>
<p>③ Read committed (读已提交)：可避免脏读的发生。</p>
<p>④ Read uncommitted (读未提交)：最低级别，任何情况都无法保证。</p>
<table>
<thead>
<tr>
<th><strong>隔离级别</strong></th>
<th><strong>读数据一致性</strong></th>
<th><strong>脏读</strong></th>
<th><strong>不可重复读</strong></th>
<th><strong>幻读</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>读未提交（read-uncommitted）</td>
<td>最低级别，只能保证不读取物理上损坏的数据</td>
<td>有</td>
<td>有</td>
<td>有</td>
</tr>
<tr>
<td>读已提交（read-committed）</td>
<td>语句级</td>
<td>否</td>
<td>有</td>
<td>有</td>
</tr>
<tr>
<td>可重复读（repeatable-read）</td>
<td>事务级</td>
<td>否</td>
<td>否</td>
<td>有</td>
</tr>
<tr>
<td>串行化（serializable）</td>
<td>最高级别，事务级</td>
<td>否</td>
<td>否</td>
<td>否</td>
</tr>
</tbody>
</table>
<p>在<strong>MySQL</strong>数据库中，支持上面四种隔离级别，默认的为<strong>Repeatable read (可重复读)</strong>；而在<strong>Oracle</strong>数据库中，只支持Serializable (串行化)级别和Read committed (读已提交)这两种级别，其中默认的为<strong>Read committed</strong>级别。</p>
<h2 id="【MVCC】">【MVCC】</h2>
<h3 id="【概念】-2">【概念】</h3>
<p>多版本控制: 指的是一种提高并发的技术。最早的数据库系统，只有读读之间可以并发，读写，写读，写写都要阻塞。引入多版本之后，<strong>只有写写之间相互阻塞，其他三种操作都可以并行</strong>，这样大幅度提高了InnoDB的并发度。在内部实现中，与Postgres在数据行上实现多版本不同，<strong>InnoDB是在undolog中实现的，通过undolog可以找回数据的历史版本</strong>。找回的数据历史版本可以提供给用户读(按照隔离级别的定义，有些读请求只能看到比较老的数据版本)，也可以在回滚的时候覆盖数据页上的数据。**在InnoDB内部中，会记录一个全局的活跃读写事务数组，其主要用来判断事务的可见性。**MVCC是一种多版本并发控制机制。</p>
<p>MVCC在MySQL InnoDB中的实现主要是为了提高数据库并发性能，用更好的方式去处理读-写冲突，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读</p>
<h3 id="【当前读和快照读】">【当前读和快照读】</h3>
<ul>
<li>当前读<br>
像select lock in share mode(<code>共享锁</code>), select for update ; update, insert ,delete(<code>排他锁</code>)这些操作都是一种当前读，为什么叫当前读？就是它读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。</li>
<li>快照读<br>
像<code>不加锁</code>的select操作就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读；之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于多版本并发控制，即MVCC,可以认为MVCC是行锁的一个变种，但它在很多情况下，避免了加锁操作，降低了开销；既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本</li>
</ul>
<p>MVCC就是为了实现读-写冲突不加锁，而这个<strong>读指的就是<code>快照读</code>, 而非当前读</strong>，当前读实际上是一种加锁的操作，是悲观锁的实现</p>
<h3 id="【当前读和快照读的关系】">【当前读和快照读的关系】</h3>
<ul>
<li>准确的说，MVCC多版本并发控制指的是 “维持一个数据的多个版本，使得读写操作没有冲突” 这么一个概念。仅仅是一个理想概念</li>
<li>而在MySQL中，实现这么一个MVCC理想概念，我们就需要MySQL提供具体的功能去实现它，而快照读就是MySQL为我们实现MVCC理想模型的其中一个具体非阻塞读功能。而相对而言，当前读就是悲观锁的具体功能实现</li>
<li>要说的再细致一些，快照读本身也是一个抽象概念，再深入研究。MVCC模型在MySQL中的具体实现则是由 <code>3个隐式字段</code>，<code>undo日志</code> ，<code>Read View</code> 等去完成的，具体可以看下面的MVCC实现原理</li>
</ul>
<h3 id="【并发场景】">【并发场景】</h3>
<ul>
<li><code>读-读</code>：不存在任何问题，也不需要并发控制</li>
<li><code>读-写</code>：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读，幻读，不可重复读</li>
<li><code>写-写</code>：有线程安全问题，可能会存在更新丢失问题，比如第一类更新丢失，第二类更新丢失</li>
</ul>
<p>备注：第1类丢失更新：事务A撤销时，把已经提交的事务B的更新数据覆盖了；第2类丢失更新：事务A覆盖事务B已经提交的数据，造成事务B所做的操作丢失</p>
<p>【<strong>优点</strong>】</p>
<p>多版本并发控制（MVCC）是一种用来解决<code>读-写冲突</code>的无锁并发控制，也就是为事务分配单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。 所以MVCC可以为数据库解决以下问题</p>
<ul>
<li>
<p>在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能</p>
</li>
<li>
<p>同时还可以解决脏读，幻读，不可重复读等事务隔离问题，但不能解决更新丢失问题</p>
</li>
</ul>
<p>【<strong>方案</strong>】</p>
<ul>
<li><code>MVCC + 悲观锁</code><br>
MVCC解决读写冲突，悲观锁解决写写冲突</li>
<li><code>MVCC + 乐观锁</code><br>
MVCC解决读写冲突，乐观锁解决写写冲突</li>
</ul>
<p>这种组合的方式就可以最大程度的提高数据库并发性能，并解决读写冲突，和写写冲突导致的问题</p>
<h3 id="【实现原理】">【实现原理】</h3>
<p>MVCC的目的就是多版本并发控制，在数据库中的实现，就是为了解决<code>读写冲突</code>，它的实现原理主要是依赖记录中的 <code>3个隐式字段</code>，<code>undo日志</code> ，<code>Read View</code> 来实现的。所以我们先来看看这个三个point的概念</p>
<h3 id="【隐式字段】">【<strong>隐式字段</strong>】</h3>
<p>每行记录除了我们自定义的字段外，还有数据库隐式定义的<code>DB_TRX_ID</code>,<code>DB_ROLL_PTR</code>,<code>DB_ROW_ID</code>等字段</p>
<ul>
<li><code>DB_TRX_ID</code><br>
6byte，最近修改(<code>修改/插入</code>)事务ID：记录创建这条记录/最后一次修改该记录的事务ID</li>
<li><code>DB_ROLL_PTR</code><br>
7byte，回滚指针，指向这条记录的上一个版本（存储于rollback segment里）</li>
<li><code>DB_ROW_ID</code><br>
6byte，隐含的自增ID（隐藏主键），如果数据表没有主键，InnoDB会自动以<code>DB_ROW_ID</code>产生一个聚簇索引</li>
<li>实际还有一个删除flag隐藏字段, 既记录被更新或删除并不代表真的删除，而是删除flag变了</li>
</ul>
<img src="/dajiangdahe.github.io/2021/10/22/MysqlMVCC/20190313213705258.png" class="" title="在这里插入图片描述">
<p>如上图，<code>DB_ROW_ID</code>是数据库默认为该行记录生成的唯一隐式主键，<strong><code>DB_TRX_ID</code>是当前操作该记录的事务ID,而<code>DB_ROLL_PTR</code>是一个回滚指针，用于配合undo日志，指向上一个旧版本</strong></p>
<h3 id="【undo日志】">【<strong>undo日志</strong>】</h3>
<p>undo log主要分为两种：</p>
<ul>
<li>insert undo log<br>
代表事务在<code>insert</code>新记录时产生的<code>undo log</code>, <strong>只在事务回滚时需要，并且在事务提交后可以被立即丢弃</strong></li>
<li>update undo log<br>
事务在进行<code>update</code>或<code>delete</code>时产生的<code>undo log</code>; <strong>不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被<code>purge</code>线程统一清除</strong></li>
</ul>
<p>【<strong>purge</strong>】</p>
<ul>
<li>从前面的分析可以看出，为了实现InnoDB的MVCC机制，<strong>更新或者删除操作都只是设置一下老记录的deleted_bit</strong>，并不真正将过时的记录删除。</li>
<li>为了节省磁盘空间，InnoDB有专门的purge线程来清理deleted_bit为true的记录。为了不影响MVCC的正常工作，purge线程自己也维护了一个read view（这个read view相当于系统中最老活跃事务的read view）;如果某个记录的deleted_bit为true，并且DB_TRX_ID相对于purge线程的read view可见，那么这条记录一定是可以被安全清除的。</li>
</ul>
<h3 id="【示例】">【<strong>示例</strong>】</h3>
<p>对MVCC有帮助的实质是<code>update undo log</code> ，<code>undo log</code>实际上就是存在<code>rollback segment</code>中旧记录链，它的执行流程如下：</p>
<p><strong>一、 比如一个有个事务插入persion表插入了一条新记录，记录如下，<code>name</code>为Jerry, <code>age</code>为24岁，<code>隐式主键</code>是1，<code>事务ID</code>和<code>回滚指针</code>，我们假设为NULL。</strong></p>
<img src="/dajiangdahe.github.io/2021/10/22/MysqlMVCC/20190313213836406.png" class="" title="img">
<p><strong>二、 现在来了一个<code>事务1</code>对该记录的<code>name</code>做出了修改，改为Tom</strong></p>
<ul>
<li>在<code>事务1</code>修改该行(记录)数据时，数据库会先对该行加<code>排他锁</code></li>
<li>然后把该行数据拷贝到<code>undo log</code>中，作为旧记录，<strong>既在<code>undo log</code>中有当前行的拷贝副本</strong></li>
<li>拷贝完毕后，修改该行<code>name</code>为Tom，并且<strong>修改隐藏字段的事务ID为当前<code>事务1</code>的ID</strong>, 我们默认从<code>1</code>开始，之后递增，回滚指针指向拷贝到<code>undo log</code>的副本记录，既表示我的上一个版本就是它</li>
<li>事务提交后，释放锁。</li>
</ul>
<img src="/dajiangdahe.github.io/2021/10/22/MysqlMVCC/20190313220441831.png" class="" title="img">
<p><strong>三、 又来了个<code>事务2</code>修改<code>person表</code>的同一个记录，将<code>age</code>修改为30岁</strong></p>
<ul>
<li>在<code>事务2</code>修改该行数据时，数据库也先为该行加锁</li>
<li>然后把该行数据拷贝到<code>undo log</code>中，作为旧记录，发现该行记录已经有<code>undo log</code>了，那么最新的旧数据作为链表的表头，插在该行记录的<code>undo log</code>最前面</li>
<li>修改该行<code>age</code>为30岁，并且修改隐藏字段的事务ID为当前<code>事务2</code>的ID, 那就是<code>2</code>，回滚指针指向刚刚拷贝到<code>undo log</code>的副本记录</li>
<li>事务提交，释放锁</li>
</ul>
<img src="/dajiangdahe.github.io/2021/10/22/MysqlMVCC/20190313220528630.png" class="" title="img">
<p>从上面，我们就可以看出，不同事务或者相同事务的对同一记录的修改，会导致该记录的<code>undo log</code>成为一条记录版本线性表，既链表，<code>undo log</code>的链首就是最新的旧记录，<strong>链尾就是最早的旧记录</strong>（当然就像之前说的该undo log的节点可能是会purge线程清除掉，向图中的第一条insert undo log，其实在事务提交之后可能就被删除丢失了，不过这里为了演示，所以还放在这里）</p>
<h3 id="Read-View-读视图">Read View(读视图)</h3>
<p>什么是Read View?</p>
<p>什么是Read View，说白了Read View就是事务进行<code>快照读</code>操作的时候生产的<code>读视图</code>(Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的ID(当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以最新的事务，ID值越大)</p>
<p>所以我们知道 <code>Read View</code>主要是用来做可见性判断的, 即当我们某个事务执行快照读的时候，对该记录创建一个<code>Read View</code>读视图，把它比作条件用来判断当前事务能够看到哪个版本的数据，既可能是当前最新的数据，也有可能是该行记录的<code>undo log</code>里面的某个版本的数据。</p>
<figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">Read View`遵循一个可见性算法，主要是将`要被修改的数据`的最新记录中的`DB_TRX_ID`（即当前事务ID）取出来，与系统当前其他活跃事务的ID去对比（由Read View维护），如果`DB_TRX_ID`跟Read View的属性做了某些比较，不符合可见性，那就通过`DB_ROLL_PTR`回滚指针去取出`Undo Log`中的`DB_TRX_ID`再比较，即遍历链表的`DB_TRX_ID`（从链首到链尾，即从最近的一次修改查起），直到	找到满足特定条件的`DB_TRX_ID`, 那么这个DB_TRX_ID所在的旧记录就是当前事务能看见的最新`老版本<br></code></pre></td></tr></table></figure>
<p>那么这个判断条件是什么呢？</p>
<img src="/dajiangdahe.github.io/2021/10/22/MysqlMVCC/20190314144440494.png" class="" title="在这里插入图片描述">
<ul>
<li>
<p><code>trx_list</code>（名字我随便取的）<br>
一个数值列表，用来维护Read View生成时刻系统正活跃的事务ID</p>
</li>
<li>
<p><code>up_limit_id</code><br>
记录trx_list列表中事务ID最小的ID</p>
</li>
<li>
<p><code>low_limit_id</code><br>
ReadView生成时刻系统尚未分配的下一个事务ID，也就是<code>目前已出现过的事务ID的最大值+1</code></p>
</li>
<li>
<p>首先比较<code>DB_TRX_ID &lt; up_limit_id</code>, 如果小于，则当前事务能看到<code>DB_TRX_ID</code> 所在的记录，如果大于等于进入下一个判断</p>
</li>
<li>
<p>接下来判断 <code>DB_TRX_ID 大于等于 low_limit_id</code> , 如果大于等于则代表<code>DB_TRX_ID</code> 所在的记录在<code>Read View</code>生成后才出现的，那对当前事务肯定不可见，如果小于则进入下一个判断</p>
</li>
<li>
<p>判断<code>DB_TRX_ID</code> 是否在活跃事务之中，<code>trx_list.contains(DB_TRX_ID)</code>，如果在，则代表我<code>Read View</code>生成时刻，你这个事务还在活跃，还没有Commit，你修改的数据，我当前事务也是看不见的；如果不在，则说明，你这个事务在<code>Read View</code>生成之前就已经Commit了，你修改的结果，我当前事务是能看见的</p>
</li>
</ul>
<h4 id="整体流程">整体流程</h4>
<p>我们在了解了<code>隐式字段</code>，<code>undo log</code>， 以及<code>Read View</code>的概念之后，就可以来看看MVCC实现的整体流程是怎么样了</p>
<p>整体的流程是怎么样的呢？我们可以模拟一下</p>
<ul>
<li>当<code>事务2</code>对某行数据执行了<code>快照读</code>，数据库为该行数据生成一个<code>Read View</code>读视图，假设当前事务ID为<code>2</code>，此时还有<code>事务1</code>和<code>事务3</code>在活跃中，<code>事务4</code>在<code>事务2</code>快照读前一刻提交更新了，所以Read View记录了系统当前活跃事务1，3的ID，维护在一个列表上，假设我们称为<code>trx_list</code></li>
</ul>
<table>
<thead>
<tr>
<th>事务1</th>
<th>事务2</th>
<th>事务3</th>
<th>事务4</th>
</tr>
</thead>
<tbody>
<tr>
<td>事务开始</td>
<td>事务开始</td>
<td>事务开始</td>
<td>事务开始</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>修改且已提交</td>
</tr>
<tr>
<td>进行中</td>
<td>快照读</td>
<td>进行中</td>
<td></td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li>Read View不仅仅会通过一个列表<code>trx_list</code>来维护<code>事务2</code>执行<code>快照读</code>那刻系统正活跃的事务ID，还会有两个属性<code>up_limit_id</code>（记录trx_list列表中事务ID最小的ID），<code>low_limit_id</code>(记录trx_list列表中事务ID最大的ID，也有人说快照读那刻系统尚未分配的下一个事务ID也就是<code>目前已出现过的事务ID的最大值+1</code>，我更倾向于后者。所以在这里例子中<code>up_limit_id</code>就是1，<code>low_limit_id</code>就是4 + 1 = 5，trx_list集合的值是1,3，<code>Read View</code>如下图</li>
</ul>
<img src="/dajiangdahe.github.io/2021/10/22/MysqlMVCC/20190313224045780.png" class="" title="img">
<ul>
<li>我们的例子中，只有<code>事务4</code>修改过该行记录，并在<code>事务2</code>执行<code>快照读</code>前，就提交了事务，所以当前该行当前数据的<code>undo log</code>如下图所示；我们的事务2在快照读该行记录的时候，就会拿该行记录的<code>DB_TRX_ID</code>去跟<code>up_limit_id</code>,<code>low_limit_id</code>和<code>活跃事务ID列表(trx_list)</code>进行比较，判断当前<code>事务2</code>能看到该记录的版本是哪个。</li>
</ul>
<img src="/dajiangdahe.github.io/2021/10/22/MysqlMVCC/2019031322511052.png" class="" title="img">
<ul>
<li>所以先拿该记录<code>DB_TRX_ID</code>字段记录的事务ID <code>4</code>去跟<code>Read View</code>的的<code>up_limit_id</code>比较，看<code>4</code>是否小于<code>up_limit_id</code>(1)，所以不符合条件，继续判断 <code>4</code> 是否大于等于 <code>low_limit_id</code>(5)，也不符合条件，最后判断<code>4</code>是否处于<code>trx_list</code>中的活跃事务, 最后发现事务ID为<code>4</code>的事务不在当前活跃事务列表中, 符合可见性条件，所以<code>事务4</code>修改后提交的最新结果对<code>事务2</code>快照读时是可见的，所以<code>事务2</code>能读到的最新数据记录是<code>事务4</code>所提交的版本，而事务4提交的版本也是全局角度上最新的版本</li>
</ul>
<img src="/dajiangdahe.github.io/2021/10/22/MysqlMVCC/20190314141320189.jpg" class="" title="在这里插入图片描述">
<ul>
<li>也正是Read View生成时机的不同，从而造成RC,RR级别下快照读的结果的不同</li>
</ul>
<h3 id="【相关问题】">【相关问题】</h3>
<h5 id="当前读和快照读在RR级别下的区别：">当前读和快照读在RR级别下的区别：</h5>
<table>
<thead>
<tr>
<th>事务A</th>
<th>事务B</th>
</tr>
</thead>
<tbody>
<tr>
<td>开启事务</td>
<td>开启事务</td>
</tr>
<tr>
<td>快照读(无影响)查询金额为500</td>
<td>快照读查询金额为500</td>
</tr>
<tr>
<td>更新金额为400</td>
<td></td>
</tr>
<tr>
<td>提交事务</td>
<td></td>
</tr>
<tr>
<td></td>
<td>select <code>快照读</code>金额为500</td>
</tr>
<tr>
<td></td>
<td>select lock in share mode<code>当前读</code>金额为400</td>
</tr>
</tbody>
</table>
<p>在上表的顺序下，事务B的在事务A提交修改后的快照读是旧版本数据，而当前读是实时新数据400</p>
<table>
<thead>
<tr>
<th>事务A</th>
<th>事务B</th>
</tr>
</thead>
<tbody>
<tr>
<td>开启事务</td>
<td>开启事务</td>
</tr>
<tr>
<td>快照读（无影响）查询金额为500</td>
<td></td>
</tr>
<tr>
<td>更新金额为400</td>
<td></td>
</tr>
<tr>
<td>提交事务</td>
<td></td>
</tr>
<tr>
<td></td>
<td>select <code>快照读</code>金额为400</td>
</tr>
<tr>
<td></td>
<td>select lock in share mode<code>当前读</code>金额为400</td>
</tr>
</tbody>
</table>
<p>而在<code>表2</code>这里的顺序中，事务B在事务A提交后的快照读和当前读都是实时的新数据400，这是为什么呢？</p>
<ul>
<li>这里与上表的唯一区别仅仅是<code>表1</code>的事务B在事务A修改金额前<code>快照读</code>过一次金额数据，而<code>表2</code>的事务B在事务A修改金额前没有进行过快照读。</li>
</ul>
<p>所以我们知道事务中快照读的结果是非常依赖该事务首次出现快照读的地方，即某个事务中首次出现快照读的地方非常关键，它有决定该事务后续快照读结果的能力</p>
<p>我们这里测试的是<code>更新</code>，同时<code>删除</code>和<code>更新</code>也是一样的，如果事务B的快照读是在事务A操作之后进行的，事务B的快照读也是能读取到最新的数据的</p>
<h4 id="RC-RR级别下的InnoDB快照读有什么不同？">RC,RR级别下的InnoDB快照读有什么不同？</h4>
<p>正是<code>Read View</code>生成时机的不同，从而造成RC,RR级别下快照读的结果的不同</p>
<ul>
<li><strong>在RR级别下的某个事务的对某条记录的第一次快照读会创建一个快照及Read View</strong>, 将当前系统活跃的其他事务记录起来，此后在<strong>调用快照读的时候，还是使用的是同一个Read</strong> <strong>View</strong>，所以只要当前事务在其他事务提交更新之前使用过快照读，那么<strong>之后的快照读使用的都是同一个Read View</strong>，所以对之后的修改不可见；即RR级别下，快照读生成Read View时，Read View会记录此时所有其他活动事务的快照，这些事务的修改对于当前事务都是不可见的。而早于Read View创建的事务所做的修改均是可见</li>
<li>而在<strong>RC级别下的，事务中，每次快照读都会新生成一个快照和Read View</strong>, 这就是我们在RC级别下的事务中可以看到别的事务提交的更新的原因</li>
</ul>
<p><strong>总之在RC隔离级别下，是每个快照读都会生成并获取最新的Read View；而在RR隔离级别下，则是同一个事务中的第一个快照读才会创建Read View, 之后的快照读获取的都是同一个Read View。</strong></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/dajiangdahe.github.io/tags/Mysql/" rel="tag">Mysql</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-JavaInterview-04"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/dajiangdahe.github.io/2021/10/22/JavaInterview-04/"
    >JavaInterview-04</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/dajiangdahe.github.io/2021/10/22/JavaInterview-04/" class="article-date">
  <time datetime="2021-10-22T07:21:44.873Z" itemprop="datePublished">2021-10-22</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/dajiangdahe.github.io/categories/JavaInterview/">JavaInterview</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h3 id="十五、Kafka">十五、Kafka</h3>
<h4 id="152-kafka-可以脱离-zookeeper-单独使用吗？为什么？">152.kafka 可以脱离 zookeeper 单独使用吗？为什么？</h4>
<p>kafka 不能脱离 zookeeper 单独使用，因为 kafka 使用 zookeeper 管理和协调 kafka 的节点服务器。</p>
<h4 id="153-kafka-有几种数据保留的策略？">153.kafka 有几种数据保留的策略？</h4>
<p>kafka 有两种数据保存策略：按照过期时间保留和按照存储的消息大小保留。</p>
<h4 id="154-kafka-同时设置了-7-天和-10G-清除数据，到第五天的时候消息达到了-10G，这个时候-kafka-将如何处理？">154.kafka 同时设置了 7 天和 10G 清除数据，到第五天的时候消息达到了 10G，这个时候 kafka 将如何处理？</h4>
<p>这个时候 kafka 会执行数据清除工作，时间和大小不论那个满足条件，都会清空数据。</p>
<h4 id="155-什么情况会导致-kafka-运行变慢？">155.什么情况会导致 kafka 运行变慢？</h4>
<ul>
<li>cpu 性能瓶颈</li>
<li>磁盘读写瓶颈</li>
<li>网络瓶颈</li>
</ul>
<h4 id="156-使用-kafka-集群需要注意什么？">156.使用 kafka 集群需要注意什么？</h4>
<ul>
<li>集群的数量不是越多越好，最好不要超过 7 个，因为节点越多，消息复制需要的时间就越长，整个群组的吞吐量就越低。</li>
<li>集群数量最好是单数，因为超过一半故障集群就不能用了，设置为单数容错率更高。</li>
</ul>
<h3 id="十六、Zookeeper">十六、Zookeeper</h3>
<h4 id="157-zookeeper-是什么？">157.zookeeper 是什么？</h4>
<p>zookeeper 是一个分布式的，开放源码的分布式应用程序协调服务，是 google chubby 的开源实现，是 hadoop 和 hbase 的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。</p>
<h4 id="158-zookeeper-都有哪些功能？">158.zookeeper 都有哪些功能？</h4>
<ul>
<li>集群管理：监控节点存活状态、运行请求等。</li>
<li>主节点选举：主节点挂掉了之后可以从备用的节点开始新一轮选主，主节点选举说的就是这个选举的过程，使用 zookeeper 可以协助完成这个过程。</li>
<li>分布式锁：zookeeper 提供两种锁：独占锁、共享锁。独占锁即一次只能有一个线程使用资源，共享锁是读锁共享，读写互斥，即可以有多线线程同时读同一个资源，如果要使用写锁也只能有一个线程使用。zookeeper可以对分布式锁进行控制。</li>
<li>命名服务：在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。</li>
</ul>
<h4 id="159-zookeeper-有几种部署模式？">159.zookeeper 有几种部署模式？</h4>
<p>zookeeper 有三种部署模式：</p>
<ul>
<li>单机部署：一台集群上运行；</li>
<li>集群部署：多台集群运行；</li>
<li>伪集群部署：一台集群启动多个 zookeeper 实例运行。</li>
</ul>
<h4 id="160-zookeeper-怎么保证主从节点的状态同步？">160.zookeeper 怎么保证主从节点的状态同步？</h4>
<p>zookeeper 的核心是原子广播，这个机制保证了各个 server 之间的同步。实现这个机制的协议叫做 zab 协议。 zab 协议有两种模式，分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，zab 就进入了恢复模式，当领导者被选举出来，且大多数 server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 server 具有相同的系统状态。</p>
<h4 id="161-集群中为什么要有主节点？">161.集群中为什么要有主节点？</h4>
<p>在分布式环境中，有些业务逻辑只需要集群中的某一台机器进行执行，其他的机器可以共享这个结果，这样可以大大减少重复计算，提高性能，所以就需要主节点。</p>
<h4 id="162-集群中有-3-台服务器，其中一个节点宕机，这个时候-zookeeper-还可以使用吗？">162.集群中有 3 台服务器，其中一个节点宕机，这个时候 zookeeper 还可以使用吗？</h4>
<p>可以继续使用，单数服务器只要没超过一半的服务器宕机就可以继续使用。</p>
<h4 id="163-说一下-zookeeper-的通知机制？">163.说一下 zookeeper 的通知机制？</h4>
<p>客户端端会对某个 znode 建立一个 watcher 事件，当该 znode 发生变化时，这些客户端会收到 zookeeper 的通知，然后客户端可以根据 znode 变化来做出业务上的改变。</p>
<h3 id="十七、MySql">十七、MySql</h3>
<h4 id="164-数据库的三范式是什么？">164.数据库的三范式是什么？</h4>
<ul>
<li>第一范式：强调的是列的原子性，即数据库表的每一列都是不可分割的原子数据项。</li>
<li>第二范式：要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性。</li>
<li>第三范式：任何非主属性不依赖于其它非主属性。</li>
</ul>
<h4 id="165-一张自增表里面总共有-7-条数据，删除了最后-2-条数据，重启-mysql-数据库，又插入了一条数据，此时-id-是几？">165.一张自增表里面总共有 7 条数据，删除了最后 2 条数据，重启 mysql 数据库，又插入了一条数据，此时 id 是几？</h4>
<ul>
<li>表类型如果是 MyISAM ，那 id 就是 8。</li>
<li>表类型如果是 InnoDB，那 id 就是 6。</li>
</ul>
<p>InnoDB 表只会把自增主键的最大 id 记录在内存中，所以重启之后会导致最大 id 丢失。</p>
<h4 id="166-如何获取当前数据库版本？">166.如何获取当前数据库版本？</h4>
<p>使用 select version() 获取当前 mysql 数据库版本。</p>
<h4 id="167-说一下-ACID-是什么？">167.说一下 ACID 是什么？</h4>
<ul>
<li>Atomicity（原子性）：一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被恢复（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。</li>
<li>Consistency（一致性）：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。</li>
<li>Isolation（隔离性）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。</li>
<li>Durability（持久性）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。</li>
</ul>
<h4 id="168-char-和-varchar-的区别是什么？">168.char 和 varchar 的区别是什么？</h4>
<ul>
<li><strong>char(n)</strong> ：固定长度类型，比如订阅 char(10)，当你输入&quot;abc&quot;三个字符的时候，它们占的空间还是 10 个字节，其他 7 个是空字节。</li>
</ul>
<p>chat 优点：效率高；缺点：占用空间；适用场景：存储密码的 md5 值，固定长度的，使用 char 非常合适。</p>
<ul>
<li><strong>varchar(n)</strong> ：可变长度，存储的值是每个值占用的字节再加上一个用来记录其长度的字节的长度。</li>
</ul>
<p>所以，从空间上考虑 varcahr 比较合适；从效率上考虑 char 比较合适，二者使用需要权衡。</p>
<h4 id="169-float-和-double-的区别是什么？">169.float 和 double 的区别是什么？</h4>
<ul>
<li>float 最多可以存储 8 位的十进制数，并在内存中占 4 字节。</li>
<li>double 最可可以存储 16 位的十进制数，并在内存中占 8 字节。</li>
</ul>
<h4 id="170-mysql-的内连接、左连接、右连接有什么区别？">170.mysql 的内连接、左连接、右连接有什么区别？</h4>
<p>内连接关键字：inner join；左连接：left join；右连接：right join。</p>
<p>内连接是把匹配的关联数据显示出来；左连接是左边的表全部显示出来，右边的表显示出符合条件的数据；右连接正好相反。</p>
<h4 id="171-mysql-索引是怎么实现的？">171.mysql 索引是怎么实现的？</h4>
<p>索引是满足某种特定查找算法的数据结构，而这些数据结构会以某种方式指向数据，从而实现高效查找数据。</p>
<p>具体来说 mysql 中的索引，不同的数据引擎实现有所不同，但目前主流的数据库引擎的索引都是 B+ 树实现的，B+ 树的搜索效率，可以到达二分法的性能，找到数据区域之后就找到了完整的数据结构了，所有索引的性能也是更好的。</p>
<h4 id="172-怎么验证-mysql-的索引是否满足需求？">172.怎么验证 mysql 的索引是否满足需求？</h4>
<p>使用 explain 查看 sql 是如何执行查询语句的，从而分析你的索引是否满足需求。</p>
<p>explain 语法：explain select * from table where type=1。</p>
<h4 id="173-说一下数据库的事务隔离？">173.说一下数据库的事务隔离？</h4>
<p>mysql 的事务隔离是在 mysql.ini 配置文件里添加的，在文件的最后添加：</p>
<blockquote>
<p>transaction-isolation = REPEATABLE-READ</p>
</blockquote>
<p>可用的配置值：READ-UNCOMMITTED、READ-COMMITTED、REPEATABLE-READ、SERIALIZABLE。</p>
<ul>
<li>READ-UNCOMMITTED：未提交读，最低隔离级别、事务未提交前，就可被其他事务读取（会出现幻读、脏读、不可重复读）。</li>
<li>READ-COMMITTED：提交读，一个事务提交后才能被其他事务读取到（会造成幻读、不可重复读）。</li>
<li>REPEATABLE-READ：可重复读，默认级别，保证多次读取同一个数据时，其值都和事务开始时候的内容是一致，禁止读取到别的事务未提交的数据（会造成幻读）。</li>
<li>SERIALIZABLE：序列化，代价最高最可靠的隔离级别，该隔离级别能防止脏读、不可重复读、幻读。</li>
</ul>
<p><strong>脏读</strong> ：表示一个事务能够读取另一个事务中还未提交的数据。比如，某个事务尝试插入记录 A，此时该事务还未提交，然后另一个事务尝试读取到了记录 A。</p>
<p><strong>不可重复读</strong> ：是指在一个事务内，多次读同一数据。</p>
<p><strong>幻读</strong> ：指同一个事务内多次查询返回的结果集不一样。比如同一个事务 A 第一次查询时候有 n 条记录，但是第二次同等条件下查询却有 n+1 条记录，这就好像产生了幻觉。发生幻读的原因也是另外一个事务新增或者删除或者修改了第一个事务结果集里面的数据，同一个记录的数据内容被修改了，所有数据行的记录就变多或者变少了。</p>
<h4 id="174-说一下-mysql-常用的引擎？">174.说一下 mysql 常用的引擎？</h4>
<ul>
<li>InnoDB 引擎：mysql 5.1 之后默认引擎，提供了对数据库 acid 事务的支持，并且还提供了行级锁和外键的约束，它的设计的目标就是处理大数据容量的数据库系统。mysql 运行的时候，InnoDB 会在内存中建立缓冲池，用于缓冲数据和索引。但是该引擎是不支持全文搜索，同时启动也比较的慢，它是不会保存表的行数的，所以当进行 select count(*) from table 指令的时候，需要进行扫描全表。由于锁的粒度小，写操作是不会锁定全表的,所以在并发度较高的场景下使用会提升效率的。</li>
<li>MyIASM 引擎：不提供事务的支持，也不支持行级锁和外键。因此当执行插入和更新语句时，即执行写操作的时候需要锁定这个表，所以会导致效率会降低。不过和 InnoDB 不同的是，MyIASM 引擎是保存了表的行数，于是当进行 select count(*) from table 语句时，可以直接的读取已经保存的值而不需要进行扫描全表。所以，如果表的读操作远远多于写操作时，并且不需要事务的支持的，可以将 MyIASM 作为数据库引擎的首选。</li>
</ul>
<h4 id="175-说一下-mysql-的行锁和表锁？">175.说一下 mysql 的行锁和表锁？</h4>
<p>MyISAM 只支持表锁，InnoDB 支持表锁和行锁，默认为行锁。</p>
<ul>
<li>表级锁：开销小，加锁快，不会出现死锁。锁定粒度大，发生锁冲突的概率最高，并发量最低。</li>
<li>行级锁：开销大，加锁慢，会出现死锁。锁力度小，发生锁冲突的概率小，并发度最高。</li>
</ul>
<h4 id="176-说一下乐观锁和悲观锁？">176.说一下乐观锁和悲观锁？</h4>
<ul>
<li>乐观锁：每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在提交更新的时候会判断一下在此期间别人有没有去更新这个数据。</li>
<li>悲观锁：每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻止，直到这个锁被释放。</li>
</ul>
<p>数据库的乐观锁需要自己实现，在表里面添加一个 version 字段，每次修改成功值加 1，这样每次修改的时候先对比一下，自己拥有的 version 和数据库现在的 version 是否一致，如果不一致就不修改，这样就实现了乐观锁。</p>
<h4 id="177-mysql-问题排查都有哪些手段？">177.mysql 问题排查都有哪些手段？</h4>
<ul>
<li>使用 show processlist 命令查看当前所有连接信息。</li>
<li>使用 explain 命令查询 sql 语句执行计划。</li>
<li>开启慢查询日志，查看慢查询的 sql。</li>
</ul>
<h4 id="178-如何做-mysql-的性能优化？">178.如何做 mysql 的性能优化？</h4>
<ul>
<li>为搜索字段创建索引。</li>
<li>避免使用 select *，列出需要查询的字段。</li>
<li>垂直分割分表。</li>
<li>选择正确的存储引擎。</li>
</ul>
<h3 id="十八、Redis">十八、Redis</h3>
<h4 id="179-redis-是什么？都有哪些使用场景？">179.redis 是什么？都有哪些使用场景？</h4>
<p>redis 是一个使用 C 语言开发的高速缓存数据库。</p>
<p>redis 使用场景：</p>
<ul>
<li>记录帖子点赞数、点击数、评论数；</li>
<li>缓存近期热帖；</li>
<li>缓存文章详情信息；</li>
<li>记录用户会话信息。</li>
</ul>
<h4 id="180-redis-有哪些功能？">180.redis 有哪些功能？</h4>
<ul>
<li>数据缓存功能</li>
<li>分布式锁的功能</li>
<li>支持数据持久化</li>
<li>支持事务</li>
<li>支持消息队列</li>
</ul>
<h4 id="181-redis-和-memcache-有什么区别？">181.redis 和 memcache 有什么区别？</h4>
<ul>
<li>存储方式不同：memcache 把数据全部存在内存之中，断电后会挂掉，数据不能超过内存大小；redis 有部份存在硬盘上，这样能保证数据的持久性。</li>
<li>数据支持类型：memcache 对数据类型支持相对简单；redis 有复杂的数据类型。</li>
<li>使用底层模型不同：它们之间底层实现方式，以及与客户端之间通信的应用协议不一样，redis 自己构建了 vm 机制，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。</li>
<li>value 值大小不同：redis 最大可以达到 512mb；memcache 只有 1mb。</li>
</ul>
<h4 id="182-redis-为什么是单线程的？">182.redis 为什么是单线程的？</h4>
<p>因为 cpu 不是 redis 的瓶颈，redis 的瓶颈最有可能是机器内存或者网络带宽。既然单线程容易实现，而且 cpu  又不会成为瓶颈，那就顺理成章地采用单线程的方案了。</p>
<p>关于 redis 的性能，官方网站也有，普通笔记本轻松处理每秒几十万的请求。</p>
<p>而且单线程并不代表就慢 nginx 和 nodejs 也都是高性能单线程的代表。</p>
<h4 id="183-什么是缓存穿透？怎么解决？">183.什么是缓存穿透？怎么解决？</h4>
<p>缓存穿透：指查询一个一定不存在的数据，由于缓存是不命中时需要从数据库查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。</p>
<p>解决方案：最简单粗暴的方法如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们就把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。</p>
<h4 id="184-redis-支持的数据类型有哪些？">184.redis 支持的数据类型有哪些？</h4>
<p>redis 支持的数据类型：string（字符串）、list（列表）、hash（字典）、set（集合）、zset（有序集合）。</p>
<h4 id="185-redis-支持的-java-客户端都有哪些？">185.redis 支持的 java 客户端都有哪些？</h4>
<p>支持的 java 客户端有 redisson、jedis、lettuce 等。</p>
<h4 id="186-jedis-和-redisson-有哪些区别？">186.jedis 和 redisson 有哪些区别？</h4>
<ul>
<li>jedis：提供了比较全面的 redis 命令的支持。</li>
<li>redisson：实现了分布式和可扩展的 java 数据结构，与 jedis 相比 redisson 的功能相对简单，不支持排序、事务、管道、分区等 redis 特性。</li>
</ul>
<h4 id="187-怎么保证缓存和数据库数据的一致性？">187.怎么保证缓存和数据库数据的一致性？</h4>
<ul>
<li>合理设置缓存的过期时间。</li>
<li>新增、更改、删除数据库操作时同步更新 redis，可以使用事物机制来保证数据的一致性。</li>
</ul>
<h4 id="188-redis-持久化有几种方式？">188.redis 持久化有几种方式？</h4>
<p>redis 的持久化有两种方式，或者说有两种策略：</p>
<ul>
<li>RDB（Redis Database）：指定的时间间隔能对你的数据进行快照存储。</li>
<li>AOF（Append Only File）：每一个收到的写命令都通过write函数追加到文件中。</li>
</ul>
<h4 id="189-redis-怎么实现分布式锁？">189.redis 怎么实现分布式锁？</h4>
<p>redis 分布式锁其实就是在系统里面占一个“坑”，其他程序也要占“坑”的时候，占用成功了就可以继续执行，失败了就只能放弃或稍后重试。</p>
<p>占坑一般使用 setnx(set if not exists)指令，只允许被一个程序占有，使用完调用 del 释放锁。</p>
<h4 id="190-redis-分布式锁有什么缺陷？">190.redis 分布式锁有什么缺陷？</h4>
<p>redis 分布式锁不能解决超时的问题，分布式锁有一个超时时间，程序的执行如果超出了锁的超时时间就会出现问题。</p>
<h4 id="191-redis-如何做内存优化？">191.redis 如何做内存优化？</h4>
<p>尽量使用 redis 的散列表，把相关的信息放到散列表里面存储，而不是把每个字段单独存储，这样可以有效的减少内存使用。比如将 web 系统的用户对象，应该放到散列表里面再整体存储到 redis，而不是把用户的姓名、年龄、密码、邮箱等字段分别设置 key 进行存储。</p>
<h4 id="192-redis-淘汰策略有哪些？">192.redis 淘汰策略有哪些？</h4>
<ul>
<li>volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰。</li>
<li>volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰。</li>
<li>volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰。</li>
<li>allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰。</li>
<li>allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰。</li>
<li>no-enviction（驱逐）：禁止驱逐数据。</li>
</ul>
<h4 id="193-redis-常见的性能问题有哪些？该如何解决？">193.redis 常见的性能问题有哪些？该如何解决？</h4>
<ul>
<li>主服务器写内存快照，会阻塞主线程的工作，当快照比较大时对性能影响是非常大的，会间断性暂停服务，所以主服务器最好不要写内存快照。</li>
<li>redis 主从复制的性能问题，为了主从复制的速度和连接的稳定性，主从库最好在同一个局域网内。</li>
</ul>
<h3 id="十九、JVM">十九、JVM</h3>
<h4 id="194-说一下-jvm-的主要组成部分？及其作用？">194.说一下 jvm 的主要组成部分？及其作用？</h4>
<ul>
<li>类加载器（ClassLoader）</li>
<li>运行时数据区（Runtime Data Area）</li>
<li>执行引擎（Execution Engine）</li>
<li>本地库接口（Native Interface）</li>
</ul>
<p><strong>组件的作用：</strong> 首先通过类加载器（ClassLoader）会把 java 代码转换成字节码，运行时数据区（Runtime Data Area）再把字节码加载到内存中，而字节码文件只是 jvm 的一套指令集规范，并不能直接交给底层操作系统去执行，因此需要特定的命令解析器执行引擎（Execution Engine），将字节码翻译成底层系统指令，再交由 CPU 去执行，而这个过程中需要调用其他语言的本地库接口（Native Interface）来实现整个程序的功能。</p>
<h4 id="195-说一下-jvm-运行时数据区？">195.说一下 jvm 运行时数据区？</h4>
<p>不同虚拟机的运行时数据区可能略微有所不同，但都会遵从 java 虚拟机规范， java 虚拟机规范规定的区域分为以下 5 个部分：</p>
<ul>
<li>程序计数器（Program Counter Register）：当前线程所执行的字节码的行号指示器，字节码解析器的工作是通过改变这个计数器的值，来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能，都需要依赖这个计数器来完成；</li>
<li>Java 虚拟机栈（Java Virtual Machine Stacks）：用于存储局部变量表、操作数栈、动态链接、方法出口等信息；</li>
<li>本地方法栈（Native Method Stack）：与虚拟机栈的作用是一样的，只不过虚拟机栈是服务 java 方法的，而本地方法栈是为虚拟机调用 Native 方法服务的；</li>
<li>Java 堆（Java Heap）：java 虚拟机中内存最大的一块，是被所有线程共享的，几乎所有的对象实例都在这里分配内存；</li>
<li>方法区（Methed Area）：用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译后的代码等数据。</li>
</ul>
<h4 id="196-说一下堆栈的区别？">196.说一下堆栈的区别？</h4>
<ul>
<li>功能方面：堆是用来存放对象的，栈是用来执行程序的。</li>
<li>共享性：堆是线程共享的，栈是线程私有的。</li>
<li>空间大小：堆大小远远大于栈。</li>
</ul>
<h4 id="197-队列和栈是什么？有什么区别？">197.队列和栈是什么？有什么区别？</h4>
<p>队列和栈都是被用来预存储数据的。</p>
<p>队列允许先进先出检索元素，但也有例外的情况，Deque 接口允许从两端检索元素。</p>
<p>栈和队列很相似，但它运行对元素进行后进先出进行检索。</p>
<h4 id="198-什么是双亲委派模型？">198.什么是双亲委派模型？</h4>
<p>在介绍双亲委派模型之前先说下类加载器。对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立在 jvm 中的唯一性，每一个类加载器，都有一个独立的类名称空间。类加载器就是根据指定全限定名称将 class 文件加载到 jvm 内存，然后再转化为 class 对象。</p>
<p>类加载器分类：</p>
<ul>
<li>启动类加载器（Bootstrap ClassLoader），是虚拟机自身的一部分，用来加载JAVA_HOME/lib/目录中的，或者被 -Xbootclasspath 参数所指定的路径中并且被虚拟机识别的类库；</li>
<li>其他类加载器：
<ul>
<li>扩展类加载器（Extension ClassLoader）：负责加载&lt;JAVA_HOME&gt;\lib\ext目录或java.ext.dirs系统变量指定的路径中的所有类库；</li>
<li>应用程序类加载器（Application ClassLoader）。负责加载用户类路径（classpath）上的指定类库，我们可以直接使用这个类加载器。一般情况，如果我们没有自定义类加载器默认就是用这个加载器。</li>
</ul>
</li>
</ul>
<p>双亲委派模型：如果一个类加载器收到了类加载的请求，它首先不会自己去加载这个类，而是把这个请求委派给父类加载器去完成，每一层的类加载器都是如此，这样所有的加载请求都会被传送到顶层的启动类加载器中，只有当父加载无法完成加载请求（它的搜索范围中没找到所需的类）时，子加载器才会尝试去加载类。</p>
<h4 id="199-说一下类装载的执行过程？">199.说一下类装载的执行过程？</h4>
<p>类装载分为以下 5 个步骤：</p>
<ul>
<li>加载：根据查找路径找到相应的 class 文件然后导入；</li>
<li>检查：检查加载的 class 文件的正确性；</li>
<li>准备：给类中的静态变量分配内存空间；</li>
<li>解析：虚拟机将常量池中的符号引用替换成直接引用的过程。符号引用就理解为一个标示，而在直接引用直接指向内存中的地址；</li>
<li>初始化：对静态变量和静态代码块执行初始化工作。</li>
</ul>
<h4 id="200-怎么判断对象是否可以被回收？">200.怎么判断对象是否可以被回收？</h4>
<p>一般有两种方法来判断：</p>
<ul>
<li>引用计数器：为每个对象创建一个引用计数，有对象引用时计数器 +1，引用被释放时计数 -1，当计数器为 0 时就可以被回收。它有一个缺点不能解决循环引用的问题；</li>
<li>可达性分析：从 GC Roots 开始向下搜索，搜索所走过的路径称为引用链。当一个对象到 GC Roots 没有任何引用链相连时，则证明此对象是可以被回收的。</li>
</ul>
<h4 id="201-java-中都有哪些引用类型？">201.java 中都有哪些引用类型？</h4>
<ul>
<li>强引用：发生 gc 的时候不会被回收。</li>
<li>软引用：有用但不是必须的对象，在发生内存溢出之前会被回收。</li>
<li>弱引用：有用但不是必须的对象，在下一次GC时会被回收。</li>
<li>虚引用（幽灵引用/幻影引用）：无法通过虚引用获得对象，用 PhantomReference 实现虚引用，虚引用的用途是在 gc 时返回一个通知。</li>
</ul>
<h4 id="202-说一下-jvm-有哪些垃圾回收算法？">202.说一下 jvm 有哪些垃圾回收算法？</h4>
<ul>
<li>标记-清除算法：标记无用对象，然后进行清除回收。缺点：效率不高，无法清除垃圾碎片。</li>
<li>标记-整理算法：标记无用对象，让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存。</li>
<li>复制算法：按照容量划分二个大小相等的内存区域，当一块用完的时候将活着的对象复制到另一块上，然后再把已使用的内存空间一次清理掉。缺点：内存使用率不高，只有原来的一半。</li>
<li>分代算法：根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代，新生代基本采用复制算法，老年代采用标记整理算法。</li>
</ul>
<h4 id="203-说一下-jvm-有哪些垃圾回收器？">203.说一下 jvm 有哪些垃圾回收器？</h4>
<ul>
<li>Serial：最早的单线程串行垃圾回收器。</li>
<li>Serial Old：Serial 垃圾回收器的老年版本，同样也是单线程的，可以作为 CMS 垃圾回收器的备选预案。</li>
<li>ParNew：是 Serial 的多线程版本。</li>
<li>Parallel 和 ParNew 收集器类似是多线程的，但 Parallel 是吞吐量优先的收集器，可以牺牲等待时间换取系统的吞吐量。</li>
<li>Parallel Old 是 Parallel 老生代版本，Parallel 使用的是复制的内存回收算法，Parallel Old 使用的是标记-整理的内存回收算法。</li>
<li>CMS：一种以获得最短停顿时间为目标的收集器，非常适用 B/S 系统。</li>
<li>G1：一种兼顾吞吐量和停顿时间的 GC 实现，是 JDK 9 以后的默认 GC 选项。</li>
</ul>
<h4 id="204-详细介绍一下-CMS-垃圾回收器？">204.详细介绍一下 CMS 垃圾回收器？</h4>
<p>CMS 是英文 Concurrent Mark-Sweep 的简称，是以牺牲吞吐量为代价来获得最短回收停顿时间的垃圾回收器。对于要求服务器响应速度的应用上，这种垃圾回收器非常适合。在启动 jvm 的参数加上“-XX:+UseConcMarkSweepGC”来指定使用 CMS 垃圾回收器。</p>
<p>CMS 使用的是标记-清除的算法实现的，所以在 gc 的时候回产生大量的内存碎片，当剩余内存不能满足程序运行要求时，系统将会出现 Concurrent Mode Failure，临时 CMS 会采用 Serial Old 回收器进行垃圾清除，此时的性能将会被降低。</p>
<h4 id="205-新生代垃圾回收器和老生代垃圾回收器都有哪些？有什么区别？">205.新生代垃圾回收器和老生代垃圾回收器都有哪些？有什么区别？</h4>
<ul>
<li>新生代回收器：Serial、ParNew、Parallel Scavenge</li>
<li>老年代回收器：Serial Old、Parallel Old、CMS</li>
<li>整堆回收器：G1</li>
</ul>
<p>新生代垃圾回收器一般采用的是复制算法，复制算法的优点是效率高，缺点是内存利用率低；老年代回收器一般采用的是标记-整理的算法进行垃圾回收。</p>
<h4 id="206-简述分代垃圾回收器是怎么工作的？">206.简述分代垃圾回收器是怎么工作的？</h4>
<p>分代回收器有两个分区：老生代和新生代，新生代默认的空间占比总空间的 1/3，老生代的默认占比是 2/3。</p>
<p>新生代使用的是复制算法，新生代里有 3 个分区：Eden、To Survivor、From Survivor，它们的默认占比是 8:1:1，它的执行流程如下：</p>
<ul>
<li>把 Eden + From Survivor 存活的对象放入 To Survivor 区；</li>
<li>清空 Eden 和 From Survivor 分区；</li>
<li>From Survivor 和 To Survivor 分区交换，From Survivor 变 To Survivor，To Survivor 变 From Survivor。</li>
</ul>
<p>每次在 From Survivor 到 To Survivor 移动时都存活的对象，年龄就 +1，当年龄到达 15（默认配置是 15）时，升级为老生代。大对象也会直接进入老生代。</p>
<p>老生代当空间占用到达某个值之后就会触发全局垃圾收回，一般使用标记整理的执行算法。以上这些循环往复就构成了整个分代垃圾回收的整体执行流程。</p>
<h4 id="207-说一下-jvm-调优的工具？">207.说一下 jvm 调优的工具？</h4>
<p>jdk 自带了很多监控工具，都位于 jdk 的 bin 目录下，其中最常用的是 jconsole 和 jvisualvm 这两款视图监控工具。</p>
<ul>
<li>jconsole：用于对 jvm 中的内存、线程和类等进行监控；</li>
<li>jvisualvm：jdk 自带的全能分析工具，可以分析：内存快照、线程快照、程序死锁、监控内存的变化、gc 变化等。</li>
</ul>
<h4 id="208-常用的-jvm-调优的参数都有哪些？">208.常用的 jvm 调优的参数都有哪些？</h4>
<ul>
<li>-Xms2g：初始化推大小为 2g；</li>
<li>-Xmx2g：堆最大内存为 2g；</li>
<li>-XX:NewRatio=4：设置年轻的和老年代的内存比例为 1:4；</li>
<li>-XX:SurvivorRatio=8：设置新生代 Eden 和 Survivor 比例为 8:2；</li>
<li>–XX:+UseParNewGC：指定使用 ParNew + Serial Old 垃圾回收器组合；</li>
<li>-XX:+UseParallelOldGC：指定使用 ParNew + ParNew Old 垃圾回收器组合；</li>
<li>-XX:+UseConcMarkSweepGC：指定使用 CMS + Serial Old 垃圾回收器组合；</li>
<li>-XX:+PrintGC：开启打印 gc 信息；</li>
<li>-XX:+PrintGCDetails：打印 gc 详细信息。</li>
</ul>
<h2 id="结尾">结尾</h2>
<p>这不止是一份面试清单，更是一种“被期望的责任”，因为有无数个待面试着，希望从这篇文章中，找出通往期望公司的“钥匙”，所以上面的每道选题都是结合我自身的经验，于千万个面试题中经过艰辛的两周，一个题一个题筛选出来再校对好答案和格式做出来的，面试的答案也是再三斟酌，生怕误人子弟是小，影响他人的“仕途”才是大过，所以如有纰漏，还请读者朋友们在评论区不吝指出。</p>
<p>也希望您能把这篇文章分享给更多的朋友，让它帮助更多的人。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/dajiangdahe.github.io/tags/%E9%9D%A2%E8%AF%95/" rel="tag">面试</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
  </article>
  

  
  <nav class="page-nav">
    
    <a class="extend prev" rel="prev" href="/dajiangdahe.github.io/page/2/">上一页</a><a class="page-number" href="/dajiangdahe.github.io/">1</a><a class="page-number" href="/dajiangdahe.github.io/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/dajiangdahe.github.io/page/4/">4</a><a class="extend next" rel="next" href="/dajiangdahe.github.io/page/4/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2015-2021
        <i class="ri-heart-fill heart_icon"></i> kengkeng
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/dajiangdahe.github.io/"><img src="/images/ayer-side.svg" alt="kengkeng&#39;s life"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/dajiangdahe.github.io/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/dajiangdahe.github.io/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/dajiangdahe.github.io/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/dajiangdahe.github.io/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/dajiangdahe.github.io/tags/%E6%97%85%E8%A1%8C/">旅行</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" target="_blank" rel="noopener" href="http://shenyu-vip.lofter.com">摄影</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/dajiangdahe.github.io/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/dajiangdahe.github.io/2019/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/dajiangdahe.github.io/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/dajiangdahe.github.io/js/jquery-3.6.0.min.js"></script>
 
<script src="/dajiangdahe.github.io/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dajiangdahe.github.io/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/dajiangdahe.github.io/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/dajiangdahe.github.io/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>